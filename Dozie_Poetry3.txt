Continuum Robot Monologue: Sensors as Alchemy

Something massive is happening at the microscale.
Micromanipulation of tissues promises pathways to curing myriad bodily abnormalities.
Yet the question persists: why is magnetic actuation uniquely suited to this domain?

Magnetism stands as a fundamental force of nature, one that demands no external batteries or power systems when permanent magnets are employed.
An untethered, wireless microrobot paired with an autonomous actuation source becomes essential when modeling systems with elegance.
Field planning reduces to a function of two Jacobian workspaces, dramatically simplifying the computational burden.

This simplicity welcomes a richer complexity when electronics are embedded within the design of the untethered microrobot itself.
Consider a wireless passive LC sensor that broadcasts its resonance frequency for signal processing.
Such a device introduces additional layers of modeling, fabrication, and characterization.

The central question emerges: how does this research address the grander vision?
In the grander vision, soft carbon-based systems merge with hard silicon-based systems.
Learning to embed computerized functionality within soft systems resembles breathing life into the inanimate — a sort of modern alchemy.

Vector network analyzers and impedance analyzers sense LC circuit resonance, processing signals to extract meaningful parameters.
Once extracted and calibrated, these parameters can be visualized in real time through straightforward programming.
Imagine a magnetic continuum robot navigating the lungs: force or vibration data could guide its subsequent movements.
The continuum robot and robotic manipulator would then become a magnetic autonomous navigation system through their sensing apparatus.
Previously limited to reactive behavior, the robot can now process information and be proactive.

This alchemy of animating the inanimate has long captivated me.
I now pursue this fascination through embedded sensors in magnetic polymers and magnetic navigation systems.
Sensors embedded in polymers could revolutionize biosensing, pollution mitigation, and thermal management.

When inert rubbers are infused with magnetic nanoparticles, they acquire a primitive agency.
When subsequently embedded with sensors, they gain a voice — the capacity to communicate their subjective experience of the environment.
This is not panpsychism; this is engineering reality.
Sensors are the alchemical key to material transmutation.

It is for this reason that the robotic revolution will prove far more transformative than our current AI revolution.
There is no rational basis for performing tasks that another accessible entity executes more effectively.
This raises a critical question: when should we cease mining our biological minds as resources and begin leveraging artificial minds instead?
This question grows urgent as habits crystallize rapidly and underutilized faculties atrophy.
Surgeons who rely on robotic methods without maintaining manual practice will eventually lose the ability to operate without technological assistance.
Human thriving has brought about the transcendence of the need to even thrive in the first place, and it leaves us floating in a liminal space.

Chiaroscuro
By Dozie Ubosi

Horridas nostrae mentis purga tenebras, accende lumin sensibus!
(Purge the horrible darkness of our mind, light a light for our senses!)

A poem called “In the Shadow of You”

You shine like a star
Effortlessly so
In a life lived for love
We can’t say that everything goes
Unable to hide the discomfort
Of being sad and alone
Leave all wonder behind
And take a better approach

Greater things depend on lesser things, and these on lesser things still.
Walking into the abandoned monastery, I see a manuscript that seems awfully old.
It belonged to an alchemist who used to be a physicist.
Academic aspirations turned into a life of hermeticism and chasing the occult.
The manuscript details how he can bind human souls.

Geometrical truth lies in the mandala.
I sink into a trance.
The limitations of language become clear — I finally understand Wittgenstein.
Ancient secrets become exposed to me through these images.
I begin to see images of a girl, a succubi.
It’s a familiar face.

Things are either concrete or abstract.
I lived an entire lifetime in the hypnotic trance.
With the succubi as my partner, we lived like hermits in Clarens, Switzerland.
The sounds of Gymnopedie echo through my mind as I fall deeper in love.
My unconscious mind is now at the forefront.

I awake from the trance seeing real life with new eyes.
Images of another lifetime haunt me.
I try to find the mandala to return, but it is gone.
Gymnopedie is replaced with Schoenberg’s atonality and screeching violins.
I still feel the love.

I left the monastery.
I live every second in fantasy.
There’s a difference between imagination and fantasy.
Ancient symbols have become contemporary Dionysian images.
If I can live a lifetime in my brain, it must be proof of the supernatural.
My psychiatrist said there are two reasons people do drugs: chasing pleasure or running from pain.

I need to find a way to expose myself to the animal impulses of my unconscious.
I dream I am a werewolf running through a field.
I feel the region of my brain responsible for peace suddenly unlock.
You glide faster on all fours.
It is ineffable.

The past, present, and future exist simultaneously.
Sometimes when I think of my life, it feels like intangible memory.
The triviality is striking.

In nature opposites seek one another.
Jung says although man and woman unite, they remain irreconcilable opposites.
I still believe being in love is the best elation attainable.
Maybe when you love enough, you can see past the hostility.

Apollo and Dionysus represent two sides of a coin, like Hermes and Aphrodite — like you and I.
The visions haven’t left me. They are just drowning among my other experiences.

Never Trust Anyone Who Listens to Nocturnes in the Morning
By Dozie Ubosi

The nocturnes were my morning and night and this turned me nocturnal.
They are meant for the night, so listening in the morning is unnatural — but I did anyway.
The melodies were relaxing. I was obsessed with nighttime.
After months, I couldn’t sleep.
Everything sweet has its hidden bitter side.
Now I only listen to nocturnes at night.

I Think It’s Because I Love You

It seems like I am the one making the decisions
But I am reacting to everything you do
I think it is because I love you

How can you love me when you do not know anything about me
I do not know
It cannot be explained
It is quantum physics or something

Where are you
I will pick you up and explain everything
Nowhere near you
I am sure about that

Einstein is not a genius
He is a violinist

Tesla’s a Freak

An unhealthy obsession with completion is my problem
If I start something
I have to finish it
It helped me escape the perils of nature

Nikola Tesla finished reading all of Voltaire
Even I am not that obsessed
My obsession with completion led me to the greatest joy of my life
I will never be afraid to go too far again

Have You Heard of Voltaire

Taylor Swift is the Einstein the Schrodinger the Hilbert and the Tesla of our day
Wait so Tesla and Taylor Swif
I can explain it to you
Do not worry

Where are you
I will pick you up
Nowhere near you
I am sure about that

A physics professor walks out of her office
Holds a bag of chocolates open in front of me
There are different colours in there
I close my eyes and pick one
It is purple
My first psychedelic

Trust Me I Can Explain It To You

I always swear I can prove the theorems I blabber
But it only makes sense in my head

Pythagorean theorem seems easy to prove
But the proof is not so straightforward

Intuition beats rigour
Rigour validates the idea

Trust me
I can explain it to you

A Quote About the Devil Making Surfaces

On the first day of class I learned my teacher was a python
He said I am a Bayesian
And now time for something completely new as they say

That is a terrible attempt at a Monty Python joke
It is now time for something completely different

He held in his laugh
I asked who is your favourite member
He said Michael Palin
Mine is Eric Idle

Have you seen the architect sketch
No
At that moment I knew he was a freemason

All a Boy Wants Is a Working Machine

I never had a working machine until last year
I took a leadership role to build one
Being a leader is hard and uncomfortable
It is rewarding in the end
I built a working machine
It even had a fail safe

Professor Hypnosis

If you think of your brain as a signal processor
Schizophrenia is a glitch in the computer

Sir I have looked forward to this signal processing class since first year
I am basically telling the professor I am a skitzo
He said avoid aliasing

Dark Academia

I am a dead poet
Stomping on table tops
Captain my captain

I look at a Kurt Schwitters painting
I am scared to death
I hate decadent art
That is why they censored Baudelaire

Misogyny

If Picasso was a known misogynist
And Stravinsky wrote a rite about a virgin sacrificed to the gods
Do I pay the price for the misogyny of the past

The trailblazers were misogynists
What separates my genius from theirs

Nocturnes

You are the last person I think about before I sleep
I imagined our whole lives together
But here I am
A lifetime away from you

My hands are buried in the soil
Inside my soul
My love wears forbidden colours

Toronto Symphony Orchestra in the Winter

I have rarely seen something so beautiful
You are the exception of course

Stochastic events lined up like a miracle
I met the love of my life through random paths I somehow controlled

Slicing time is a science
It is an art like anything else

Solo Hikes

I had my first daydream about you on a trail
It happened randomly
One cold morning in November I woke to catch the sunrise
The thought of you kept me warm
As though you were there

Handel’s Messiah or Mozart’s Requiem

You might go to Messiah
I got you tickets for the Requiem if you change your mind

Count your dollars on the train
It will be a party
I cannot accept a penny for my thoughts
My thoughts are worth millions

Arrested for Chasing the Green Light

I left my house early
Stopped for scones
Hiked all day
Sleep deprived
Hallucinating

I offered a taxi driver scones for a ride to the airport
He asked for cash

I found a building with a green light
Green is my favourite colour
I trespassed
Signed in under a new name
The cops came

They took me to a hospital
One of them called me brilliant

How Do You Think Legends Are Made

I choose to maximize my youth
Belief does not make things exist

Why shift focus to the supernatural
As though this life is not everything I have

Joys and dangers follow this way of thought
There is risk in everything
Even thieves take risks
I think I found my kung fu

We’re Minimalists Aren’t We

Watching reruns
Wearing old clothes
Little furniture
We are minimalists
Aren’t we

Can We At Least Embrace Surrealism

The pursuit of pleasure is paramount
Because pleasure feels good
Find something you love and push
Push the door open and discover
Through discovery you create

Your Texts Have Me Stressing

Let us do a rendezvous before the rendezvous
I will pick you up in sweater and button up drip
It makes little sense on the surface
It is abstract for me

Words can carry deeper meaning
That is why they censored Baudelaire

Frogs Are Friendly Neutral

I saw a repulsive frog on my hike
It made me think of you
It cannot be explained

It takes a genius to understand that it cannot be explained
It must be felt

I have to do it when and how I feel it
Candles burn for only so long
I do not choose when my fire dwindles

Trace the Sinusoids

Trace the sinusoids
You will find no beginning
You will find no end

You will find a fragment of a vast fractal
Patterns are not illusions
Determinism is law
Stochasticity is the limit of perception

Nowhere Near You I’m Sure About That

I listen to certain songs to feel the feeling of falling in love again
They are the closest I may get to the warmth of love

Songs like
Erik Satie Gymnopedie
Sergei Rachmaninoff Piano Concerto in minor Adagio sostenuto
Claude Debussy Clair de Lune
They bring me near to that warmth

Dreams and Reality as Spaces

Have you had a dream that felt senseless
Meaning out of reach does not mean no meaning

I propose that dreams have a one to one linear relation with reality
Represented in a denser randomized topological space

Reality as a Hilbert Space
Treat reality as a structured space
Events and perceptions as states
Physical experiences memories sensory inputs cognitive processes

Dreams as a Homeomorphic Space
Dreams reside in a space homeomorphic to reality
They are transformations of real experiences
Distorted projections with an underlying connection

Infinitesimal Generators as Cognitive Processes
An infinitesimal generator produces transformations between spaces
Memory imagination and subconscious association evolve states from reality to dreams
Dreams are modified reflections of reality

Homeomorphism and Continuity
Dreams and reality are topologically similar
Continuity of the transformation implies systematic relation
Emotional weighting recombination of memories symbolic transformations

Conclusion
Dreams are transformations of real experience through subconscious processes
Governed by infinitesimal changes in cognitive evolution
Linked to reality through a homeomorphic relation
They preserve structure while altering form into denser symbolic topology

Attention and Relay Networks

I sit in my dark room as the cold creeps in
Information and meaning are not the same
More information can yield less meaning

Attention yields meaning
But attention collapses when information is astronomical

I explore mental states connected by context through relay networks
When people speak
They phase align through shared contexts
Model minds as signals through a channel that preserves and filters recurrences
Meaning arises from how coherences interfere in a shared field

In a saturated world
Meaning emerges from the scarce relay of attention
Minds are oscillators in a larger field
When profiles meet through a relay
Their interference cannot be decomposed into personal parts
The latent trace is joint and inseparable
The world clusters us into resonance domains

Extend from pairs to many agents
Collective fields arise from superposed oscillators
Cross spectral matrices coherence graphs and latent inseparability reveal clustering
Joint traces are collective

Coherence is a property of fields not individuals
Interference terms and cross spectral densities are irreducibly joint
Interactive systems are relays that filter amplify and suppress
They shape patterns that decide which communities form and which voices fade

Attention is redistributed by these systems
They become fields of coherence
Design determines stability fairness and balance between synchrony and fragmentation
Echo chambers lock groups into rigid synchrony
Better relays open new domains of resonance
The direction depends on how we tune the relays

On Vampires Religion and the Will to Live Forever

Living forever is an enticing promise
Vampires hold their place in culture because one lifetime feels small

Eternal life often mirrors earthly life
The mind cannot picture beyond the known
The idea persists as an extension of earth

The Faustian bargain and wagers of faith frame existence as a payoff game
Should an existence be based on an economic game
Desire often hides as greed

Religion can enable bad faith and philosophical escape
Accepting no life after death is a heavy selfless choice
It faces the spirit of nothingness directly

Nietzsche offers a path where others outline the problem
The overman is responsibility without borrowed authority
Not manipulation of rewards and punishments
The tightrope walker shows the risk
Failure is likely
Yet striving is necessary

Forge your purpose
Refuse templates
It is a hard path
It is selfless
It affirms life

First Meeting

I saw you with Shostakovich
You told him a dark secret
What can you tell me
It is best we meet

Why Don’t We All

The first time I held your hand
I felt like a child spun in a circle
You were the sage
Marking the border between joy and self destruction
Fire is the substance
I want it in abundance

Rebellious Youth

It is summer and I am home
I landed a research job at the university
I can see you twice a week

Leaning on the hood of the car
You feel like a breath from an inhaler
You sound like strawberry fields forever
Let us drive

Right Off the Freeway

Park and step out
I have not seen you in days

When we hold hands I am with you when you are gone
I coax you out of your shell so you are softer when it matters
I find better ways to get the most out of you

I do not need to eat when I know I will see you
I am not hungry
You excite me chemically

Making Me Creative

You are the substance
The sage who shows me my mind

Why do ideas flood when I am speeding
I do not party much now
The party is in my head
Boring sometimes
A party nevertheless
Shining with other clairvoyants is cool

Passive Meditations

The Bhagavad Gita is the greatest book on detachment
Detachment is cool so long as I am not detached from you my love
That I cannot accept

Twice a week is little
It is burdensome
I will see you briefly before the gym

Fear for the body harms the soul
One must suffer
I choose my body

Quiet Vows

It is silly to broadcast our love
I like it when it is just us

I can anthropomorphize you as I like
Cerebrotonia or somatonia
What is the catch

Is existence so precious that I cast away enjoyment for the sake of existence alone

Against the Wind

How do we hold hands and move with the wind when the wind grows harsh
You are not an indoor person

Maybe if I loop a summer interlude it will work
But I already milked your love through the fall
Winter came suddenly
I cannot do this every year

April Is the Cruelest Month

The weather warms
The air thickens
My skin glistens
Of course I care for another

The earth blooms again
The cycle restarts
Just like the first time

Bacchanal Doors

The flow of unbridled passions lives in these doors
Pay for your ecstasy
It was not fun in the moment
Reflection breeds fondness

It is not like you never tried
Look back and say I tried

Bloodlust and violent leanings live here
A portal to the bacchanal without end
The gods reach hysteria through laughter
We swarm like particles toward the unknown

Light Beams and Symmetry

Beams harden into symmetry
I stare as I drive past lights
Double aspect theory says my soul is on these streets
My body is far removed
Lines penetrate both eyelids
I cannot look away

Broken Textures

In a land of broken textures I leave to live
Wandering the barren streets
Cooking an appetite for self destruction

What are you hungry for mate
He opens a comically large coat
Black out

I wake near noon
Seeking transcendence in old texts

Chasm of Fantasy

Death is in my pocket
Worms eat through the fabric
Boots stomp the sludge
I feel elemental

Quantum Confinement

Sit there so I can look into your eyes
This moment is ours

We were blasted from the electron gun
Now we sit in a world of probabilities
Overwhelming

Our interference cannot be decoupled

Vulgar Nocturne

The sun sets with your blinds open
The end of burning desperation

Another glass of water
Conversations with yourself

Motivation peaks as the day dies
Summer will end
Another winter to harness potential

What will we make of today
Dreams live in the land of tomorrow
And tomorrow never knows

Birds Don’t Dream of Stars

It is bleak to be confined to the ground
There is more above us

I think it should be pronounced grave ity
Because of the grave nature of it all

Mental Soup

Ideas cook each day
In a viscous concoction

What a privilege
To receive such violence

Disinformation and entertainment
Butter and toast

Nausea

A blood curdling longing
Falling for a while
Head throbbing

One day you will enter the land where you belong

Clairvoyance

The upsetting dream stalks the night
The day carries its baggage

This is a march toward the cracked door of fate
Cue the marching band

An aerial view of the city clarifies it
Another Faustian hominid unfazed by ambition
Onward and upward to the stars

Half-Mad Sadistic Magician

In the basement of the institute
Machines of loving grace oscillate

The fear of effort is the fear of knowing
A tug of war with death
Hope is the rope

Markov Chains Time and The Robotic State

Mathematicians assume Markov chains remain hidden
Consider Hilbert spaces and relativity abstractly
Reconsider the infinitesimally close situation of a robot’s motion
The seconds that doctor human time may not suit the robot model

Step into the robotic state
It is a world of mathematics and philosophy

Markov models predict the future using present and past values
If the past repeats and remains pertinent
The robot stagnates
Flux between states that do not move it toward or away from its goal

Lead a horse to water
It may not drink
The horse must like you

To dissect this agenda we meet at the intersection of probability and differential equations
We test the Gaussian and its limits

Glossary
Markov process — a stochastic process deciding a sequence of events
Gaussian distribution — a continuous probability distribution
Stochasticity — randomness as nature
Determinism — events by fate
Bayesianism — events by probability

With stochasticity at the core of hidden models and determinism a strong view
We must choose whether to starve or flood neural networks with data
Accept waiting as a side effect
Keep morale with mid scale performance
Or gamble and risk burnout chasing uncertain scaling

The Gaussian assumes symmetry around a mean
When assumptions shift
Frequency and intensity warp
Processing time delays or does not
Cause and effect bend across perspectives

Non standard distributions remind us
The model and the world are not always in sync

The debate is not binary
It reflects temperament and worldview
As a wise voice said
I am a Bayesian until further notice

 Early Influences on Nietzsche’s Aesthetics
The Birth of Tragedy was first published in 1872, when Nietzsche was a professor of classical philology at the University of Basel. At the time, it was a work of striking originality in both philosophical aesthetics and classical scholarship (although it was not enthusiastically received in either field). Original though it may be in many regards, The Birth was also written under a wide array of influences. Any discussion of Nietzsche’s debut work, and by extension any consideration of his aesthetic thought as a whole, must begin with at least a brief discussion of these influences. What follows is a cursory treatment of three of the most important: Arthur Schopenhauer (1788–1860), Richard Wagner (1813–1883), and Friedrich Schiller (1759–1805). This examination will provide important context for understanding Nietzsche’s programme in aesthetics.

1.1 Arthur Schopenhauer
The best-known philosophical influence on the early Nietzsche is Arthur Schopenhauer, whom Nietzsche first read in 1865. Schopenhauer’s metaphysics, ethics, and aesthetics all had a profound effect on the young philologist. This metaphysics sets out from an acceptance of Kant’s distinction between appearances (or the ‘World as Representation,’ as Schopenhauer styles it) and things-in-themselves. Unlike Kant, however, Schopenhauer offers a positive account of the nature of noumenal reality. The account holds that the inner essence of the world is a single, timeless, quasi-volitional “striving,” or “Will.” Schopenhauer’s basic metaphysical position thus consists of two main theses:

Transcendental Idealism: Space and time are subjective properties of the mind, and not properties of the world as it is in-itself.

Will-Monism: Reality in-itself is a single being that is characterized essentially by quasi-dynamic or conative properties (termed ‘Will’).

Will-Monism, together with a broadly Epicurean theory maintaining that willing is a source of ceaseless suffering, underwrites a key descriptive claim of Schopenhauer’s philosophy: human life necessarily involves more suffering than pleasure or happiness. From this descriptive claim, Schopenhauer infers his major normative thesis, the doctrine of philosophical pessimism: life is not worth living, or, the world’s non-existence would be preferable to its existence (for an excellent in-depth treatment of Schopenhauer’s arguments for pessimism, see Simmons 2024). Inspired by (certain versions of) Christianity and Buddhism, Schopenhauer holds the appropriate response to this pessimistic truth to be “denial of the will to life,” understood as the ascetic repudiation of desire. Schopenhauer’s aesthetics is best understood against the backdrop of this pessimistic ethics and its attendant metaphysics.

For our purposes, the most important claim of Schopenhauer’s aesthetics is that the pleasure we take in art (and beauty more generally) is a disinterested one. When confronted with a beautiful object, Schopenhauer thinks, our consciousness becomes possessed, so to speak, by its purely objective properties, focusing simply on what the object itself is (or the “Platonic idea” it expresses) rather than on its relations to other objects. This means, amongst other things, that aesthetic appreciation ignores the relations the object has to our will, i.e. how it conduces to or impedes the satisfaction of our subjective interests. Schopenhauer puts a peculiar twist on this relatively common modern conception of aesthetic appreciation by conceiving of such enjoyment as a silencing of the will. Since willing, for Schopenhauer, is a—indeed the—source of suffering, aesthetic pleasure constitutes a relief from the burden of our existence. Aesthetic experience, then, resembles and, so to speak, presages salvation from the world and its ills, which is to be attained through asceticism (see, e.g., Shapshay 2024). Though such will-lessness is a feature of all aesthetic appreciation, the negation of the will is overtly thematized in tragedy, and Schopenhauer accordingly reserves particularly high praise for this artform. Tragedy makes “palpable the futility of human striving and the nothingness of this whole existence in a great and striking example, thus revealing the deepest meaning of life; this is why tragedy is acknowledged to be the most sublime form of literature” (SW 3:730–31/WWR 2:651). It is Schopenhauer’s own pessimistic philosophy, then, which constitutes the ultimate content of the tragic genre.

The early Nietzsche took much from Schopenhauer. The Birth of Tragedy appears to operate under the assumption that Schopenhauer’s descriptive claim about suffering is true. Nietzsche also officially endorses Schopenhauer’s basic picture of aesthetic enjoyment as essentially will-less (see, e.g., BT 5). Schopenhauer’s metaphysical distinction between representation and will is also evidently taken on board, as is his theory of music, according to which music, unlike the other arts, is a direct “copy” the noumenal will. Matters may not be quite this straightforward, however. Some scholars have noted divergences between The Birth’s and Schopenhauer’s respective theories of music (Janaway 1998; Vandenabeele 2003). And the question of whether The Birth accepts Schopenhauer’s metaphysics has been a subject of considerable dispute in recent years (see §2.2 below). Whatever should be said about The Birth, Nietzsche’s later aesthetics is clearly set up in opposition to Schopenhauer’s. In works such as Beyond Good and Evil (1886), On the Genealogy of Morals (1887), and Twilight of the Idols (1888), Nietzsche rejects Schopenhauer’s conception of disinterested aesthetic appreciation, as well as his more general attempt to connect aesthetic experience to the ethics of pessimism. Schopenhauer remains an important foil against which Nietzsche formulates his own aesthetic position and, precisely for this reason, his influence is never entirely absent.

1.2 Richard Wagner and German Romanticism
While Schopenhauer’s influence on the early Nietzsche was crucial, it would be misleading to suggest that it was entirely overriding. Of perhaps equal importance was the influence of the composer and librettist Richard Wagner (1813–1883). Nietzsche knew and was on friendly, even intimate, terms with Wagner during the planning and construction of the Festspielhaus in Bayreuth. During this period (roughly 1869–1876), Nietzsche was profoundly impressed not only with Wagner’s artistic achievements and plans, but also with Wagner’s attempts to develop a theoretical framework for his artistic project. By the time the two had become acquainted, Wagner was himself a convert to Schopenhauer’s philosophy, and was especially taken with Schopenhauer’s theory of music (Wagner would expound a somewhat idiosyncratic version of this theory in Beethoven [1870]). Yet, Wagner’s aesthetic thought—contained in a voluminous corpus of difficult theoretical writings produced over the course of four decades—was quite protean, and owes at least as much to the older tradition of German Romanticism as it does to Schopenhauer. Two elements of Wagner’s philosophy are worth emphasizing here: his conception of the “total work of art”, and his vision of a “new mythology.”

With ancient Greek drama as his model, Wagner advocated, and hoped himself to (re)create, “total,” “complete,” or “collected” works of art (Gesamtkunstwerke). Greek drama, and Wagner’s own Musikdrama, are supposed to be “collected” in the sense that they constitute a synthesis of distinct artforms—music, poetry, mime, dance, etc. (see 1850, 186–87). Yet they are also “collected” in another, more social sense: Greek dramas were, and Wagner’s own Musikdramen would be, religious festivals where the whole community could gather and share in a common experience of profound significance. Though present in the earlier Romantic tradition, the idea that aesthetic experience should not be coolly critical, but instead something almost sacramental, is an especially pronounced and consistent theme in Wagner’s prose writings. This thought had a decisive influence on the early Nietzsche. One of his main aims in The Birth of Tragedy is to take seriously the fact that Greek tragedies were performed as part of religious festivals, and thus to show that the modern tendency to view them as “mere” literature is badly misguided.

The need for an artform that can perform this sort of sacramental role arises from what Wagner, sounding a common refrain of post-Kantian thought, takes to be a chief malady of modernity. Modern culture has become increasingly specialized, solipsistic, and crudely materialistic. As a result, we find ourselves profoundly alienated—from one another, from nature, even from ourselves. Adding to the insult, art itself has become commodified and serves the purpose of mere pleasant diversion, rather than answering to a “true need” (1850, 12). One such need is the need for “unity,” or, as Novalis famously put it, “the drive to be at home everywhere” (Werke, 491). Wagner believed that art could answer this need by serving as a vessel for a new mythology. By the time he was writing, this was an established theme in Romantic thought—one which was given special prominence by the Schlegel brothers, Schelling, and others. The idea was roughly that mythologies provide a synoptic worldview which, unlike that of philosophy or the natural sciences, speaks directly to the senses and the imagination. Because it does not require any specialist knowledge to be understood, mythology is uniquely capable of speaking to a community as a whole. As Wagner expresses it, “the work of art is the living presentation of religion––but religions are not invented by the artist, they emerge only from the folk (Volk)” (1850, 36).

The Birth of Tragedy’s debt to this Wagnerian vision is unmistakable. But Nietzsche would soon become disillusioned, and by the time of his last writings his attitude toward Wagner is overwhelmingly, even irrationally hostile. Whether this means Nietzsche jettisoned his early Wagnerian ideals, or whether he only felt Wagner, the man and the artist, failed to live up to them, is difficult to say (see Gemes 2022, 33). What is certain is that Wagner and his art would remain central concerns of Nietzsche’s for the duration of his career—so much so that he would devote two full books to criticizing Wagner in 1888 (The Case of Wagner and Nietzsche contra Wagner).

The influence of German Romanticism on the early Nietzsche came from other quarters too. Of particular importance is A.W. Schlegel, whose Lectures on Dramatic Art and Literature (Schlegel VKL) Nietzsche read carefully prior to writing The Birth of Tragedy (see NF 1869: 1[85]–1[105]). Schlegel’s conception of the tragic chorus and his interpretation of Greek mythology influenced Nietzsche’s presentation in The Birth, and many of Nietzsche’s notorious broadsides against Euripides are prefigured in the Lectures (though both are likely drawing on an older critical tradition as well). Also of importance was the poet Friedrich Hölderlin (1770–1843), a favorite of Nietzsche’s as a schoolboy and one possible source for his famous distinction between the Apollonian and the Dionysian (Young 2013, 96–98). Studies of Nietzsche’s intellectual and personal connections to Wagner include Köhler 1998; Young 2010, 105–34, 2014, 131–40; and Scruton 2014. For recent treatments of Nietzsche and the wider romantic tradition, see Williamson 2004, 234–84, Ameriks 2012, and Gemes 2023.

1.3 Friedrich Schiller and German Classicism
A third critical influence on Nietzsche’s early aesthetics was the German poet, dramatist, historian, and philosopher Friedrich Schiller. That Nietzsche felt an affinity with Schiller at this period of his career is hardly surprising—Schiller was generally recognized as a “German Shakespeare”, a consummate master of the artform that most interested the young philology profressor; Schiller’s theoretical works were amongst the most important sources of inspiration for the younger Romantic generation that had in turn inspired Wagner’s thought; and Schopenhauer and German pessimists also often enlisted Schiller as a key predecessor (Beiser 2018). Above all, the period of artistic collaboration between Schiller and Goethe, the extraordinary era of Weimar Classicism, seemed to hint at the true creative potential of the “German spirit”—potential which Nietzsche thought was being squandered on staid academic specialism and crude, militaristic nationalism.

Like his successors in the Romantic tradition, Schiller thought that the ancient world was characterized by a now-lost sense of unity and harmony. This issue is thematized in his influential treatises On the Aesthetic Education of Man in a Series of Letters (1795) and On Naïve and Sentimental Poetry (1795/6). In the latter, Schiller distinguishes between two novel types of aesthetic response to nature and art, the titular naïve and sentimental. Cultures like Homeric and Classical Greece typically produce works of naïve art. Such art expresses their easy oneness with themselves and nature, a state in which the human being “operates as an undivided sensuous unity, as a harmonizing whole” (NA 20:436). Sentimental art, by contrast, expresses our longing for this bygone time when “we were happy and perfect” (NA 20:427). In other words, it presents such harmony, not as a fact, but as an ideal, and is thus the art proper to the modern world, in which the two sides of our nature—reason and feeling—have been divided and set against one another. The distinction between naïve and sentimental seems not only to have been the inspiration for the slightly later distinction between Classical and Romantic art (see Eckermann 1837 [1981, 379–80]), but possibly also for Nietzsche’s Apollonian-Dionysian duality (Kalar 2008; cf. BT 2–3; NF 1870: 7[126]). Crucially, however, Nietzsche does not see the ancient world as purely “naïve.” The Greeks’ naïveté was not a natural endowment, but an achievement—a coping mechanism developed in response to a world of unspeakable cruelty (see e.g. letter to Rohde 16 July, 1872 [SB 1872: 239]). Moreover, Nietzsche finds a degree of sentimentality already characteristic of the Greek outlook, in particular of the Dionysian mysteries (BT 2/KSA 1:33).

Thus, while Nietzsche never accepted Schiller’s conception of the Greeks wholesale, that conception significantly influenced his interpretation of the “Apollonian” element of Greek culture. This influence is evident in other ways as well. In the Aesthetic Letters, Schiller develops an influential notion of “beautiful semblance” (schöner Schein) (see esp. NA 20:398–405). Such semblances are distinctive types of illusions produced by works of fine art: they are paradigmatically non-deceptive and enjoyed for their own sake. The phrase ‘beautiful semblance’ plays on the double meaning of the German word Schein, which can signify both “semblance,” “illusion,” “appearance,” as well as “gleam,” “brilliance,” “radiance.” Schiller’s point is that aesthetic appreciation involves taking joy in the mere look of things without being concerned with what they are, though also without confusing appearance and reality. We recognize, for example, that the appearance of three-dimensionality on the flat surface of the canvas is a mere illusion, and this is part of what we enjoy. Nietzsche would make this sense of beautiful semblance central to his account of both Apollonian art and culture in The Birth of Tragedy (Stoll 2019).

Between 1792 and 1803, Schiller also wrote extensively about tragedy. His essay “On the Use of the Chorus in Tragedy,” prefigures a central claim of The Birth—that the tragic chorus is essential to the proper experience of tragedy. The chorus, he argues, allows the spectators of a tragedy to adopt a “distanced” perspective on the drama and to view it as an idealized product of the poet’s imagination rather than a crude imitation of reality (see Stoll 2022 for further discussion of this view). In both his lectures on Sophocles and in The Birth of Tragedy itself, Nietzsche explicitly endorses Schiller’s account of the chorus (KGA II.3, 25–27; BT 7/KSA 1:54–55). Nor could he ignore Schiller’s remark in a letter to Goethe that “I have always had a certain faith in the opera—that tragedy would develop out of it into a nobler form, just as it did from the choruses of the ancient Bacchic festivals” (NA 29:179; cf. NF 1871: 9[83]). Of Schiller’s account of tragic pleasure, however, he is more critical. In essays such as “On the Ground of the Enjoyment in Tragic Objects” and “On the Sublime,” Schiller argues that the pleasure we take in tragedy is a form of the sublime. Tragic heroes, he suggests, paradigmatically sacrifice their well-being or even their lives for the sake of some higher principle (the Antigone of Sophocles’ eponymous play is a good example). And this somehow makes the audience aware of a similar power in themselves, which Schiller takes to be the Kantian faculty of transcendental freedom. It is this awareness that constitutes our pleasure in tragedy. Schiller’s strategy is thus to apply to tragedy Kant’s notion of the “dynamically sublime,” according to which a “counter-purposive” object can give us a feeling of ennoblement by stimulating awareness of the power of our moral faculties. In The Birth, Nietzsche does suggest that tragic pleasure is a species of the sublime (BT 7/KSA 1:57), though he explicitly refuses an explanation in terms of “the morally sublime” (BT 24; for comment, see Raymond 2014).

As with his attitude toward Schopenhauer and Wagner, Nietzsche’s feelings toward Schiller would sour in his later years. During these years there is little explicit engagement with Schiller, and when his name is mentioned, he is dismissed as a “moral trumpeter” (TI “Skirmishes” 1). However, a number of scholars have made the case that elements of Schiller’s thought, and the broader German Classicist tradition to which it belongs, continue to exercise considerable influence on Nietzsche beyond The Birth of Tragedy (see Martin 1996; Bishop & Stephenson 2005; Katsafanas 2011; Stoll 2019; and Lichtenstein 2019). Noteworthy is that Nietzsche’s emphasis on the importance of artistic Schein would continue into his later works, as would his positively valanced use the term ‘naïve’ in reference to members of the ancient nobility (GM I.10/KSA 5:272, GM II.7/KSA 5:304).

2. The Birth of Tragedy
2.1 The Problem of The Birth of Tragedy
The Birth of Tragedy is, in the first instance, an attempt to offer a philosophical theory of the nature and value of tragedy, and in this sense it belongs to a tradition stretching from Aristotle’s Poetics to Hegel’s Aesthetics lectures and beyond. Two questions have tended to orient this tradition: (1) in what does the value of the experience of tragedy consist (if indeed it is valuable at all)? (2) what explains the pleasure we take in watching tragedies? (1) owes its urgency to Plato’s argument in Republic X that tragedy corrupts its viewers. (2) is motivated by the familiar but paradoxical fact that the pleasure we get from tragedy depends on emotional responses—pity, fear, anxiety, and so forth—that are intrinsically unpleasant. Predictably, The Birth does attempt to answer both questions ((1) is a constant theme; for (2) see esp. BT 22/KSA 1:141–44, BT 24/KSA 1:152–53). But, surprisingly enough, neither is really its main animating problem. Nietzsche’s chief goal in the book is rather to suggest that there is a profound existential problem facing humanity, and to argue that tragedy (or, more specifically, Attic tragedy and its supposed reincarnation in the Wagnerian Musikdrama) is the only way of solving it. His answers to (1) and (2) then fall out of this more general argument, rather than constituting his philosophical starting point.

What, then, is that problem? The natural suggestion is to locate it in Schopenhauer’s pessimism (e.g. Young 1992, 26–9; Young 2006, 14–15; Soll 1990, 1998; Came 2005). Schopenhauer’s pessimism combines the descriptive thesis that life necessarily involves more suffering than happiness, with the normative thesis that life is therefore not worth living, or that non-existence is preferable to existence. That this is Nietzsche’s main concern is strongly suggested by his repeated references to the so-called “wisdom of Silenus” (BT 3/ KSA 1:35, BT 4/KSA 1:39–41, BT 7/KSA 1:57, BT 24/KSA 1:151). This “wisdom”—which is reported in a fragment of a lost dialogue of Aristotle’s (see F 44 R3 [Aristotle CW, 2401–2]), and alluded to in Sophocles (see Oedipus at Colonus, 1211)—is that “the best of all things for you is entirely unattainable: never to have been born, not to be, to be nothing. But the second best is for you—to die soon” (BT 3/KSA 1:35). Nietzsche, on this story, accepts the descriptive thesis of pessimism, and he also believes the Greeks of the 6th and 5th centuries BC recognized its truth. The question, then, is how to respond to the “terrors and horrors of existence” (BT 3/KSA 1:35); the best answer is contained in Greek tragedy (specifically, the tragedies of Aeschylus and Sophocles). One might suppose that another, perhaps preferable, way to phrase the problem would be in terms of how to overcome the “terror and horror of existence,” or how to accept the descriptive thesis of pessimism while rejecting Silenus’ (and Schopenhauer’s) verdict on the value of human life. But, for reasons canvassed below (see §2.3), some scholars believe that The Birth remains committed in some sense to Schopenhauer’s negative verdict on the value of life. What is important for now is simply that, on the present proposal, Nietzsche views suffering—in particular, its predominance and inextricability—as the main problem facing us.

A competing proposal instead views lack of meaning—in particular, the lack of meaning-conferring cultural institutions—as the main problem to which Nietzsche addresses himself, and not suffering per se (Gemes & Sykes 2014, 2015; cf. Tanner 2000, 8). Gemes and Sykes, e.g., read Nietzsche’s project in The Birth as inspired more by Wagner and the Romantic tradition than by Schopenhauer (Gemes & Sykes 2014, 100–101). As noted above, that tradition saw modernity as characterized by a kind of “homelessness”—a growing sense that we no longer have a place of “belonging” in the world. On this reading, Nietzsche is animated mainly by the same sorts of concerns. And his aim in The Birth is to show how the tragic festival can help us regain our lost sense of belonging by providing our culture with an overarching, unifying “mythic” worldview. Thus, remarking critically on the contemporary state of German culture, Nietzsche says that “without myth, every culture forfeits its healthy, creative natural power; only a horizon encircled by myth brings a whole cultural movement to a unity” (BT 23/KSA 1:145).

There is ample evidence in the text that Nietzsche is concerned both with the problem of suffering and the problem of modern cultural fragmentation. One need not place exclusive emphasis on either problem, and they can even be seen as interrelated (see Huddleston 2019, 13–14). The feeling that one’s life lacks some sought for place of belonging may itself be the cause of great psychological suffering. On the other hand, the ordinary suffering endemic to life might be easier to bear if one feels that we are “in it together,” so to speak, and this may prompt one to seek such meaning. It is possible to see an even tighter and more general connection between the two problems. For, arguably, Schopenhauer’s pessimistic conclusion only follows from his thesis that life is inescapably full of suffering together with his rejection of traditional narratives about its meaning. The Christian, for example, may admit that suffering is a problem, but still conclude that life is worth living insofar as it achieves something of value (e.g., an atonement for original sin). The Birth of Tragedy might then be read as occupied neither with the problem of suffering nor with the problem of meaning as such, but with the problem of meaningless suffering (Came 2022, 44–45). Even if the two issues can be seen as related in this way, there is still a question of emphasis. For, it may make a difference to our interpretation of The Birth whether we see Nietzsche’s main interests as stemming from the Schopenhauerian question about the justification of suffering, or from the Romantic concern with cultural flourishing.

2.2 The Metaphysics of The Birth of Tragedy
The most famous aspect of The Birth of Tragedy is without doubt Nietzsche’s distinction between the “Apollonian” and the “Dionysian.” These terms are first introduced (BT 1, 2) as names for two “art drives” (Kunsttriebe) which are originally operative in nature, and then derivatively associated with distinct artistic paradigms. The paradigmatically representational arts that deal in “forms” (painting, sculpture, epic poetry) are associated with Apollo, whereas the paradigmatically non-representational, or more typically “expressive” arts (music and lyric poetry) are associated with Dionysus. One of Nietzsche’s main contentions in The Birth is that Greek tragedy is a unique hybrid artform that combines and unifies the Apollonian and Dionysian impulses. The former is manifested in the dramatic dialogue and action, while the latter finds expression in the musical accompaniment of the chorus.

Yet, in many places throughout the book, Nietzsche appears to accord the Apollonian/Dionysian duality a quite distinct, if distantly related, significance. In this second sense, ‘Apollonian’ and ‘Dionysian’ are ways of referring to two metaphysical sides of the world à la Schopenhauer. The merely apparent world of spatiotemporally discrete individuals, governed by causal laws, is associated with Apollo, whereas the non-spatiotemporal monistic reality underlying the appearances is associated with Dionysus. The clear impression is that Nietzsche thus commits himself to Schopenhauer’s Transcendental Idealism and Will-Monism, that his “Apollo and Dionysus are … simply Representation and Will in Greek costume” (Nussbaum 1999, 358).

Commentators divide over the question of how seriously Nietzsche intends the metaphysical associations his key terms sometimes carry, and indeed whether The Birth advances any metaphysical doctrine at all. The orthodox reading has been that “The Birth incorporates without modification Schopenhauer’s metaphysics” (Young 1992, 26; cf. Young 2006, 14; Silk & Stern 1981, 291; Geuss 2012; Soll 1998, 103; Clark 2015). This was, indeed, the view of Nietzsche’s closest friend and intellectual confidant at the time, Erwin Rohde (see Rohde 1872), and the evidence for it is clear and straightforward. Nietzsche calls Apollo the god of the principii individuationis (principle of individuation)—a scholastic term Schopenhauer uses to refer to space, time, and causality qua merely subjective principles of the mind (BT 1/KSA 1:28). Nietzsche repeatedly refers to a “primordial unity” (das Ur-Eine), “will” (Wille), or “world-will” (Weltwille) that he associates with Dionysus and which he apparently takes to transcend the empirical order (BT 1/KSA 1:24, 30, BT 3/KSA 1:37–38, BT 4/KSA 1:38–39, BT 5/KSA 1:43–44, BT 17/KSA 1:109, 112, BT 18/KSA 1:115, BT 21/KSA 1:135, BT 22/KSA 1:141). Closely paraphrasing a passage from The World as Will and Representation (SW 2:497–98/WWR 1:447), Nietzsche tells the reader that,

The tremendous courage and wisdom of Kant and Schopenhauer has achieved the most difficult victory, victory over the optimism which lies hidden in the essence of logic… . While the latter believed in the possibility of knowing and fathoming [die Erkennbarkeit und Ergründlichkeit] all riddles of the world … and treated space, time and causality as entirely unconditioned laws of the most universal validity, Kant revealed how these laws really only serve to raise mere appearance, the work of Maya, to the level of the sole and highest reality and to put it in place of the innermost and true essence of things, and in this way to make actual knowledge [Erkenntniss] of this essence impossible, i.e., as Schopenhauer puts it, to lull the dreamer ever deeper asleep. (BT 18/KSA 1:118)
The Birth of Tragedy seems, moreover, to endorse Schopenhauer’s theory of music (BT 5/KSA 1:46–7; BT 16/KSA 1:46–7, 104–7), according to which music is a direct “copy” of the noumenal Will. And, finally, this theory along with the metaphysical picture on which it relies appear to be directly implicated in Nietzsche’s account of tragedy’s ultimate lesson:

It is only out of the spirit of music that we understand a joy in the annihilation of the individual. For, the individual examples of such annihilation only make clear to us the eternal phenomenon of Dionysian art, which brings to expression the will in its omnipotence behind, so to speak, the principio individuationis, eternal life beyond all appearances and in spite of all annihilation. The metaphysical joy in tragedy is a translation of the instinctive unconscious Dionysian wisdom into the language of the image. (BT 16/KSA 1:108)
For all the idiosyncrasies of Nietzsche’s mode of expression, such passages do seem to suggest a straightforward acceptance and deployment of Schopenhauer’s core metaphysical ideas.

This received, metaphysical reading of The Birth is by no means the consensus view, however. Indeed, the general trajectory of recent scholarship has been toward rejecting it (Staten 1990, 187–216; Poellner 1998; Han-Pile 2006; Porter 2000; Gardner 2013, 603–6; Gemes & Sykes 2014; Mulhall 2014, 261–63; Daniels 2013, 69–71). The most plausible versions of this reading admit that The Birth contains numerous metaphysical claims, but hold that these are put forward in a self-consciously fictional register.

There are three key pieces of evidence for these anti-metaphysical readings of The Birth. The first is an early notebook entry (composed sometime between October 1867 and April 1868) entitled “On Schopenhauer” (see FS 3.352–61/WEN 1–8). This entry presents four criticisms of Schopenhauer’s identification of the thing-in-itself with the Will, and seems to conclude with a wholesale rejection of Schopenhauer’s metaphysics. One of the main points is that Schopenhauer’s adherence to Kant’s thesis that the world in-itself is unknowable vitiates his attempt to offer any positive account of its nature. If Nietzsche rejected the metaphysics of Will possibly as early as 1867, then he could not, it is supposed, have seriously intended to revive it in 1872.

The second piece of evidence is the following, surprising passage from The Birth of Tragedy itself:

It is an eternal phenomenon: the voracious will always finds a means, through an illusion that it spreads over things, to chain its creatures to life and to compel them to live on. One is captivated by the Socratic lust for knowledge and the delusion [Wahn] of being able to heal the eternal wound of existence through it; another is ensnared by art’s seductive veil of beauty that flutters before his eyes; and yet another by the metaphysical solace that eternal life flows on indestructibly beneath the whirl of appearances. (BT 18/KSA 1:115, emphasis added)
Note that the description of the “metaphysical solace” sounds remarkably close to the Schopenhauerian view that there is some single willing being underlying the apparent world of individuation. The passage appears to assert in no uncertain terms that this is only an illusion; it is presented as on a par with the “Apollonian” illusion that “makes human existence seem more beautiful than it really is” (Han-Pile 2006, 382), and the Socratic view, which Nietzsche rejects, that we can end human suffering by coming to understand its causes (BT 15/KSA 1:97–102).

The third datum for non-metaphysical readings is Nietzsche’s valorization throughout The Birth of “myth.” A major theme of Nietzsche’s early work, as noted above, is that shared mythologies are essential for the flourishing of a culture: Nietzsche’s emphasis on the importance of myth is not as such evidence against a metaphysical interpretation of the book, since the fact that he valorizes myth does not on its own show that he views his metaphysics as a myth. However, it does provide a plausible rationale for the otherwise baffling BT 18 (our second piece of evidence). The idea is that, in those places of The Birth where Nietzsche appears to advance a grand metaphysical theory, he is really trying to supply late 19th-century German culture with the sort of myth that he thinks it needs to flourish. His confession in §18 that that “theory” is an illusion is an indication to the reader that this is what is going on. The most developed version of this reading in found in Gemes & Sykes 2014, 2015.

How can proponents of the more orthodox metaphysical reading respond to these apparently compelling pieces of evidence? With respect to the first, some have suggested that, while Nietzsche did reject Schopenhauer’s identification of the thing-in-itself with the Will on the grounds that the former is completely unknowable, he still cleaved for a time to the basic dichotomy between a monistic Will and a spatiotemporal realm of discrete individuals. On this revised Schopenhauerian reading, the Will itself is degraded to the status of an appearance, but an appearance that “provides a deeper account of the world than its description in terms of material bodies… . ‘Will’ is, then, … a description of penultimate rather than ultimate reality” (Young 2010, 92; cf. Ridley 2007, 26). Julian Young has suggested that this sort of view in fact represents the position of the later Schopenhauer himself (2010, 92; 2005, 96–98), thus making The Birth’s metaphysical picture still genuinely Schopenhauerian. (Whether the later Schopenhauer actually degrades the Will to the status of an appearance in this way is, of course, controversial (see, e.g., Janaway 1999, 162–63; Özen 2021, 261–63).)

With respect to the second piece of evidence, it might be noted that what Nietzsche specifically refers to as an illusion in BT 18 is the “metaphysical solace,” and it may not be correct to identify this “solace” with the book’s metaphysics per se. Young (2006), e.g., reads the passage from BT 18 as claiming, not that the Schopenhauerian metaphysics of will is an illusion; it is rather the idea we get from Dionysian tragedy—that life is worth living in spite of its terrors—that is the illusion (15–16). It might also be observed that, interpreted as a straightforward rejection of that metaphysics, there is something especially peculiar about the passage from BT 18. Suppose that by ‘the metaphysical solace’ Nietzsche really does mean ‘the idea that there is a single metaphysical Will underlying all appearances.’ Read straightforwardly, then, the passage claims that the metaphysical Will creates the illusion of its own existence (this peculiarity is noted by Paul de Man (1979, 99–102) though he takes it as evidence that BT aims to metatextually call its own claims into question). This bizarre result would seem to suggest that we need a more nuanced reading of the passage.

There are not many direct attempts on behalf of orthodox readers to rebut the third piece of evidence (and, in general, there has been to date very little effort at developing a metaphysical reading of The Birth that countenances the above evidence). As noted already, however, the emphasis on the need for myth is not itself evidence that The Birth’s metaphysics is itself such a myth. This theme lends credence to the anti-metaphysical hypothesis only together with the other pieces of evidence. If proponents of the orthodoxy have convincing explanations for these countervailing texts, then perhaps they do not need to deal with directly with the issue of myth. (For a discussion of the role of myth in The Birth, which finds it compatible with taking the book’s metaphysical commitments seriously, see Young 2010, 130–31.)

2.3 Pessimism, Optimism, and the Aesthetic Justification of Existence
Interpreters diverge over the question of The Birth’s commitment to Schopenhauer’s pessimism just as they do over its commitment to his metaphysics. One natural view, supported especially by some of Nietzsche’s retrospective comments about The Birth, holds that the book contains a “complete rejection of the normative ethics of pessimism” (Nussbaum 1999, 362; cf. Kaufmann 1974, 131; Schacht 2001; Soll 1998, 100–101; Ridley 2019, 318–19). A competing view maintains that Nietzsche in fact “endorsed Schopenhauer’s inference from the pain and purposelessness of human existence to its worthlessness” (Young 1992, 26; cf. Young 2006, 15–16; Geuss 2012, 47, 61; Huddleston 2019, 15, 19). Predictably, decisive textual evidence in favor of either reading proves elusive. To be clear, the disagreement is not over whether Nietzsche accepts the descriptive pessimistic thesis that suffering is life’s fundamental constituent—it is generally agreed that the early Nietzsche is pessimistic in this sense. Rather, the disagreement concerns his answer to the normative question of whether life is, or is not, deserving of negation, whether it would be better not to be.

At first blush, the non-pessimistic reading is bound to appear the most plausible. This reading can rely on Nietzsche’s own later remarks about The Birth. Writing in 1888, for example, Nietzsche says that the book aimed to explain “how the Greeks got over pessimism—with what means they overcame it … tragedy is precisely a proof of the fact that the Greeks were not pessimists” (EH “Books,” BT:1). This sort of remark resonates with much of what Nietzsche says in The Birth itself. “The arts generally,” he claims there, “make life possible and worth living” (BT 1/KSA 1:27–28, emphasis added); or again, “art … alone is capable of turning those thoughts of disgust with the horror or absurdity of existence into representations with which one can live” (BT 7/KSA 1:57). And, of course, there is Nietzsche’s overriding claim in the book that “only as an aesthetic phenomenon is [sic] existence and the world eternally justified” (BT 5/KSA 1:47; cf. BT 24/KSA 1:152). If they are genuinely justified, it would seem that Schopenhauer’s verdict—that they ought not to be—cannot be correct.

Yet, The Birth does not speak unequivocally in favor of this “optimistic” reading. For example, Nietzsche also says that tragedy contains “a profound and pessimistic worldview,” and communicates to us “the conception of individuation as the primordial ground of evil, … the hope that the spell of individuation may be broken” (BT 10/KSA 1:73). That suffering arises from the pursuit of one’s individual interests and that salvation lies in freeing oneself from these interests by piercing the “veil” of individuation is, of course, precisely Schopenhauer’s position. This Schopenhauerian interpretation of tragedy is especially prominent in Nietzsche’s 1870 lecture course on The Tragedy of Sophocles, where he states bluntly “life appeared no longer as worth living. Tragedy is pessimistic” (KGA II.3, 10) and that Sophocles’ “teaching is unconditional submission and resignation” (KGA II.3, 27). Elsewhere he straightforwardly identifies “the content of the tragic myth” with “the wisdom of Silenus” (BT 24/KSA 1:151)—the view that the best thing for human beings is never to have been born. And, after telling us that tragedy aims at transfiguration, he clarifies that what it transfigures is “least of all the ‘reality’ of this world of appearance, for it says to us precisely: ‘Look! Look closely! This is your life! This is the hour hand on the clock of your existence!” (ibid.). These seem odd ways, to put it mildly, of expressing the idea that life is, in spite of it all, justified and worth living. Proponents of a pessimistic reading can also point to evidence that Nietzsche thinks both Apollonian art and tragedy “justify” existence only by means of illusions (e.g., BT 18/KSA 1:115–16). For if, as this may suggest, art helps us affirm life only by deceiving us about its quality, “this implies that in the fullness of knowledge one would not affirm life as worth living. It implies, more briefly, that life is not worth living” (Young 1992, 48).

There are, at least in principle, two distinct questions here: did Nietzsche endorse Schopenhauer’s verdict about the value of life? And, what does he think tragedy tells us (or what does he think it told the Greeks) about the value of life? It is possible that Nietzsche thought tragedy is life-affirming, but himself endorsed Schopenhauer’s life-denying stance. If this is right, however, Nietzsche’s position is highly unstable. Clearly, The Birth aims not only to describe the normative outlook contained in Greek tragedy, but also to, in some sense, advocate for it, and would therefore be committed to recommending an attitude toward life Nietzsche himself believes is unwarranted.

The evidence regarding Nietzsche’s attitude toward pessimism is therefore equivocal. How we are best to interpret it depends to a significant degree on how we interpret The Birth’s major thesis that tragedy offers an “aesthetic justification” of existence. The thesis appears to be that tragedy somehow encourages us to see the world—and the inescapable presence in it of suffering, struggle, death, etc.—from a god’s-eye perspective. From this perspective, the world as whole appears a terrible, but glorious spectacle, and our lives—short and tormented though they may be—have an aesthetic significance insofar as they occupy an integral place in this aesthetically valuable whole, like shadows in a beautiful painting or dissonances in a magnificent piece of music:

We may assume of ourselves that we are already images and artistic projections for [the world’s] true creator and have our highest dignity in our significance as works of art—for only as an aesthetic phenomenon is existence and the world eternally justified. At the same time, our consciousness of this significance of ours hardly differs from that which painted soldiers on a canvas have of the battle depicted on it. (BT 5/KSA 1:47)
Precisely the tragic myth has to convince us that the ugly and disharmonic is an artistic game, which the will, in the eternal fullness of its pleasure [Lust], plays with itself. This primordial phenomenon of Dionysian art, difficult to grasp, is made directly understandable and comprehended immediately only in the miraculous significance of musical dissonance: just as music in general, placed next to the world, can give us a concept of what is to be understood by the justification of the world as an aesthetic phenomenon. The pleasure caused by the tragic myth has the same home as the pleasurable sensation of dissonance in music. (BT 24/KSA 1:152)
In this respect, then, the lesson Nietzsche thinks tragedy teaches us closely resembles Leibniz’s response to the problem of evil (1697 [1989, 153]; on analogies between Nietzsche’s project and traditional projects in philosophical theodicy, see esp. Came 2004, 2005, 2022; May 2011). The mechanism by which Nietzsche thinks tragedy communicates this lesson is obscure. However, the basic idea relies on his idea that Greek spectators identified, not with the characters on the stage, but with the tragic chorus, and from this perspective experienced the drama as a mere ephemeral “vision” (BT 8). And just as the chorus-audience relates to the spectacle on the stage, so too does the primal unity relate to the empirical world as a whole. The analogy, then, intimates to the audience something of their metaphysical-cum-aesthetic significance (BT 8, 10).

Naturally, given the deep interpretive issues surrounding the status of The Birth’s metaphysics (see §2.2 above), not all agree that Nietzsche intends such talk of a “world-artist” literally. And, it might be suspected that whether one takes Nietzsche to be a pessimist or not depends on what one thinks about the intended status of the metaphysical picture the aesthetic justification of existence apparently encodes. If this picture is intended to be true, then it would seem that tragedy is supposed to show us that life really is justified. If, conversely, the metaphysics is not so intended, then it would appear that the so-called metaphysical solace is merely an illusion. In fact, these issues can come apart. It is possible, for instance, that Nietzsche is claiming that when watching a tragedy we catch a glimpse of what the world would look like from a god’s-eye perspective, even if nothing really occupies that perspective. The question, then, is only whether the world would manifest aesthetic value if seen in this way, and not whether there is any being to whom it does manifest such value. Conversely, not all proponents of the metaphysical interpretation of The Birth conclude that Nietzsche believes life is genuinely justified. Young, for example, argues that the fact

that pain and death are indispensable to life as an entertainment for the primordial unity … does nothing at all to justify life to those who—like Christians in the Roman arena—have the misfortune to have to be parts of the entertainment… . To the question of whether life as a human individual is worth living, The Birth replies with the same ‘No’ as does Schopenhauer. (2006, 24)
Evidently, much depends here on how we understand the status of the world’s alleged aesthetic value. If that value consists merely in the fact that it happens to please the “primordial unity,” then this sort of reading seems plausible. If, on the other hand, that value is supposed to be of a non-agent relative sort, then the reading might be resisted.

A further question, though one which is less often thematized, concerns the relation between the aesthetic justification of existence and what Nietzsche frequently calls “the metaphysical solace [Trost]”—something with which, he argues, “every true tragedy leaves us” (BT 7/KSA 1:56). It is natural to suppose that this solace consists precisely in the idea that existence is aesthetically justified. However, some characterisations of the metaphysical solace seem to differ substantively from this idea. Nietzsche describes the “metaphysical joy in the tragic” as rooted in the recognition that “the hero … is ultimately only appearance, and the eternal life of the will is untouched by his annihilation” (BT 16/KSA 1:108). The “metaphysical solace” lies in the thought “that under the whirl of appearances eternal life flows on indestructibly” (BT 18/KSA 1:115). Though each seems to depend on The Birth’s metaphysics, the idea that the world is justified as an aesthetic spectacle and the idea that there is something in us which survives death would appear to be quite distinct thoughts. The former idea might incline us to an overall optimistic reading of the text, if we assume that such a justification could be a genuine one, whereas the latter, which suggests that redemption from this life will be found in some metaphysical beyond, would incline toward a more pessimistic interpretation.

One interesting way of mediating between pessimistic and non-pessimistic interpretations of the early Nietzsche is proposed by Wolt (2025), who draws special attention to affinities with the views of Nietzsche’s older colleague and friend, the cultural historian Jacob Burckhardt (1818–1897). According to this proposal, Nietzsche is genuinely pessimistic inasmuch as he takes life in its typical form to be intrinsically bad, and thus rejects a broadly Christian view according to which every life has an innate positive worth. Yet this pessimistic thesis allows that certain exceptional lives may still achieve a value that makes them worth living. Though Wolt does not develop this interpretation specifically with an eye to Nietzsche’s theory of tragedy, it is not difficult to see how it might apply. Tragedy may be pessimistic insofar as it shows us the lamentable character of life as it typically occurs, but optimistic insofar as it hints at a way in which life could attain value.

3. Art and Illusion
Nietzsche’s aesthetic thought is centrally, if often only obliquely, engaged with Plato’s. In the Genealogy, Nietzsche dubs Plato “the greatest enemy of art Europe has yet produced” (GM III.25), referring, of course, to Republic X’s attack on the imitative or illusory nature of much art, which accordingly appeals to “a part of us that is far from reason” (603a). This engagement is hardly surprising given that the main target of Plato’s censure is Greek tragedy, the very artform which centrally occupied Nietzsche in his first philosophical work, and which would, to a greater or lesser extent, continue to occupy him through his final writings (see §6 below). What is more surprising are Nietzsche’s reasons for disagreeing with the Republic’s verdict: he does not typically deny the central charge that art is false; he instead questions the value Plato (and most of the rest of us, for that matter) place on the truth. This central theme of Nietzsche’s thought is evident already in The Birth’s valorization of Apollonian semblance, and denigration of “Alexandrian” culture—Nietzsche’s term of abuse for a society which accords the highest value to scholarship and science instead of to art. Shortly before The Birth, Nietzsche had written: “my philosophy is inverted Platonism: the further something is from true being, the more beautiful, the better it is. Living in semblance [Schein] as the goal” (NF 1870: 7[156]). Much later in his career, he continues to speak approvingly of art as a “cult of the untrue” and “the good will to semblance” (GS 107), in which “the lie sanctifies itself, in which the will to illusion [Täuschung] has the good conscience on its side” (GM III.25). And in notes from his final productive year, he writes: “the truth is ugly: we have art so that we do not perish from the truth” (NF 1888: 16[40]). Nietzsche is opposed, in short, both to Plato’s condemnation of artistic illusion, and to theories, like Schopenhauer’s or Schelling’s, that try to make art into the source of some deep, metaphysical truth. Art is false, and it is valuable (at least in part) because of its falsity.

There are several basic interpretive questions to ask here: (1) in what sense is art “false” according to Nietzsche? (2) Why does he think artistic falsity is valuable? And (3) how does this valorization cohere with his own views about the value of truthfulness? In approaching these questions, it is useful to keep in mind a simple distinction drawn by Stoll (2019, 332–33). Art might be representationally false, say, by attributing properties to the things it represents that they do not possess, or by expressing, implicitly or explicitly, false propositions. In contrast, art might be mimetically false because it is illusory, “fake,” non-genuine, merely resembles without really being what it depicts. In the first sense, ‘false’ is being used roughly in the way it typically is in, e.g., epistemology. In the second sense, it is being used in the way it is when we speak of a “false friend” or “false teeth.” These senses clearly come apart; an object like a set of false teeth has no representational or propositional content, and a portrait painting might capture its subject’s likeness with tremendous accuracy while (and perhaps for that very reason) creating a powerful illusion.

The prevailing tendency is to see Nietzsche as claiming that art is representationally false, and specifically as misrepresenting the undesirable or unaffirmable aspects of life. The following remarks seem to capture his position his well: “the role of art is to supply a (dishonest) fantasy that is to replace a reality that one cannot face” (Ridley 2007, 140); “art must represent life as beautiful, as affirmable, precisely because life is not beautiful” (Young 1992, 134); “artistic representation falsifies its object by depicting it as other than it is” (Came 2013, 220; cf. May 1999, 29–36; Janaway 2014). Assuming that Nietzsche is best interpreted in something like this manner, there are still several different ways art might be taken to be misrepresentational. First, and most straightforwardly, art might misrepresent what it overtly depicts. For example, a portrait painter might idealize some of her subject’s features, remove certain blemishes, and so forth. Second, art might misrepresent “life” or the “world” in some more general way, say, by implicitly or explicitly expressing false propositions about it, or otherwise suggesting a non-veridical outlook of some sort. For example, while Macbeth is on one level about specific members of the medieval Scottish nobility, perhaps its “real” message is that traitors meet a sorry end, and this message is not (without exception) true. Along similar lines, but much more generally, Nietzsche sometimes suggests that the world “in itself” simply lacks “beauty” and “order” (e.g. GS 109, 299). So, perhaps he thinks that art misrepresents the world simply to the extent that it represents things beautifully. Third, art might misrepresent by being fictional, in the sense of representing things which do not exist, rather than distorting what does exist. Greek art, for example, is preoccupied with mythical stories about non-existent gods, heroes, monsters, and so forth. And, presumably, Nietzsche would consider Christian art—with its various narratives about saints, prophets, miracles, and the like—false in just the same way.

If one or more of the above types of misrepresentation best capture Nietzsche’s conception of artistic falsity, then it is natural to suppose that the value of such falsehoods lies primarily in their content. A key facet of Nietzsche’s thought is the idea that it can be better, from a pragmatic perspective, to hold false beliefs than to hold true beliefs (BGE 3). We might then suspect that the value of artistic misrepresentations is rooted in the fact that their content is nonetheless “life-promoting.” Thus, for example, Gemes and Sykes have argued that tragedy fosters “the belief in a unity which underlies the apparent world, and offers the myth that in death, the individual will find redemption and reunification with the reality beneath appearance” (2014, 93). While the later Nietzsche may prefer less Schopenhauerian myths, he may still retain the basic structure of this conception of the value of artistic falsehood.

There are several problems, both philosophical and textual, with the above sort of view. One philosophical problem with the kind of aesthetic beautification Nietzsche sometimes seems to have in mind is that it arguably conflates representing something as beautiful with representing it beautifully (cf. Huddleston 2022, 125). So, the portraitist might depict her subject as more beautiful than he is, and thus misrepresent him. But, it is far from clear that Homer’s Iliad—or Malick’s The Thin Red Line, for that matter—misrepresent war as beautiful simply in virtue of being beautiful stories about war. Relatedly, we do not often infer that properties attributed to objects by works of art are properties those objects actually possess. If Nietzsche’s conception of artistic falsity depends on the assumption that we are deceived by it, then he is assuming art appreciators to be far more gullible than they typically are (though, for a reading according to which Nietzsche does not need this assumption, see Reginster 2014, 15–23).

The above sort of reading also raises interpretive puzzles. For one, as Aaron Ridley observes (2013, 421), whether a work of art is misrepresentational in any of the above senses seems to be a purely contingent matter. But, Nietzsche appears to think that art is necessarily false, or at least that art and falsity are very intimately related. Second, Nietzsche himself often appears opposed to precisely the conception of aesthetic beautification it is tempting to attribute to him (cf. Young 1992, 42–43). He suggests approvingly that “art also brings much that is ugly, hard, and questionable about life to appearance” (TI “Skirmishes” 24) and claims that “the good, strong will of the older Hellene” was characterized by a “longing for the ugly” (ASC 4). Finally, it is unclear how Nietzsche’s insistence on the value of artistic falsity coheres with his broader normative commitments. Specifically, and as Ridley (2007) in particular has argued (cf. Reginster 2014, 25–29), the suggestion that art supplies us with life-affirming fictions seems to conflict with Nietzsche’s conception of life-affirmation as requiring “that one does not want to have anything differently … not merely to endure what is necessary, still less to conceal it … but to love it” (EH “Clever” 10). There is a broader question here about the extent of Nietzsche’s commitment to the ideal truthfulness (see Janaway 2024, for an exploration of many of the difficult issues involved). But the idea that the need to conceal certain difficult facts about reality from oneself expresses a kind of contemptible cowardice is a consistent theme in Nietzsche’s later works (e.g., BGE 39, 227; A 54; EH Preface 3, “Books” BT:2). Nietzsche’s apparent suggestion that we need art precisely to conceal such facts from ourselves evidently conflicts with that idea.

There are a number of potential responses to this last difficulty. One is to suggest that Nietzsche fails to entirely resolve it (Ridley 2007, 123–27), or leaves it unresolved specifically to call attention to what he takes to be genuine tensions in our values (Janaway 2014, 51–56). Another proposal holds that his view develops away from the idea that art should be in any sense false (Reginster 2014). More ecumenically, one might suggest that Nietzsche sees art and truthfulness as competing “regulative ideals” which have to be appropriately balanced against one another (Anderson 2005, 203–11), or that art is permitted to misrepresent, but only those aspects of life that we cannot change or otherwise accept (Ridley 2007, 80–83; 2013, 422–23). According to these sorts of readings, Nietzsche may advocate maximal honesty while still recognizing that life-affirmation might ultimately require some degree of falsification. One problem for this suggestion is that Nietzsche appears to insist that it is precisely the necessary aspects of existence we must acknowledge and affirm (EH “Clever” 10; cf. NF 1888: 16[32]).

A contrasting approach to Nietzsche’s views on artistic falsity is developed by Stoll (2019; cf. Page 2024). According to this approach, Nietzsche’s primary interest is in mimetic falsity, in the fact emphasized by Plato that much art is centrally occupied with imitating, with producing semblances or simulacra of the things it depicts. It was, recall, this sense of falsity that was at issue in Schiller’s conception of Schein (see §1.3 above), which had been important for Nietzsche since early in his career. Nietzsche can thus consistently say that art is false while denying that it should falsely beautify things; its falsity is a function of the medium not the representational content. Divorced as it is from the content of specific works, this account of artistic falsehood can arguably make good sense of the tight connection Nietzsche seems to see between art and illusion (though certain artforms, such as music or architecture, may still pose problems). It also suggests that the aim of art is not to instil in us a set of false, but useful beliefs. Instead, art glorifies or celebrates illusion and falsity themselves, inviting us to reconsider our—mostly negative and, Nietzsche thinks, unhealthy—attitude toward them. Stoll argues that this interpretation allows us to locate the underlying consistency between Nietzsche’s ideal of uncompromising honesty and his valorization of art in a way that does not require the balancing approach suggested by Ridley and Anderson (Stoll 2019, 339–42).

Whether this solves the textual problem depends to some degree on how we think of the positive value art accords to falsity. If to value something positively is to be inclined to pursue or promote it, then it might seem the answer is “No.” Yet, not all positive valuing is like this for Nietzsche. He thinks, for example, that it is possible for the nobleperson to have “reverence” (Ehrfurcht) and even “love” for an enemy whom they aim to defeat (GM I.10). To suggest that it is better for us to be positively disposed to illusion might, but need not be to say that it is better for us to cleave to benighted self-conceptions.

4. Beauty, Disinterestedness, and Creativity
One of the more significant developments Nietzsche’s aesthetics undergoes in the time between The Birth and his later works concerns the idea of disinterested aesthetic appreciation. While the early Nietzsche apparently accepts some version of the thesis that aesthetic pleasure is disinterested, he consistently rejects it by at least 1886 (e.g., BGE 33; though, for a dissenting view according to which Nietzsche’s later aesthetics is still wedded to a broadly Schopenhauerian version of aesthetic disinterest, see (Denham 2014)). The best-known discussion of this issue in the later works comes in GM III.6, where it is connected with an intriguing, albeit obscure, criticism of aestheticians—Kant and Schopenhauer are singled out in particular—who take their cue “purely from the standpoint of the ‘spectator’ [vom ‘Zuschauer’ aus]” (KSA 5:346).

There are a few obvious questions to be had here. First, what is Nietzsche’s criticism of the notion of aesthetic disinterest? This notion has meant different things to different philosophers, and how extensive and successful his criticism is will depend in part on how he understands the notion. Second, how is this criticism related to his apparent preference for an aesthetics that focuses on “the experiences of the artist (the creator)” rather than on those of the spectator? Schopenhauer, after all, believes that the creative genius is someone uniquely capable of attaining a state of pure, will-less cognition (SW 3:430–31/WWR 2:393–94); it is not immediately obvious that looking at the artist will cast suspicion on disinterestedness. Perhaps Nietzsche’s point is that Schopenhauer only reads his faulty theory of aesthetic spectatorship into the creative process. If this is right, then we may still wonder why we should prefer an aesthetics of the creator. Answers to these obvious questions may depend on answers to some more subtle questions. For instance, what precisely would we be looking at in considering “the experiences of the artist?” Artists’ own judgments of what is beautiful/aesthetically valuable? The nature of the creative process? Something else entirely? Nietzsche faults Kant and Schopenhauer for “envisioning the aesthetic problem” from the spectator’s point of view. What is this “aesthetic problem?” It is plausible to suppose that he has in mind the question of the nature of the beautiful (Ridley 2011), but Nietzsche does not explicitly say.

Zangwill (2013) provides one plausible set of answers to these sorts of questions. On his reading, Nietzsche’s objection to Kantian aesthetics concerns more its emphasis on the universality of judgements of taste than their disinterestedness (though, of course, the two issues are not unrelated for Kant). More specifically, Nietzsche’s main target, on this reading, is the idea that beauty is universally available. Nietzsche need not be read as claiming that no beauty is so available, only that “some beauty, higher beauty, can be grasped only by a select few” (Zangwill 2013, 84). This yields one potential way of understanding the objection to aesthetic disinterest and its connection with the spectator. The thought would be that there are some types or instances of beauty which are best or only accessible through “enhanced ecstatic experiences” (ibid., 90), not through dispassionate reflection. And if it is specifically artists who are capable of such experiences, or who are most saliently aware of having had such experiences, then there is reason to find fault with an aesthetics that focuses solely on the experience of the artistic laity. This account, of course, raises questions of its own. Why, for example, should the “select few” capable of grasping higher beauty be identified with artists? And why should disinterested aesthetic experience itself not also be the province of a select few? Even if Kant is right that I can only legitimately require agreement from everyone if my aesthetic judgment is disinterested, it might still be the case that, as a matter of fact, most are unable to judge disinterestedly. Schopenhauer, for one, appears to think that the capacity for genuine will-less cognition is exceedingly rare.

Note that Zangwill’s approach treats ‘experiences of the artist’ and ‘the aesthetic problem’ as referring to judgements or perceptions of beauty. A more radical interpretation finds Nietzsche critical of the very idea that the aesthetic state is to be construed in such “passive” terms to begin with. On this view, “the significance of art is to be found less in its products than in the creative activity by which they are produced” (Reginster 2014, 25; cf. Soll 1998, 108ff.). Here, the problem with a spectator’s aesthetics is that it locates aesthetic value in the wrong place—works of art, or the experiences they cause—or at least fails to see that such value can be located elsewhere as well (cf. Huddleston 2020, 5). In its most extreme version, the view is that beautiful objects are “only by-products, and their beauty is merely a reflection of a … beauty that is, in the first place and most authentically, the artist’s own” (Ridley 2011, 321).

What, if anything, can be said in favor of such a view, and how is it connected with Nietzsche’s criticism of disinterestedness? One suggestion is that the spectator’s standpoint cannot account for the artist’s motivation to create a work (Soll 1998, 108; Reginster 2014, 24). This is particularly problematic if the genius is conceived along Schopenhauerian lines—as someone possessing a heightened ability for perceiving nature in the purely intellectual, will-less manner, supposedly characteristic of all aesthetic enjoyment. The question is why, having achieved this state themselves, the artist should be motivated to make it available to others in the form of an artwork. One might of course wonder what this psychological question has to do with the question of the nature of beauty. After all, it is hardly clear that a successful theory of beauty has also to explain what motivates artistic creation. A potential response, here, would be that Nietzsche operates under the general assumption, shared with Schopenhauer, that the state from which the artist creates is the same as the state that their work aims to produce (Young 1992, 120). If conceiving the latter in terms of disinterestedness cannot explain the possibility of the former, such a theory would then need to be rejected. (For a critical assessment of the argument here, see (ibid., 121–25).)

In addition to its critical reflections on earlier aesthetic theories, GM III.6 also gives us some indication of Nietzsche’s own positive account of beauty or positive aesthetic value. The section quotes approvingly a claim from Stendhal’s Rome, Naples et Florence (1817) that “the beautiful is a promise of happiness,” and Stendhal is said to constitute, in contrast to Kant and Schopenhauer, a “genuine spectator.” One important question, here, is whether the fact that Stendhal is called a “spectator” means that Nietzsche regards even his definition of beauty as in some sense deficient (Ridley 2011). Most, however, take Nietzsche to be adopting Stendhal’s definition. What, then, is the import of this definition? In his later works, Nietzsche often associates the experience of beauty with the feeling of power and with erotic arousal. A particularly developed interpretation focusing on these associations is (Reginster 2014; cf. Soll 1998; Ridley 2011, 319–25). Reginster’s reading takes inspiration from Nehamas’s recent theory of beauty (2007). According to this theory, to find something beautiful is not to experience or judge it to be of positive value—aesthetic experience is not a “verdict.” To find something beautiful is rather to be impelled to continue engaging with it, to get to know it better. The version of this theory that Reginster locates in Nietzsche holds similarly that to find something beautiful is, not to find oneself in a state of passive enjoyment, but to be impelled by it toward “new artistic creation” (2014, 31). This sort of account offers a way of making sense of Nietzsche’s somewhat surprising approval (TI “Skirmishes” 22–23) of Plato’s definition of love in the Symposium as “reproduction and birth in beauty” (206e). Here, one might think too of the way in which we speak both of artistic geniuses as “inspired,” and of their works as “inspiring.” It also promises to cast further light on Nietzsche’s dissatisfaction with conceptions of aesthetic pleasure as disinterested. If we assume that activity is inherently interested, and if to find something beautiful is to be impelled by it to some kind of activity, then it would seem appreciation of the beautiful could not be disinterested. (For another sophisticated account of Nietzsche’s conception of beauty that emphasizes the dimension of erotic arousal, see Leiter 2018, and for a compelling proposal connecting it to Nietzsche’s remarks about glorification and praise, Fox 2020.)

Not all of Nietzsche’s remarks about beauty and aesthetic value sit naturally with these functional sorts of definition. Especially in his discussions of self-creation, though in other contexts as well, he seems to operate with a more traditional understanding of beauty as harmony or unity amongst diversity. He likens beauty to “order, division, form” (GS 109), to the imposition of a coherent style (GS 290), and this imposition is described as involving the ability “to become master over the chaos that one is; to compel one’s chaos to become form; to become necessity in form: to become logical, simple, unequivocal, mathematics; to become law” (NF 1888: 14[61]). These very same aesthetic standards are deployed—fairly or not—against Wagner’s alleged stylistic decadence:

With what does every literary décadence distinguish itself? By the fact that life no longer dwells in the whole. The word becomes souvrain and leaps out of the sentence, the sentence reaches out and eclipses the sense of the page, the page wins life at the cost of the whole—the whole is no longer a whole. But that is the image for every style of décadence: every time anarchy of atoms, disintegration of the will. (CW 7)
In these contexts, Nietzsche seems to adopt the conception of beauty, common amongst the eighteenth-century rationalists, as a kind of “perfection” or organic unity (see Hassan 2022, for a particularly sophisticated treatment of Nietzsche on organic unity; cf. Soll 1998, 102–105). Here, beauty (or at least aesthetic value of some kind) seems to be treated as a perfectly objective property of beautiful objects rather than, say, a mere invitation to further activity. This is not to say that these two strands could not in principle be connected. Ridley (2013, 419–20), for example, emphasizes the close connection Nietzsche often draws between creativity and form-giving. If Nietzsche does endorse the above-mentioned Schopenhauerian view that there is a kind of parity between creative activity and the aesthetic experience, he might maintain that seeing an object as “formed chaos” is precisely what incites us to creativity ourselves.

5. Aesthetics and Physiology
In the Genealogy, Nietzsche remarks parenthetically that he will one day write a “Physiology of Aesthetics” (GM III.8), and in The Case of Wagner (published the following year) he refers prospectively to “a chapter of my magnum opus, which carries the title ‘toward a physiology of art’” (CW 7). The plan for this work never came to fruition, though some material appearing under that heading in Nietzsche’s notebooks seems to have been utilized in the writings of his final year, particularly in Twilight of the Idols (cf. NF 1888: 17[9], TI “Skirmishes” 8–11, 19–24). One of the most extreme claims from this period comes in Nietzsche contra Wagner: “Aesthetics is indeed nothing but an applied physiology” (NCW “Objections”). These remarks are largely promissory, and further textual evidence for the shape of the envisioned “physiology of art/aesthetics” is relatively thin. Nevertheless, some scholars have argued that the biological sciences are central to Nietzsche’s mature aesthetic theory.

Moore (2002, 85–111) sees Nietzsche as part of a post-Darwinian tradition of German thinkers, along e.g. with Ernst Haeckel (1834–1919), aiming for evolutionary explanations of aesthetic experience. A Darwinist interpretation is developed, in particular, in Richardson 2004 (219–70). According to Richardson, Nietzsche believes that evolution selects for a tendency to discriminate between traits associated with fitness and those not so associated, and to find pleasure in the former and displeasure in the latter (236–43; cf. Stern 2020, 58). This reading can explain well Nietzsche’s tendency to associate aesthetic experience with sexuality (e.g., GM III.6; TI “Skirmishes” 22–23), as well as his occasional privileging of human beauty (TI “Skirmishes” 20). It will apparently struggle to explain why we find beautiful a vast array of things—instrumental music, abstract art, non-human animals, landscapes, etc.—that are not humans (cf. Janaway 2007, 190). Nietzsche might suggest that the features we find beautiful in non-human objects are those that remind us in some way of physical human beauty. Such a suggestion might be broadly in the spirit of Burke’s theory that the same general qualities which make for beautiful human bodies (smoothness, relative smallness, etc.) are the qualities which also cause us to find beautiful non-human objects. Alternatively, Nietzsche might appeal, in proto-Freudian fashion, to a sublimation of the sex-drive (e.g., GM III.8; cf. Moore 2002, 106–7). As noted in the previous section, one plausible reading holds that the experience of the beautiful, for Nietzsche as for Plato, involves being spurred toward creativity. Thus, the pleasure taken in a sunset or a Beethoven symphony might be said to be “erotic” to the extent that it puts us in this general sort of state, even if it does not involve arousal or the desire for literal procreation.

A distinct, if potentially related point of contact between physiology and art is Nietzsche’s frequent use of medical concepts when assessing art and artists in his later writings. Art of which he disapproves—especially Wagner’s art—is castigated as “decadent” (décadent) or “degenerate” (entartete), whereas the art of e.g. Raphael, Bizet, or Goethe is celebrated as “healthy,” and a “stimulus to life” (TI “Skirmishes” 24). To call an artist or their work “decadent” might be to make a functional or an expressive claim. Both sorts of claims can be found in the text, and are sometimes made in the same breath—e.g., “is Wagner even a human being? Is he not rather a sickness? … He has made music sick” (CW 5). The claim would appear to be that decadent art, like Wagner’s, is both a “symptom” of the artist’s own degeneration, as well as a kind of “contagion,” and garners rebuke on both grounds. In other contexts, however, Nietzsche suggests a slightly different point. In a well-known passage from The Gay Science, for example, he writes:

Every art, every philosophy, may be viewed as an aid and means of salvation for growing, struggling life: they always presuppose suffering and sufferers. But there are two types of sufferers: those who suffer from the overabundance of life, who want a Dionysian art and likewise a tragic view of and insight into life—and then those who suffer from the impoverishment of life, who seek quiet, stillness, a placid sea, redemption from themselves through art and knowledge. (GS 370)
Here, Nietzsche appears to be sounding a theme that was with him since The Birth of Tragedy—that art is one of humanity’s fundamental means for coping with the suffering endemic to life. Now, however, he comes to think that not all suffering is equal; some suffering is “healthy,” while other suffering is “sick.” Nietzsche clearly signals his approval for art that responds to suffering of the former kind. Note, however, that this need not mean he disapproves of the latter sort of art on functional grounds; such art may be genuinely good for those who need it, and otherwise innocuous. Instead, his objection may be to the fact that such art perforce expresses an evaluative outlook of which he disapproves (cf. Ridley 2007, 124).

In still other contexts, Nietzsche develops this general idea in the direction of what appears to be an unusual form of aesthetic non-cognitivism. After claiming that “aesthetics is irrevocably tied to these biological presuppositions [of ‘ascending’ or ‘declining’ life],” he suggests that “these opposing forms in the optics of values … are ways of seeing, which one cannot get at with reasons and refutations. ... One does not refute a disease of the eye. ... The concepts ‘true’ and ‘untrue,’ it seems to me, have no meaning in optics” (CW Epilogue). The suggestion appears to be that, when we make judgments of the form ‘x is beautiful,’ we are not making truth-apt claims so much as expressing our general (“healthy” or “diseased”) evaluative perspective. It is not obvious, however, that the passage must be read in this irrealist way. The image of the diseased eye, e.g., seems to imply that some such perspectives are wrong or distorted. And the insistence that “reasons and refutations” are out of place in the context of aesthetic disagreement might simply be a way of indicating that they are dialectically ineffective, not that they are conceptually inappropriate.

However one resolves these various unclarities, the important point is that Nietzsche’s idea for a “physiology of aesthetics” may be about looking at art from a diagnostic perspective, rather than investigating its evolutionary origins (though, of course, it is also possible that it be both; see Moore 2002 (165–92) for an interpretation along these lines). Now, much of Nietzsche’s language here—which calls to mind 19th-century “degeneration theory” and its connections with racism, misogyny, and eugenics—is rather ugly. One might wonder how literally he intends such talk—whether he really believes, e.g., that Wagner’s art is the product of some physiological malady, or whether the notions of health and sickness are being used more metaphorically. The latter option is bound to seem more attractive to contemporary readers, though it should be borne in mind that degeneration theories enjoyed wide acceptance in Nietzsche’s day, and he may well have taken them seriously in a way that we cannot. Discussing this issue in any depth would take us far beyond the ambit of this entry. But, it bears mentioning that Nietzsche, atypically for a nineteenth-century degeneration theorist, often does not treat degeneration as unqualifiedly bad (Gemes 2021). He will even suggest, paradoxically, that “sickness itself can be a stimulus to life” (CW 5).

6. Nietzsche’s Later Theory of Tragedy
Nietzsche never treated tragedy (or any specific artform, for that matter) in as much depth as he treated it in The Birth of Tragedy. Yet, in his mature writings, he would continue to style himself “the first tragic philosopher” (EH “Books” BT:3), and to suggest that we need a tragic outlook on life (GS 370; cf. GS 1, EH “Books” BT:4). Nietzsche’s name remains to a certain degree inextricable from the philosophy of tragedy, given his landmark contribution to this field. It is thus worth asking how, if at all, his position evolved since his first book. We may approach this question by considering Nietzsche’s developing views on the pleasure and value of tragedy. These issues are, of course, distinct. The former question is why we evidently take pleasure in the experience of intrinsically unpleasant emotions (pity, fear, etc.). There may be reasons why tragedy is valuable other than that it causes pleasure, and the reasons for valuing it might not be the same as the reasons it is pleasurable. Nevertheless, it is natural to expect that these two issues would be intimately related, so we shall consider them together.

In The Birth of Tragedy, answers to both questions were apparently rooted in the idea that tragedy provides an “aesthetic justification” of life and a “metaphysical solace” (see §2.3 above). Tragedy is valuable because it gives us (or gave the Greeks, at least) an adequate way of confronting the suffering of life, or a way of conferring meaning on life, or both. Nietzsche’s attempt to resolve the famous paradox of tragic pleasure here is somewhat more obscure. But, it is plausible to think that—like Schiller, Schlegel, Schopenhauer, and others—he means to appeal to the notion of the sublime (see BT 7/KSA 1:57), understood as involving the realization or feeling that there is something in us that transcends the boundaries of empirical reality (see Young 2013, 178–182 for discussion). Alternatively, the suggestion is that “the pleasure produced by the tragic myth has the same home as the pleasurable sensation of dissonance in music” (BT 24/KSA 1:152). The idea would appear to be that tragic pleasure lies in the realization or feeling that our suffering is an integral part of a wider aesthetic whole (Raymond 2014, 71; Hassan 2022, 126). It is natural to suspect that Nietzsche would soon have come to abandon these answers, appealing as they do to a speculative metaphysics.

This suspicion would appear to find confirmation in Nietzsche’s subsequent remarks about tragedy. In The Gay Science, for example, he offers a far more reductive account of the pleasure in tragedy—or at least of the pleasure the Greeks took in it—in terms simply of hearing “good speeches” (GS 80). The explanation resembles Hume’s suggestion that tragic pleasure “proceeds from that very eloquence, with which the melancholy scene is represented” (1757, 190–91), albeit without Hume’s further suggestion that painful emotions thereby undergo a kind of “conversion.” In fact, Nietzsche here denies that negative emotions—pity and fear—are part of the proper response to tragedy at all. Arguably, this account solves the problem of tragic pleasure only at the expense of denying what might seem an obvious fact: that the experience of tragedy is hedonically “ambivalent.”

In Beyond Good and Evil (BGE 229) and the Genealogy (GM II.7), Nietzsche offers a slightly different account. He suggests that “that which constitutes the painful voluptuousness of tragedy is cruelty” (BGE 229). The most straightforward way to understand this claim is that the pleasure we get from witnessing the suffering of the tragic hero is a kind of Schadenfreude. This would explain the pleasure in tragedy, though it does not address what most philosophers have found perplexing about it—the fact that we seem to enjoy the intrinsically unpleasant emotions (e.g. pity) it provokes. Moreover, it seems plainly inadequate from a phenomenological perspective. Typically, a well-written tragedy makes us identify with and feel for the hero, not her tormentors. A different reading, which addresses these concerns, sees Nietzsche as attempting to explain tragic pleasure as a peculiar form of sadomasochism. At this stage of his career, Nietzsche holds that a fundamental feature of human psychology is a tendency to take pleasure is causing suffering, including causing ourselves to suffer. In engaging with the painful experience of tragedy, then, one takes pleasure “in one’s own making-oneself-suffer” (BGE 229). This explanation also seems problematic in at least a couple of ways. For one, even if we agree with Nietzsche’s account of the pleasure of self-directed cruelty, it is unclear why tragic pleasure should count as such an instance. If the point is that in watching a tragedy we willingly undergo an experience that we know will be painful, then it may be noted that there are many such cases which are neither pleasurable nor instances of self-cruelty (e.g., going to the dentist). A second problem arises from the fact that Nietzsche apparently still wants to deny that pity is a genuine response to tragedy; “tragic pity” is merely one of the many “inoffensive names” we use to morally sanitize our enjoyment of cruelty (GM II.7). But, if that is right, then it is unclear why the experience of tragedy is painful to begin with, since the pain is usually thought to be rooted precisely in the experience of emotions like pity.

In the works written between 1878 and 1887, the impression is strong that Nietzsche is experimenting with various accounts (cf., HH I.103, 166, D 172) of tragic experience without having settled on a considered, developed view (though, see Prince 1998 and Kirwin 2023 for contrasting assessments). Noteworthy too is the fact that in this period Nietzsche makes few claims for the positive value tragedy. Matters change somewhat dramatically in the works of his final productive year, especially Twilight of the Idols. Here, Nietzsche devotes much more sustained attention to the tragic artform and is once again keen to insist on its central importance to his ethical outlook. One question that arises in this connection is whether this renewed focus on tragedy also brings with it a return to The Birth’s substantive doctrines. Young (1992, 136–39) and Ridley (2007, 126–27; 2019), in particular, have argued that it does. Consider, for example, the following suggestive, but characteristically gnomic, passage:

Tragedy is so far from proving something about the Hellenes’ pessimism in Schopenhauer’s sense, that it must rather count as its most decisive refutation and counter instance. Saying Yes to life itself even in its strangest and most difficult problems; the will to life rejoicing of its own inexhaustibility in the sacrifice of its highest types—that is what I called Dionysian, that is what I guessed to be the bridge to the psychology of the tragic poet. Not to be freed of terror and pity, not to purify oneself of a dangerous affect by means of its vehement discharge—this is how Aristotle understood it—: but rather, beyond terror and pity, to be the eternal pleasure of becoming itself,—that pleasure which also includes the pleasure in annihilation… And with this I touch again on that point from which I once began—the Birth of Tragedy was my first revaluation of values. (TI “Ancients” 5)
What is especially surprising here is not Nietzsche’s continued adherence to the Dionysian affirmation of life even at its “strangest and most difficult,” but his apparent reprisal of the idea that such affirmation involves identifying (if only imaginatively) with an inexhaustible “will to life” that takes pleasure in sacrificing its creatures. It is difficult, here, not to hear echoes of The Birth’s “primordial unity” or “world-artist.” What this seems to indicate is that Nietzsche has returned to the idea that tragic experience involves transcending one’s individual existence and identifying with a kind of suprapersonal being—one is supposed to somehow “be the eternal pleasure of becoming itself”—or, at least, that it involves the feeling of such transcendence and identification.

While a reasonable conclusion to draw from the above passage taken in isolation, this suggestion is surprising, given the later Nietzsche’s clear opposition to Schopenhauer’s metaphysics and his ideal of self-transcendence. Those wishing to resist this conclusion might note that the passage concludes a chapter entitled “What I Owe to the Ancients,” in which Nietzsche attempts to summarize the influence that his studies of antiquity have had on his broader thought. It is possible that he here intends only to be recalling rather than reasserting the position of The Birth of Tragedy, and to be emphasizing that some elements of his later thought—his opposition to Schopenhauer’s pessimism, his view that life must be affirmed in spite of its “terror and absurdity”—were already nascent there. There are difficulties that such a reading would need to meet (see Ridley 2019, 321–22), but the impression that the above passage does not exactly capture Nietzsche’s mature view is reinforced by the fact that he elsewhere presents an apparently quite different conception of tragedy:

What does the tragic artist communicate of himself? Is it not precisely the condition without fear in the face of the fearsome and questionable that he shows? … The courage and freedom of feeling before a powerful enemy, before a sublime catastrophe, before a problem that arouses dread—this triumphant condition is what the tragic artist selects, what he glorifies. Before tragedy what is warlike in our soul celebrates its saturnalia; he who is used to suffering, who seeks out suffering, the heroic man, extols his own existence with tragedy. (TI “Skirmishes” 24)
A different reading, which draws on these comments, is developed in Reginster 2014. Recall Reginster’s functional account of aesthetic value (see §4)—to find something beautiful is to be spurred by it to further creative activity. Reginster’s application of this general view to the case of tragedy builds on his influential reading of the will to power as the overcoming of resistances (2006). To will power is to will the overcoming of resistances, and this inevitably brings suffering along with it. Yet, it is also a requirement of great achievements, which often, if not always, require meeting great challenges. Instead of showing us suffering as a reason for withdrawing or turning away from life, the tragic artist invites us to “respond to ‘the terrifying and the questionable’ in our existence as he does—as so many challenges, calls to adventure, or opportunities for overcoming” (2014, 34). This reading has the advantage of not relying on a supraindividual perspective Nietzsche gives us reason to reject, and of connecting his later thoughts on tragedy with his other major philosophical preoccupations (power, self-overcoming, etc.). It does, however, raise further questions about the adequacy of Nietzsche’s position. To say that all achievement requires suffering, in the form of overcoming resistance, is not to say that all suffering also constitutes an occasion for great achievement. Some suffering might be purely and utterly destructive. And, indeed, one might have supposed that the sorts of suffering encountered by tragic heroes are often and paradigmatically of that sort—calamities that come entirely unbidden, undeserved, and by which the hero together with their achievements are undone. One may think here of Oedipus’ exile from Thebes, which plunges the city he once saved into civil war and leads to the death of three of his children. This was a fact that Nietzsche himself had been particularly keen to emphasize early on (KGA II.3, 7–10), but of which he would now apparently have lost sight.

It is difficult not to conclude that, despite his lifelong engagement with the topic, Nietzsche never arrived at a fully satisfactory theory of tragedy, let alone one that he was able to clearly articulate. This is not to devalue his achievements here. Perhaps no philosopher has done as much to emphasize that an understanding of tragedy must be rooted in an understanding of the historical and cultural institutions of the ancient world from which it emerged. Few have seen as keenly the ways in which the tragic resists explanation in terms of traditional moral categories. And even fewer have tried so forcefully to articulate something which many seem inchoately to feel—that tragedy contains the answers to our most profound existential questions. Nietzsche’s thoughts here can at times seem muddled, even quixotic. But they are deep thoughts, and ones which merit serious philosophical consideration.

Bibliography
Editions of Nietzsche’s Texts
The following editions of Nietzsche’s works have been referenced. Translations are the author’s own.

[KGA]
Werke: kritische Gesamtausgabe, 40 volumes, Colli, Montinari, Gerhardt, Miller, Müller-Lauter and Pestalozzi (eds.), Berlin: Walter de Gruyter, 1967ff.
[KSA]
Sämtliche Werke: kritische Studienausgabe, 15 volumes, Colli and Montinari (eds.), Berlin: Walter de Gruyter, 1980ff.
[SB]
Sämtliche Briefe: kritische Studienausgabe, edited by Giorgio Colli and Mazzino Montinari. Berlin: Walter de Gruyter, 1975–84.
[FS]
Frühe Schriften, 5 volumes, Mette and Schlechta (eds.), Munich: C.H. Beck, 1994.
[WEN]
Writings from the Early Notebooks, Nehamas and Geuss (eds.), Löb (trans.), Cambridge: Cambridge University Press, 2009.
Abbreviations of Nietzsche’s Works
In citing Nietzsche’s published works, this article follows the North American Nietzsche Society’s system for abbreviations. Abbreviations referring to the titles of individual works are followed by section numbers in Arabic numerals. Where the work in question is divided into chapters or parts across which sections are not numbered consecutively, Roman numerals or abbreviations of chapter titles precede section numbers (e.g. GM III.5 = On the Genealogy of Morals, Third Essay, section 5).

A
The Antichrist: A Curse on Christianity
BGE
Beyond Good and Evil: Prelude to a Philosophy of the Future
BT
The Birth of Tragedy out of the Spirit of Music
CW
The Case of Wagner: A Musician’s Problem
D
Daybreak: Thoughts on the Prejudices of Morality
EH
Ecce Homo: How One Becomes What One is.
GM
On the Genealogy of Morals: A Polemic
GS
The Gay Science
HH
Human, All-Too-Human: A Book for Free Spirits
NCW
Nietzsche contra Wagner: Documents of a Psychologist
TI
Twilight of the Idols: or How One Philosophizes with a Hammer
UM
Untimely Meditations
Z
Thus Spoke Zarathustra: A Book for All and None
Where helpful for locating a particular passage, references have also been given to volume and page number of the KSA. Citations of Nietzsche’s unpublished notes [NF] refer to year, and notebook and fragment numbers as established in the critical edition.

Traditions of Ideology Theory
Although ideology has come to be inextricably tied to Marxism and critical theory, liberal and conservative traditions of thinking about ideology predate Marxist theorizations and set the terms on which the term was integrated into Marxist theory.

The term is attested as far back as the seventeenth century, but its coinage is usually attributed to Antoine Louis Claude Destutt, compte de Tracy (1754–1836), one of the scholars affiliated with the Institut de France. Destutt de Tracy used the term to name a science of ideas. Like many other “-ology” terms—biology, pathology, mythology, and methodology are prominent examples—the term’s meaning in common speech has migrated such that it generally names the object of the study of ideas, rather than the study itself. In the case of ideology, this migration was prepared by the reflexive and pedagogical nature of Destutt de Tracy’s science, which had to both account for itself as a set of ideas and enact a process of intervening in the world of ideas so as to reform the ideas of its audience.

This established “ideology” as a term akin to “Enlightenment”, and traditions of theorizing ideology can be distinguished according to the broad schools of response to the political, economic, and pedagogical projects of the Enlightenment. Liberal theorists of ideology have focused on the dangers of passionate or unacknowledged partiality and partisanship on the moral, political, and scientific projects by which Enlightenment might be advanced. Conservative and counter-Enlightenment thinkers, by contrast, have focused on the moral and political risks of rationalism and idealism, and have adopted “ideology” as a pejorative name for purposeful efforts to reform traditions, beliefs, and practices.

Marxism, which has produced the most extensive and varied tradition of ideology theory, has alternated between a debunking approach to Enlightenment projects and a militant embrace of radical versions of them. In the debunking mode, Marxists try to show that Enlightenment projects, and the ideologies in which they are expressed, are merely attractive excuses for or mystifications of social domination and violence. In the radicalizing mode, Marxists argue, instead, that the aims of Enlightenment cannot be achieved by Enlightenment methods, and that projects of economic and political transformation must take precedence over “ideological” projects of changing people’s minds.

Understanding these traditions of thinking about ideology is required to make sense of the contemporary field of study, in which elements of this history are constantly, if inconsistently, mobilized.

1.1 Liberal Conceptions of Ideology
“Ideology” originated as part of the liberal project of social reform via education. Destutt de Tracy’s works on the moral and political sciences, written as part of his activities at the Institut, were published as Elements of Ideology (1801–15). In the Elements, Destutt de Tracy derived from empiricist epistemological premises a canon of probabilistic reasoning, a sentimentalist moral theory, and a liberal political economy. The work summarized programmatically the hopes for a secular and rationalist educational system that Destutt de Tracy—together with Pierre Jean Georges Cabanis (1757–1808) and other allies in the French Senate—tried to realize under the Directory and then the Consulate.

The aim of ideology was “to place the moral and political sciences on their true basis, a knowledge of our intellectual faculties” (Destutt de Tracy 1817 [2011: 10]). Given that every perception is, “taken each separately, and in itself”, true, Destutt de Tracy traced all error to the will, which “denaturalizes” our original perceptions (1817 [2011: 34, 36]). The will or desire, our faculty of preference, leads us into error by inducing us to see what we want to see and to discount what is inconvenient to recognize.

For this reason, ideology can help us by teaching us how “to analyze our sentiments” and to distinguish between those that “direct us well” and those that

form within us a false and blind consciousness [une fausse et aveugle conscience], which always removes us further from the road of reason, the only one leading to happiness. (1817 [2011: 255]; 1815 [2015: 254])

Therefore, despite the later identification of the two terms, Destutt de Tracy conceived ideology as a remedy for false consciousness.

This basic orientation has continued to characterize liberal approaches to ideology. Liberal education in its modern form aims to counteract the human tendencies to defer to authority, to succumb to peer pressure, and to exploit perceived inequalities of status (Nussbaum 2009: 9). It pursues this aim by teaching critical thinking, an appreciation for scientific methods, and a standard of reasonableness. Extremism, moral absolutism, and confirmation bias are thought to be inimical to liberal society, and a broad tradition of social scientific and social theoretical research, as well as political philosophical reflection, has grown up around the study and management of these problems (see, e.g., Putnam 1971).

The continuity between this work and that of the idéologues has been obscured by the fact that “ideology” has come to name the danger to be overcome or managed by liberal education rather than the intellectual project of overcoming or managing that danger. Thus, the end-of-ideology thesis expressed the hope that “civil politics could replace ideological politics” (Bell 1988: 138), but such a civil politics was also the aim of ideology for Destutt de Tracy, Cabanis, and company.

Within the liberal tradition, some have distinguished “ideologies”—understood as inherently unreasonable intellectual visions for ordering human affairs (Arendt 1953; Bell 1960 [1962])—from religious and ethical traditions and worldviews more generally. Others consider all “comprehensive doctrines” together, allowing that there might be reasonable or unreasonable adherents of any given doctrine (Rawls 2005).

Another division among liberal approaches emerges from the practical and political question of how liberal polities and institutions ought to respond to unreasonable ideologies. For public reason liberals, policy must be susceptible to public justification—i.e., justification in terms acceptable to all—in order to be legitimate (Gaus 1996, 2011; Rawls 2005). This might be understood as requiring a language of political justification that is neutral among all comprehensive doctrines or ideologies. In practice, this approach leans in the direction of strong universalism in legislation and a reliance on markets and market-like incentives as the predominant socializing or civilizing mechanisms. However, some liberals believe this is insufficient to protect the political sphere from capture by illiberal forces, and advocate instead a program of civic education and/or “militant” measures like establishing cordons sanitaires to prevent dangerous ideologies and illiberal parties from corrupting the public sphere (Castiglione & McKinnon 2003; Gauthier 1977).

1.2 Conservative Conceptions of Ideology
The conservative tradition of thinking about ideology has not been so affected by the historical transformations in the meaning of the term. The conservative objection to ideology has remained very close to the attacks leveled by Napoleon Bonaparte (1769–1821) against the idéologues after 1801. Bonaparte, for political reasons, appealed to the Catholic faithful by attacking the idéologues for their rationalism and materialism. Ideology, according to Bonaparte—influenced in this regard by the Romanticism of François-René, vicomte de Chateaubriand (1768–1848)—sought to replace Christian tradition and mores with a rational system of belief and legislation, ordering social and political life by means of the human mind (Kennedy 1979: 358–60).

Ideology, on this account, retains its particular connection to the Enlightenment and is understood to encompass any political project of social or political reform guided by a rational or abstract doctrine. Hume’s objection to “parties of principle” is a recognizable precedent (Hume 1777 [1994]). The conservative approach to ideology expresses a skepticism about modern rationalism (understood as encompassing empiricism and scientific methods generally), especially insofar as rationalism is applied to social and political life (Oakeshott 1962 [1991]).

Conservative thinkers tend to identify rationalist or ideological politics with social engineering, or the illegitimate “assimilation of politics to engineering” (Oakeshott 1962 [1991: 9]). Opposition to social engineering (AKA “scientism”, “positivism”, or “high modernism”) also characterizes liberalism and critical theory (Habermas 1981 [1987]; Hayek 1952; Horkheimer 1968 [1982]; Popper 1944–45 [2002]; Scott 1998). However, conservative thinkers often identify any political appeal to scientific or social scientific knowledge—including that of economics—as social engineering, refusing the liberal distinction between “piecemeal” and “utopian” engineering, or between “the practical management of problems” and “the clash of ideologies”. Hence, Oakeshott claimed that Hayek’s Road to Serfdom amounted to “a plan to resist all planning”, thereby converting resistance to rationalism “into an ideology” (Oakeshott 1962 [1991: 26]). Hence, also, many contemporary conservatives focus on “social constructivism” and “gender ideology” as attempts to make over putatively immutable or natural facts, accessible by “common sense”, as mind-dependent matters for human decision.

However, the conservative critique of ideology is not reducible to antirationalism. According to the conservative account, ideologies are themselves abstracts or abridgments of practical experience. While ideologies present themselves as premeditated principles that can guide and evaluate human activity, they actually simplify, schematize, or rationalize previous activity. This can be appropriate. Like digest books, ideologies provide an easily assimilable form of knowledge. This is useful when “new and politically inexperienced social classes … have risen to the exercise of political initiative and authority” (Oakeshott 1962 [1991, 30]). Ideology becomes essentially inimical to good politics, however, when it derives, not from political experience, but from some other realm of human activity—“war, religion, or the conduct of industry, for example” (Oakeshott 1962 [1991: 54]).

This concern with the colonization of the lifeworld of politics by alien vocabularies and logics of belief is not confined to conservatives (Lukács 1923 [1971]; Arendt 1958; Habermas 1981 [1987]), but it is distinctive of the conservative tradition to identify this concern with the critique of ideology.

1.3 Marxist Conceptions of Ideology
The Marxist tradition has always been torn between two approaches to ideology in the original liberal sense. On the one hand, Marxists have been drawn to a radicalizing strategy, which endorses the aims of liberal Enlightenment but disputes the pedagogical, economic, and civil means endorsed by liberals. On the other hand, however, Marxists have also pursued a debunking strategy by which they aim to unmask the ideological project as either a ruse of power or as a form of mystification. Since the meaning of “ideology” has drifted, these critical strategies have drifted as well, proliferating Marxist and post-Marxist critical theoretical accounts of ideology. Because philosophical discussion of ideology very often begins from claims associated with the Marxist tradition, the multitude of Marxist accounts of ideology has been a major source of both diversity and confusion in the philosophical literature on the topic.

1.3.1 Ideology in Marx and Engels
Karl Marx (1818–1883) read and took notes on Destutt de Tracy’s Elements of Ideology, both in 1844 and again in preparation for writing Capital. To a first approximation, the idéologues’ project was always the exemplary case of ideology for Marx, but he also identified the writings of Young Hegelians like Bruno Bauer (1809–1882) and Max Stirner (Johann Kaspar Schmitt, 1806–1856) as instances of a “German ideology”, and generalized the model further to encompass at least significant aspects of political economy, law, politics, religion, philosophy, journalism, and the military (Marx & Engels 1846 [2017: I/5.120]; Marx 1867 [2024: 409]).

What unites this disparate and capacious catalog of ideologies and ideologists, for Marx, is the “unproductive” or “immaterial” nature of their activity (Mills 1992). According to Marx, the development of the social division of labor and the exploitation of the growing productive powers of labor establishes a basis upon which rests “the state and the rest of the idealistic superstructure” (Marx & Engels 1846 [2017: I/5.115]). The people who occupy themselves with the business of this superstructure are, by dint of “their practical position in life, their business, and the division of labor”, prone to the “illusion” that they are independent and directive of society (Marx & Engels 1846 [2017: I/5.66]). This is supposed to be a sociologically materialist explanation of why ideologists like Destutt de Tracy, Bauer, and Stirner try to instruct and direct society according to ideas. “Ideology”, therefore, comes to name, for Marx, both the “sociological idealism” to which superstructural workers are susceptible and the superstructure itself, which explains their susceptibility (Mills 1992).

In his later work—especially in his Ludwig Feuerbach and the End of Classical German Philosophy (1886)—Friedrich Engels (1820–1895) returns to this sociological explanation of ideology, emphasizing Marxism’s opposition to philosophical idealism. According to Engels, ideology entails an “occupation with thoughts as with independent entities, developing independently and subject only to their own laws”. However, it is actually “the material life conditions” of the ideologist that determines, “in the final analysis”, the course of this development of thought (Engels 1886 [2003: 26.394]). The ideologist, as an idealist, is necessarily ignorant of the economic determinants of their thought process. Hence, the ideologist, as Engels put it in a letter, accomplishes a conscious process of thinking with a “false consciousness”, since their consciousness attributes to the immanent development of ideas what is actually a reflection or effect of the development of material life (Engels 1893 [2004: 50.164]).

Because Marx and Engels never developed an explicit theory of ideology, later scholars have attempted to do so by connecting Marx and Engels’s scattered comments about ideology with three other topics in their writings.

Most prominently, their critique of ideology is but one instance in which they seek to give a sociologically materialist explanation of ideas or consciousness, and this has led many to assimilate their account of ideology and their theory of ideas or consciousness (Cohen 1978 [2001: 376]; McCarney 1980; Torrance 1995). On this approach, the question of ideology is the question of the social determination of ideas or of forms of consciousness (Mannheim 1929 [1936]), and, since Marx and Engels identify classes and class struggle as the fundamental social determinant, their theory of ideology is taken to be their account of how forms of consciousness arise out of and/or serve class interests (McCarney 1980: chap. 1).

However, Marx and Engels’s comments about ideology have also been attached to their claim that “the thoughts of the dominant class are in every epoch the dominant thoughts” (Marx & Engels 1846 [2017: I/5.60]). This has given rise to the “dominant ideology thesis”, according to which, in any (stable) class society, the ruling class formulates (perhaps unintentionally or unconsciously) a set of beliefs that reflect and/or serve their own class interest in continued domination and exploitation, ideas which are largely adopted by the subordinate classes, thereby explaining their continued subordination (Abercrombie, Hill, & Turner 1980; Weber 1921/22 [1954: 336]). While this may appear to be a specification of the identification of ideology with partisan class ideas, it cannot be true that ideology refers both to partisan class ideas in general and to the “ruling ideas” in particular (Abercrombie & Turner 1978: 251). The “class ideas” approach gives rise to an evaluatively neutral account of ideologies, while the “ruling ideas” approach inclines towards a pejorative or critical account of ideology as a bulwark of domination and exploitation.

Finally, some have taken Marx’s discussion in Capital of fetishism to be an extension or development of his account of ideology (Eagleton 1991: 84–88; Geras 1971; Louette 2023; Mepham 1972). On this account, capitalist society, because of the market-imposed separation between production and consumption, necessarily appears to its participants as a mysterious entity operating according to occult mechanisms, a socially produced mystification that should properly identified as ideology. It has been noted, however, that Marx never refers to fetishism as ideology, and that the handful of appearances of the word in Capital refer to the activity of superstructural workers in general and political economists in particular (Balibar 1994: 89; Mills 1992; Roberts 2024). Nonetheless, this has been an important strand of ideology theory in the twentieth and twenty-first centuries, and it does reconnect the concept of ideology to the specifically modern and liberal domain from which it originally emerged, insofar as fetishism is proper to commercial modernity.

1.3.2 Ideology in the Marxist Tradition
Because Marxism developed as a party idea among socialists prior to the publication of the manuscripts on “the German ideology” (see Johnson, 2022), Marxist partisans tended to use “ideology”—in keeping with Marx’s mention of the term in the 1859 preface to A Contribution to the Critique of Political Economy—to refer to the “forms in which men become conscious” of the class contradiction in the economic base of society “and fight it out” (Marx & Engels 1859 [1987: 29.263]). Hence, ideology had both a theoretical aspect and a political aspect. Theoretically, it was the form in which socio-economic matters rose to conscious awareness. Politically, it was the set of fighting creeds by which parties justified their tactics and attempted to win supporters.

Because the capitalist economy was taken to be defined by a tendency of the class conflict between the bourgeoisie and the proletariat to sharpen and to increasingly determine everything else, many Marxists followed Lenin in reading all ideologies through their intersection with this fundamental conflict. This led to the simplifying assumption that people face a binary choice, “either bourgeois or socialist ideology” (Lenin 1902 [1973: 37]). Within a highly polarized political context, this assumption led to the conclusion that “to belittle the socialist ideology in any way, to turn aside from it in the slightest degree means to strengthen bourgeois ideology” (Lenin 1902 [1973: 37]). This, in turn, led to the shorthand equation of “socialist ideology”—i.e., the correct party line—with “class consciousness” and of any ideological deviation from the correct line with “false consciousness”.

In the works of Hungarian Marxist philosopher Georg (György) Lukács (1885–1971), the identification of the (long-term) interests of the proletariat with the interests of humanity as such develops this connection between (non-Marxist) ideologies and false consciousness (Lukács 1923 [1971]; Márkus 1981: 131). This conceptual connection was crucial for the Frankfurt School of critical theory, as well as for the broader tradition of Marxist humanism (e.g., Thompson 1957: 107–9). For Lukács, and even more so for members of the Frankfurt School, false consciousness is not opposed to science—which, in the form of positivism, has been part of the increasing dominance of rational calculation and formal rationality characteristic of capitalist modernity—but to a reflexive, normative, and critical theory (Geuss 1981). For this critical theory, negation plays the special role of unmasking the particular interests that animate and are served by any positive project (including, especially, the party line pursued by Leninist organizations) (Cook 2001). In this tradition, the critique of ideology becomes tightly bound up or even identified with the critique of alienation and reification. Alienation is the process whereby the artifacts of human practices—e.g., the products of labor, or the institutions of the labor movement—escape from the control of and turn against their creators (James 2012; Mattick 1969). Reification is the process whereby the relational totality that constitutes society is obscured by its seemingly independent, quantifiable, and non-relational parts (Lukács 1923 [1971]).

It is from this perspective that it makes sense to interpret Marx’s discussion of the fetishism of commodities as an extension and development of the analysis of ideology into the analysis of impersonal forms of social domination (Balibar 1994; Postone 1993). It also makes sense to flesh out the discussion of ideology either by reference to Hegel’s criticisms of the understanding as he opposes this operation of the mind to dialectical reason (Abazari 2019), or by reference to Freud’s analysis of the processes of the unconscious, which allows critical theorists to diagnose ideology as the substitute gratification of real needs (Fromm 1968: 63).

In contrast to this line of development, another Marxist tradition of thinking about ideology is represented by the work of the Italian journalist and philosopher Antonio Gramsci (1891–1937). Gramsci, emphasizing the texture of political conflict, developed the idea of ideology as class consciousness into an account of culture and institutions as the terrain of politics (Gramsci 1971). According to Gramsci, a socialist party, in its efforts to establish leadership of the working classes, articulates Marxist theory with other elements of popular consciousness and popular culture. If this process is successful, the party will advance to the head of a coalition of different class forces (a “historic bloc”) and be positioned to exercise hegemony over society as a whole. This line of thinking leads away from the critique of ideology as false consciousness and towards the descriptive analysis of the (contradictory) elements and tendencies of cultural traditions and institutions.

Although this tradition has been developed primarily in histories of popular culture (Guha 1983; Hobsbawm 1959 [1963]; Thompson 1963 [1968]), and in interdisciplinary cultural studies (Denning 2004; Hall 2016; Hall et al. 1978 [2013]), it has also received significant philosophical elaboration, beginning especially with French philosopher Louis Althusser (1918–1990). Althusser, like the Frankfurt School of critical theory, associated ideology with the reproduction of the social order and sought to use psychoanalytic concepts, including that of the unconscious, to develop the theory of ideology. However, he drew on the structuralist psychoanalysis of Lacan to argue that ideology “interpellates individuals as subjects” (Althusser 1995 [2014: 188]). What Althusser meant by this formula is that ideology constitutes and calls to us as agents, providing the framework and orientation within which we experience ourselves (and one another) as centers of action beholden to normative demands. According to Althusser, ideology can perform this function only because it is always embedded in ritual practices and institutions.

Althusser also affirmed two other theses about ideology that sharply differentiate his approach from that of the Frankfurt School. First, because of its central function in subject-formation, ideology is not a symptom of class struggle or a feature of human society that could ever wither away, even in an ideal communist future, but is instead “a structure essential to the historical life of societies” (Althusser 1965 [1969: 232]). Hence, his approach is not a critique of ideology as such, and “ideology” is not a pejorative term for Althusser. Moreover, rather than opposing ideology to a critical theory that is essentially reflexive and normative, Althusser opposes ideology to science, understood as a subject-less body of concepts that make possible the production of knowledge within a specific domain (Lewis 2009 [2022]).

Althusser’s claim that ideology interpellates individuals as subjects has been the most influential aspect of his theory. The Swedish sociologist Göran Therborn has developed it away from its functionalist tendencies by stressing the irreducible plurality of forms in which agents might “qualified” (both specified as a particular type and authorized to act in that capacity) (Therborn 1980 [1999]). Judith Butler both drew upon Althusser’s understanding of subjectification in their account of the production of gendered subjects and reacted against his account of interpellation—in a manner broadly consonant with critical theory—by emphasizing the ever-recurrent possibility of de-subjectification (Butler 1997). Quill Kukla has argued that Althusser provides an account of how subjects are inducted into the normative “space of reasons” by being misrecognized as already bound by the normative conventions into which they are being recruited, an account that aligns Althusser with Sellars’s account of epistemic authority (Kukla 2000). Sally Haslanger has directly taken up Althusser’s argument for the material existence of ideology in institutions and practices, a theme she has developed in her account of “cultural technēs” (Haslanger 2015, 2017, 2019, 2021).

2. Mapping Ideology
Due to the heterogeneity of the traditions of ideology theory, efforts to mobilize the concept must begin by establishing the sense in which the word is being understood. A first-cut specification is to distinguish between evaluatively-neutral or descriptive conceptions and evaluatively-critical or pejorative conceptions. (For critical conceptions, one may then ask about the target of the criticism. What is wrong with ideology? This question will be the focus of Section 3 below.) A second specification, however, distinguishes the location of ideology—what Shelby calls the “unit of analysis” (2003) and Balkin calls the “object of study” (1998: 101). Does “ideology” pick out a type of consciousness, a set of ideas, the world of culture, a set of expectations, or something else? Finally, there is the question of scope. Does ideology encompass all ideas, for example, or only those ideas with a certain content, a certain social function, a certain etiology, and/or a certain set of effects?

One must establish where a given conception of ideology fits in a classificatory scheme of all possible conceptions in order to know how it relates to any other conception. However, this classification, while necessary, is less interesting than the considerations that lead an author to classify ideology as they do in the first place. Authors are rarely explicit about this, and a fully explanatory theory may not be possible. However, two important variables can be established. First, the valence of ideology tracks attitudes towards partiality and partisanship. Critical conceptions of ideology indicate a commitment to moral and/or epistemic impartiality, while descriptive conceptions indicate greater openness to the moral and/or epistemic virtues of partisanship. Meanwhile, intuitions about the location of ideology track theories of agency. Adherents of the standard theory of agency as rational-intentional action are drawn to cognitivism regarding ideology (Elster 1983; Shelby 2003; Stanley 2015), while those who downplay the role of representational mental states in agency, such as enactment or embodiment theorists, will tend to locate ideology in habitual schemas of meaning, cultural technēs, affects, or bodily practices (Althusser 1995 [2014]; Hall 2016; Haslanger 2015; 2019; Hennessy 1993; Jaeggi 2009; Therborn 1980 [1999]).

2.1 The Valence of Ideology
The most obvious divide in uses of “ideology” is between uses with a pejorative or critical sense and uses with a neutral or descriptive sense (Geuss 1981; Haslanger 2021; Shelby 2003). While a positive or laudatory sense is also logically possible, only in the case of the idéologues could ideology in general be positively valorized. As soon as the word became a common rather than a proper noun—as soon as there were many ideologies rather than only one—any attempt to positively valorize ideology used the term in a descriptive sense while distinguishing between good and bad instances on independent moral, political, or epistemic grounds (contra Geuss 1981: 22–26).

Although it is common to associate the Marxist tradition with the critical conception (e.g., Shelby 2003: 156–57), critical and descriptive conceptions are equally at home both within and outside of Marxism. Marx and Engels deployed “ideology” critically, but only because they found the strategy of the ideologists ridiculous; their conception is descriptive, even if their use of the term takes on pejorative color. Lenin, Gramsci, and Althusser all developed descriptive concepts. Conservatives, liberals, and non-Marxist socialists are similarly divided.

Rather than broad political traditions, it is helpful to associate the division between critical and descriptive valences with differing commitments to epistemic and/or moral impartiality or universalism. The tradition of Frankfurt School critical theory, which is closely connected to the project of Ideologiekritik, is strongly committed to both moral universalism and to a Hegel-influenced epistemological claim that, at least for social phenomena, knowledge can only be achieved from the perspective of the totality (Jay 1984). From within this tradition, therefore, ideology appears essentially as false (because partial) consciousness, which must be transcended through criticism. For anyone committed, on the other hand, to the notion that knowledge and/or right action can or must emerge from some particular perspective or worldview, ideology appears as a necessary medium. Intermediate positions are possible, too. For instance, “ideology” may be reserved as a pejorative term for an excess of partiality or partisanship, say, a “closed-system” worldview that insulates the adherent against novel or disconfirming experiences.

2.2 The Location of Ideology
A second variable in the study of ideology is where theorists look for instances. Broadly speaking, theories of ideology are divided between cognitivist and culturalist approaches. Cognitivist accounts locate ideology among ideas, beliefs, “forms of consciousness”, mental representations, or propositional attitudes (Elster 1983: chap. IV; 1985: 459–65; Habermas 1981 [1987]; Mills 2005; Shelby 2003; Stanley 2018). Culturalist accounts locate ideology among “forms of life”, habits, practico-symbolic schemas, or “cultural technēs” (Balkin 1998; Haslanger 2015, 2019, 2021; Jaeggi 2009, 2014 [2018]; Therborn 1980 [1999]).

Among cognitivist accounts, some restrict ideology to “beliefs and values consciously entertained by some individual or individuals” (Elster 1985, 462). More commonly, however, it is recognized that an ideology “may be only implicit in the behavioral dispositions, utterances, conduct, and practices of social actors”, and that ideological beliefs “are quite frequently confused and may be expressed only in the form of stereotypes, clichés, and fragmented narratives” (Shelby 2003: 161). Cutting across this distinction between explicit and implicit beliefs (Plamenatz 1970: 17) is the distinction between beliefs and values—between beliefs “about the world as it is” and beliefs “about the world as it ought to be” (Elster 1985: 466)—as well as the further distinction between beliefs about how something can be done and beliefs about how something ought to be done (Stanley 2015). These distinctions are often submerged in a more generic cognitivist claim that, e.g., ideology consists of “the descriptive vocabulary” or “language of consciousness” appropriate to everyday social life (Fields 1990), or that “ideology” names “partisan group ideas” (Mills 1992).

Culturalist accounts of ideology are motivated by some of the same observations that incline many cognitivists to include implicit beliefs in the field of ideology. They go further, however, by foregrounding “‘non-discursive’ elements,’” such as “characteristic gestures, rituals, attitudes, forms of artistic activity, etc.” (Geuss 1981: 6); or else, they find ideology in “manifestations of a particular being-in-the-world of conscious actors” (Therborn 1980 [1999: 2]); or, alternatively, they identify ideology as a species of “cultural technē”, the “network of social meanings, tools, scripts, schemas, heuristics, principles, and the like which we draw on in action, and which gives shape to our practices” (Haslanger 2017: 155). Regardless of the language used, the intent is to locate ideology in the pre-reflective and habitual dimensions of human agency and social life.

Althusser might represent a third option, neither cognitivist nor culturalist, since he seems to locate ideology in practices, rituals, and “apparatuses” (or institutions), which are, according to him, the material existence of ideas (Althusser 1995 [2014: 260–61]). Some would include Foucault (1975 [1995]) and Butler (1997) in this lineage (Montag 2013; Lepold 2018). However, cognitivists and culturalists are at pains to demonstrate that their accounts of ideology are also both material and practical (e.g., Stanley 2018: 508–9; Haslanger 2018, 2022). More work would have to be done to demonstrate that the Althusserian approach is genuinely distinct in this regard.

Whether a theorist is drawn to a cognitivist or a culturalist account of ideology seems to depend on whether they adhere more closely to the standard conception of agency as rational-intentional action or incline more towards alternative conceptions of agency that emphasize pre-reflective and embodied aspects of human action.

This suggests a significant consensus underlying the more obvious disagreements. Ideology shares basic features with human agency, and to conceive of anything—a text, an idea, a convention, or a cultural form—as ideology is to bring it into focus as either a condition of or burden on human agency. When this regularity is combined with the regularity noted above—that ideology denotes partiality, “in one of the two senses of that term, expressed in French by ‘partiel’ and ‘partial’” (Elster 1983: 145–46)—the disorder of ideology theory as a field of study becomes intelligible as expressing the diversity of theorists’ intuitions about the roles that partiality and partisanship play (or should play) in human action.

2.3 The Scope of Ideology
Regardless of disagreements over the valence and location of ideology, most accounts agree that certain scope restrictions must be placed on ideas or cultural elements in order for them to be rightly classed as ideological. Ideology pertains to social life, in one manner or another. Four types of social pertinence, in particular, are commonly indicated. Most universally, ideology is held (1) to have social causes and (2) to have social effects. These stipulations are only scope restrictions for cognitivists, since all culture, by definition, has social causes and effects.

That ideology is “explained by social facts” (Elster 1985: 459) is uncontroversial, with disputation reserved for the questions of which facts count as social facts and which social facts are explanatory (Haslanger 2022). Not all cognitivists accept that ideology has social effects, however. This is one reading of Marx and Engels’s original criticism of ideology: ideology is an effect of the ideologists’ placement in the social division of labor, but this placement also condemns ideology to inconsequence, since ideologists have no access to or ability to affect the mode of material production.

More controversially, ideology is often thought (3) to have social content, or to be about society. It is doubtful that culture, as the culturalist ideology theorists intend it, has content or about-ness in this sense. If it does, then it seems that culture reduces to “a network” of beliefs and values (Shelby 2003: 159), with the consequences that (a) the culturalist approach to ideology reduces to the cognitivist approach and (b) all culture is “about” society, since it regulates and makes sense of social practices. Therefore, this social content proviso is a scope restriction that applies only to cognitivist accounts. However, again, not all cognitivists accept this scope restriction. Theological and metaphysical beliefs are classic objects of ideology critique, despite lacking (explicit) social content (Forster 2015).

Finally, ideology is frequently thought (4) to have a social function. (This is usually the rationale for classing theology and metaphysics as species of ideology.) What this function might be varies. For many Marxists, “part of what makes a form of social consciousness ideological is the role it plays in establishing or reinforcing relations of oppression” (Shelby 2003: 173; compare to Forster 2015: 817; Freeland 2000: 368; King 1991). For Althusser, however, ideology has no special connection to oppression or exploitation; instead, “all ideology has the function (which defines it) of ‘constituting’ concrete individuals as subjects” (Althusser 1995 [2014: 262]). The attribution of functions is controversial, since it goes beyond the identification of predictable social effects to claim that an ideology exists or has the characteristics it has because of its predictable social effects. To escape backwards causation, any function attribution must, to be complete, identify a causal feedback or selection mechanism, either intentional or systematic but unintentional, whereby ideologies that successfully fulfill their social function come into being, propagate, and are resilient, while those that do not are never produced, are winnowed out, or fade away. The debate on the appropriateness of functional explanation in social theory, and in the theory of ideology especially, is long-standing and complex (important entries in the debate include: Carling 2002; Cohen 1978 [2001]; Elster 1985, 1986; Pettit 1996b; Rosen 1996; Wood 1986). Nonetheless, using a social function to restrict the scope of ideational or cultural phenomena to be captured by the theory of ideology remains common (Haslanger 2019; Shelby 2003).

3. Varieties of Ideology Critique
Ideology can be criticized along three lines. It might be criticized for exhibiting or leading to faults of rationality, or for being interest-undermining. It might also be criticized for exhibiting or leading to faults of morality, or for being justice-undermining. Finally, it might be criticized for exhibiting or leading to epistemic faults, or for being knowledge-undermining. Each of these criticisms might be either unconditional or conditional. For example, one ideology critic might dismiss all ideology as interest-undermining partisanship, while another might differentiate between interest-serving and interest-undermining partisanship, criticizing only the latter but referring to both as “ideology”.

Regardless of the fault, the criticism of faulty ideology is not concerned with individual cases of irrationality, immorality, or ignorance, except as these are, at least, indicative or typical of wide-spread irrationality, immorality, or ignorance with a social etiology. Ideology critique is a form of social criticism, and it can only proceed by showing that a certain form of rational, moral, or epistemic failure is both widespread within a society or a group and traceable to the common social situation of its sufferers. Two types of social groups attract the special focus of ideology critics: the “negatively privileged” or “subordinate” (henceforth, “the dominated”) and the “positively privileged” or “ruling class” (henceforth, “the dominant”).

3.1 Rationally Faulty Ideology
When ideology is criticized for exhibiting or leading to faults of rationality, the substance of the criticism is that an agent or group of agents is in the grip of (a faulty) ideology just to the extent that, due to certain social facts, they are partial to or partisans of a state of affairs or course of action that is contrary to their own interests.

The conceptual connection between rationality and interests is tight; an interest can be defined as “the most rational course of action in a predefined game, that is, a situation in which gain and loss have already been defined” (Therborn 1980 [1999: 10]). However, the connection between interests and partiality or partisanship seems equally tight; interests are rational strategies, and therefore seem to be inherently instrumental (“partisan”) and perspectival (“partial”). This suggests that criticisms of interest-undermining ideology will necessarily be conditional criticisms based on a descriptive conception of ideology. An interested being is by definition partial to their interests, and so it seems to follow that any interested being will have ideological beliefs, and only a subset of those ideological beliefs—the ones that somehow do not fulfill their function of supporting the being’s pursuit of their interests—pose a problem.

In fact, however, this does not obtain. Instead, the literature displays a strongly bimodal distribution of criticisms of interest-undermining ideology. A fully substantive conception of rationality identifies rational strategies with the end to be achieved (with well-being or human flourishing or the like). On this account, rationality just is rational autonomy or rational self-determination (Ng 2015). Therefore, critics of interest-undermining ideology go one of two ways: either they embrace a formal or instrumental conception of rationality and a descriptive conception of ideology, or else they embrace a substantive conception of rational autonomy and a pejorative conception of ideology.

That partisanship or partiality can lead us into irrational beliefs and behaviors is not seriously in doubt. For instance, a suburban car commuter may vote for a mayoral candidate who praises the suburbs and car culture and promises to slash funding for public transit and expand downtown parking, even though the voter’s interest as a commuter—in being able to drive quickly and efficiently to work—would be better served by decreasing demand for roadspace. Being publicly affirmed in one’s choices and one’s identity can override or derail a rational assessment of the available options.

Two types of criticism should be excluded from this category of ideology critique, however. First, prioritizing short-term over long-term interests is not a fault of rationality, per se, and cannot furnish the basis for a critique of interest-undermining ideology, even if it may be an incident of that critique. Strategizing for long-term interests is beset by higher uncertainty in a way that cannot be eliminated. Second, prioritizing individual interests over group or collective interests is not itself a fault of rationality, since it concerns a divergence between two different games. It is one thing to say that the individualist cannot achieve their own ends by their individualist strategy; it is another to say they should be pursuing other, solidaristic ends (Gaventa 1980: 90). Criticism of individualist strategies is better conceptualized, therefore, as aimed at justice-undermining ideology rather than at interest-undermining ideology. (A practical difficulty for the proponents of a fully substantive conception of rationality is that this distinction is, on principle, not available to them.)

3.1.1 Are the Dominated Especially Prone to Rationally Faulty Ideology?
Here a special case must be considered. The belief that the dominated are partial to or partisans of their own subordination, contrary to their own interests, is widespread in the literature on ideology. It is often taken for granted that there is a

tendency of the oppressed and exploited classes in a society to believe in the justice or at least the necessity of the social order that oppresses them. (Elster 1983: 146; compare Heath 2000: 363)

In this context, the theory of ideology is supposed to answer the question:

why do the victims of oppression not rise up against their oppressors (and sometimes not even seem to show any inclination to do so)? (Finlayson 2015: 135; compare Lafont 2023: 391)

It is supposed to answer this question by reference to the possession, by the victims of oppression, of an ideological false consciousness, “a distorted view of social practices and institutions” (Lafont 2023: 391).

Since this answer only pushes the question back one step (Cudd 2006: 55), the puzzle become: why do the dominated have a false consciousness, or “why do oppressed groups accept the ideology of their own inferiority?” (Stanley 2015: 222). This perspective attributes to the members of dominated groups a belief (or set of beliefs) about themselves and/or their situation the possession of which is contrary to their own interests (Geuss 1981: 45). While there is nothing odd about claiming that an individual may be mistaken about a matter that affects their own best interests, it seems puzzling that a significant group or class of people would commonly accept a false belief that undermines their interests, since this implies a general, unidirectional, and stubborn failure of rationality. Hence, Dotson (2018) has dubbed it the “Not-Very-Bright Thesis”.

Numerous mechanisms for resolving this puzzle have been proposed in the literature, but substantial doubts have also been raised about each of them.

3.1.1.1 Sociological Mechanisms
By far the most commonly proposed mechanism is pedagogical indoctrination or coercive socialization (Althusser 1995 [2014: 142–47]; Bartky 1990; Berger & Luckmann 1966: 121–25; Gaventa 1980: 22; Guha 1997: 165–69; Stanley 2015: 234–37). This is the basis of the dominant ideology thesis in its classic form: the dominant group, monopolizing the means of mental production, ensures that members of the dominated group internalize the dominant belief system, which includes as a component the (false) idea that members of the dominated group are incapable of self-rule or freedom and are better off in their current, dominated position.

Another commonly proposed mechanism is elite control of the media and of expert authority. For example, it is claimed that, in any stable society, “the central moral, political and economic ideas that dominate discussion in the mass media and in the corridors of power” will generally “promote the interests of the ruling class of that society” (Leiter 2004 [2005: 159]). Similarly, Stanley claims that, since belief is not under our direct voluntary control, dominant group control of the authoritative narratives and sources of testimonial evidence will result in dominated groups being insulated from any alternative ideology, and therefore believing the dominant account of their inferiority (2015: chap. 6).

The effectiveness of these sociological mechanisms has been doubted, however, on multiple grounds. First, since the socioeconomically disadvantaged tend to be less exposed to elite media, have lower educational attainment (and hence less exposure to elite expert discourse), and exhibit lower levels of trust in conventional expert opinion, it is unclear why these mechanisms would not be primarily effective at securing coordination and cohesion among the dominant (Scott 1990: chap. 3). Moreover, the legitimating discourses of the dominant consists, at least in part, of the rules, promises, and normative ideals that the present order is said to instantiate, realize, or pursue, and these can be used to make demands of the dominant (Gramsci 1971: 161; Piven & Cloward 1977; Scott 1990: chap. 4; Willis 1977). This has given rise to an adage for radical organizers, “Make the enemy live up to their own book of rules” (Alinsky 1971: 128). Third, as feminist standpoint epistemologists have argued, the dominated can often draw upon the direct evidence of their lives and the lives of those similarly positioned in order to contest the guidance of dominant experts (Hartsock 1983; McKinnon 2018). Finally, the dominant belief system is very easily contradicted in two ways, with mental resources that would seem to be universally available. A belief about a normative social hierarchy might be contradicted either by reversal (e.g., “the last shall be first, and the first last”) or by negation (e.g., “There is neither Jew nor Greek, there is neither slave nor free, there is neither male nor female”). Such reversals and negations of dominant moral frameworks are widely attested among subordinate groups historically, and this seems to call into question the thesis that the dominated are, as a rule, susceptible to internalizing interest-undermining ideological beliefs (Scott 1977a, 1977b, 1985: chap. 8).

3.1.1.2 Psychological Mechanisms
Interest-undermining false consciousness might arise, instead, from the operation of various psychological mechanisms within the social context of subordination (Cudd 2006).

Elster suggests, for example, an optical illusion or unwarranted generalization, whereby the dominated attribute to the whole of society a pattern or logic that obtains only locally, in the neighborhood of society with which they are most familiar (1983: 145–46). For instance, a serf may recognize that they depend for their well-being and protection on the local lord, and generalize from this to the conclusion that a society of peasants without lords would be intolerably miserable and violence-prone. However, Elster only gives examples of academics either making this sort of error themselves or stipulating that subaltern populations “had to believe” something like this (1983: 146, including n. 12).

Another proposed mechanism is “stereotype threat”, a widely-studied phenomenon in social psychology. In a condition of stereotype threat, members of a group subject to a negative social stereotype are more likely to act in ways that seem to confirm the stereotype. It has been suggested that this type of self-fulfilling prophecy might underpin the adoption by subordinate groups of ideological beliefs regarding their own abilities (Stanley 2015: 239–40; compare Cudd 2006: chap. 6). However, anxiety about confirming a negative stereotype can affect performance without any supposition that negative beliefs about oneself have been internalized. Disidentification with dominant modes of achievement—the hypothesized route by which negative stereotypes become self-fulfilling prophecies—implies a mode of evaluation contrary to the dominant mode, which is the opposite of what the theory of ideological incorporation predicts.

A third psychological mechanism that features widely in the literature is preference adaptation, the spontaneous alteration of desires so as to minimize cognitive dissonance or frustration (Elster 1983). In its weaker form, this entails resignation, believing that the absence of some previously desired social state of affairs is necessary, even if regrettable. In its stronger form, it entails the belief that the absence of some previously desired social state of affairs is just or good. This psychological process is often referred to, generically, as “naturalization” or “reification”, and the stronger version is sometimes picked out as “legitimation”.

Whether preference adaptation is a significant phenomenon, or a case of interest-undermining false consciousness, may be doubted, however. In the first place, genuine instances of adaptive preferences would have to be sorted out from cases of preference falsification, the misrepresentation of one’s desires in the face of perceived pressure or opposition from others (Kuran 1995). The dominated have good reason to falsify their preferences in most circumstances (Jacobs 1861 [2015]; Scott 1985, 1990). Even if genuine cases of adaptive preferences can be identified, it is not clear that they are cases of irrationality. If a desired social state of affairs really is out of reach, changing one’s beliefs about its desirability may be the rational thing to do. Again, one must ask whether the adaptive preference would adapt again if an avenue to the previously desired outcome opens up. If it would—if, in the terms of the fable of the fox and the sour grapes, the fox would decide that the grapes were sweet after all were a means of attaining them presented—then the adaptive preference is not a significant phenomenon.

3.1.1.3 Conclusions
Too much of the literature on interest-undermining ideology among the dominated simply assumes what would have to be demonstrated, that widespread ideological false consciousness leads the dominated “to believe that their place is deserved” (Cudd 2006: 180). The “massive fact of history that the values and the beliefs of the subjects tend to support the rule of the dominant group” (Elster 1983: 166) is not so much a fact as an illusion: the only evidence for it is that the subjects are not generally in open revolt, but this is what ideological false consciousness is supposed to explain, so it cannot count as evidence that the beliefs of the subjects are, in fact, partial to their subordination.

As Finlayson points out, if we accept that people are

imperfectly rational, imperfectly informed and somewhat confused, often fearful, subject to more or less reasonable forms of hope, prone to a condition sometimes termed learned helplessness, motivated to a considerable extent to pursue what we would think of as their own interests, but also motivated in ways that cannot plausibly be reduced to this, for example, by aesthetic, altruistic, experimental or self-destructive urges,

then it is not really

a mystery that such beings, having been born into a social order that oppresses them, often do not rise up and overthrow it. (2015: 138)

No appeal to widespread false consciousness or ideological incorporation is necessary to explain the persistence of oppressive social systems.

It might also be argued that the entire dispute about “the dominant ideology thesis” was a red herring to begin with. Those who proposed that ideology was a crucial factor in the persistence and stability of systems of domination and exploitation were, in the first instance, revolutionaries and radical reformers. What they meant, arguably, was not that the dominated and exploited are partisans of the system that dominates and exploits them, but that they are not yet partisans of revolutionary or radical projects that aim to abolish that system (Lenin 1902: chap. 2 [1973]).

From this perspective, the diagnosis of “false consciousness” among the dominated means only that the patient is not an adherent of the best or correct political program or practices, as identified by the diagnostician. In this sense, ideology critique is endemic to popular, contestatory politics, which consists in advocating one ideology (partisan view) against rivals and trying to win over the uncommitted (Freeden 1996), in part by arguing that adherents of rival ideologies are deluded or mistaken about the rationality of their own partisan views.

3.1.2 Are the Dominant Especially Prone to Rationally Faulty Ideology?
If the ideology-critical argument that the dominated are prone to accepting interest-undermining ideology is questionable, however, there appears to be better evidence for the converse theory: that the dominant are, due to certain social facts, prone to accepting interest-undermining false beliefs or myths, at least in certain instances. (That the dominant might also adhere to interest-promoting ideological beliefs is no surprise; any criticism of these beliefs will base itself on moral or epistemic grounds.)

It is an ancient complaint about tyrants and princes that they surround themselves with flatterers and sycophants, but the prevalence of the complaint suggests a structural cause: flattery is a strategy among the dependent, since it may help them retain their office, status, or even their life (Kapust 2018). The result is that the dominant may find themselves surrounded by unreliable narrators, whose testimony about states of affairs is regularly distorted by their expectations about what the dominant want to hear. This is a politically salient instance of preference falsification, since it may lead the dominant to seriously underestimate opposition to their rule (Kuran 1995: chap. 15). By this mechanism, then, the dominant may be prone to accepting false beliefs about their own security, their own virtue, and their own popularity, false beliefs that undermine their rationality.

The same factors may lead to the dominant being insulated from rational arguments, such that they are less able to be rational themselves. This, at least, was Frederick Douglass’s claim about slaveholders: because they never encourage frank conversation with the slaves and other dependents who surround them, and because their every whim is law, “reason is imprisoned” on a plantation, and the slaveholder never learns how to control their own passions or to pursue a reasoned policy (Douglass 1855 [2003: 61–62]).

3.2 Morally Faulty Ideology
When ideology is criticized for exhibiting or leading to moral faults, the substance of the criticism is that an agent or group of agents is in the grip of (a faulty) ideology just to the extent that, due to certain social facts, they are partial to or partisans of a state of affairs or course of action that is unjust.

If, however, an agent is partial to or a partisan of an unjust social system because that system seems to them to operate in their interest—to deliver or secure for them important material and/or social goods—then this situation is not generally considered to call for ideology critique. A conflict between personal interests and the demands of justice is too widely taken for granted to call forth critical social theory in most cases. There are exceptions to this rule. Theorists who claim that we have moral or ethical interests—a rational interest in doing the right thing or in acting according to our duty (e.g., Stanley 2015: 266)—may assimilate the criticism of self-interested support for injustice to ideology critique. Similarly, theorists who identify instrumental rationality as an inappropriate governor of social interaction may conclude that “a state of society in which humans treat themselves and others as if they were things, not people” is a reified state of society, and is the proper object of ideology critique (Geuss 2008: 123). Note, however, that reification, even if it has bad moral consequences, is subject to ideology critique for the epistemic fault of mistaking a socially contingent circumstance for something necessary (Geuss 1981; Habermas 1981 [1987]; Honneth 2008).

The critique of justice-undermining ideology focuses on cases in which, because of how they are socially situated, and perhaps contrary to their own wishes and even efforts to the contrary, agents are complicit in injustice.

Accounts of structural injustice and findings from social psychology (Young 2011; Jost 2020), have been mobilized in efforts to explain why people often accept an unjust social and political status quo. The question of ideological complicity is not, as in the critique of interest-undermining ideology, directed at one group in society—“Why do you support a system that exploits you?”—but is addressed to everyone in general—“Why do you accept and help to reproduce an unjust social system?” The issue animating much of the recent writing on ideology is, thus, “how we all become agents of injustice” (Haslanger 2015: 12, n. 4).

In this general form, however, the question admits of only one answer: it depends. The causes and effects of unjust social practices are often obscure or controversial. There is often great uncertainty or disagreement about what practices would be genuinely disruptive of structural injustices, even where there is agreement about the existence of such injustices. Bucking established norms and practices is likely a costly course of action, and when these costs are compounded by the uncertainty that one’s actions will contribute to effective improvements, inaction (or minimal, conciliatory forms of protest) may seem the most prudent course (Finlayson 2015; Heath 2000; Sankaran 2020).

However, this combination of ignorance, uncertainty, disagreement, and costliness does not make a significant appearance in the most prominent recent critical accounts of ideology (by way of contrast, see Dworkin 1983). In general, the literature on the formal features of social norms and the literature on ideology have parted ways (Sankaran 2020). This does not mean there are no possibilities for fruitful engagement, however. Ideology critique might be understood as an effort to assist agents stuck in a suboptimal equilibrium, either by redescribing the action-relevant characteristics of the options they face so as to induce a change in preferences, or by highlighting discounted possibilities for action, thereby expanding the set of feasible options (Barrett 2022). The former would be appropriate where agents are complicit because they do not know that or how their actions contribute to injustice. The latter would be appropriate where agents are complicit because they do not know how they can act so as not to contribute to injustice. Both forms of ameliorative ideology critique depend upon social theory (Haslanger 2019, 2022); they require that the ideology critic possess knowledge of alternative social norms or forms of action that are not—or not as significantly—complicit in injustice.

3.3 Epistemically Faulty Ideology
When ideology is criticized for exhibiting or leading to epistemic faults, the substance of the criticism is that an agent or group of agents is in the grip of (a faulty) ideology just to the extent that, due to certain social facts, they are partial to or partisans of untruths or other epistemically dubious beliefs. This is the most diverse category of ideology critique. Generally speaking, an ideology is criticized for exhibiting or leading to either (1) faults of scope, (2) faults of modality, or (3) faults of reflective endorsement.

Partisanship or partiality leads to a fault of scope when an agent, because of their social position, background, or interests, mistakes something particular or local for something universal or general, or vice versa. It is commonly asserted, for instance, that ideology represents a particular interest as a universal interest (e.g., Celikates 2006: 33; Eagleton 1991: 56–58; Geuss 1981: 14; Žižek 1994: 10). This is very similar to the ideology critique that Charles Mills directs at the social contract tradition. The figure of the social contract is exemplary of the problem diagnosed by the epigraph to The Racial Contract: “When white people say ‘justice,’ they mean ‘just us’” (Mills 1997). A fault of scope critique is also exemplified by Elster’s analysis of cases in which members of social classes exhibit a “proneness to unwarranted generalization” that leads to making “errors about social causality” specific to the class’s position in the economic structure of society (1983: 144–49). The same could be said of some of the epistemic errors encoded in the formation and use of stereotypes, insofar as these give a universal and/or normative scope to select, privileged observations (Cudd 2006: chap. 3; Haslanger 2014).

Partisanship and partiality can also lead to the opposite epistemic problem, however: taking general or universal aspects of human social life to be the special property of one’s own social group, or acting as if they were. This may appear in political life as a variety of what Bertrand Russell called “emotive conjugation” (as stated by Griffin 1999), as, for example, “we pursue justice, you fetishize procedures, they are fanatics”. In a case of ideological bias, we may attribute to our own group or form of life an exclusive or privileged exercise of certain generically human capacities or virtues, simply because we experience and understand our culture and customs from within.

A different strand of epistemic ideology critique focuses, instead, on faults of modality. Partisanship or partiality leads to a fault of modality when an agent, because of their social position, background, or interests, believes something contingent to be something necessary, or vice versa. The most widely-encountered instance of this type of epistemic ideology critique is the critique of reification or naturalization, in which institutions, conventions, and other “social constructions” are taken to be or treated as if they were natural, eternal, inevitable, or otherwise not amenable to human alteration or control (Berger & Luckmann 1966: 88–92; Habermas 1981 [1987]; Honneth 2008; Lukács 1923 [1971]; Shelby 2003: 177; Thompson 1957). This is also called, by Geuss, “an ‘objectification’ mistake” (1981: 14). Critiques of ideological reification or naturalization tend to be quite sweeping, but they are akin to a critical strategy that is used more locally and on a smaller scale: the diagnosis of self-fulfilling prophecies or self-confirming beliefs. In these cases, believing something (or acting on a certain belief) generates evidence for the belief. If the police believe that Black motorists are more likely to possess illegal drugs, and act on that belief by implementing a form of racial profiling, this will generate a disproportionate number of drug arrests among Black motorists, a fact that may then be cited as evidence for the belief that Black motorists are more likely to possess illegal drugs.

Again, ideology can also be criticized for the opposite modal error: the necessary can appear to be contingent. Scapegoating and casting blame can be a way of attributing to individual agents’ contingent choices what would better be understood as a necessary outcome of background social facts and institutional dynamics. A feature of large-scale social interaction is that incentive structures may make it inevitable or nigh inevitable that some agents will engage in a particular behavior, even if it is impossible to say which individual agents will engage in that behavior. To use an example from Philip Pettit, that an increase in unemployment might explain a subsequent increase in crime illustrates a form of structural explanation in the social sciences that neither violates the commitment to intentional psychology nor runs afoul of scientific norms of explanation in other domains (Pettit 1996a: pt. II; building on Jackson and Pettit 1990, 1992; see also: Haslanger 2016). Therefore, a political campaign against crime that vilifies individual figures might be criticized as knowledge-undermining ideology on the grounds that it makes what is necessary (given the state of the economic system) appear to be contingent on an identified set of agents (Hall et al. 1978 [2013]).

Partisanship or partiality leads to a fault of reflective endorsement when an agent, because of their social position, background, or interests, believes x only because of some social facts, where these social facts (a) are not reasons for believing x, and (b) are such that, awareness of these facts would undermine the agent’s belief in x. This is the sort of fault that Engels intended with his claim that ideology is a process accomplished with a false consciousness: the economic facts and motives that actually cause the ideologist to believe certain things cannot be acknowledged by the ideologist without undermining the beliefs they gave rise to. Another prominent example of this form of ideology critique is the critique of legitimation. Habermas and other critical theorists argue that, if a belief in the legitimacy of a form of domination could not be arrived at or sustained under conditions of free and open deliberation, then the belief suffers a fault of reflective endorsement. The belief is only sustained by a background condition of coercion or fraud, the acknowledgment of which is incompatible with maintaining the belief (Habermas 1981 [1987]; see also: Geuss 1981: 26–44). Others have drawn on Susan Stebbing’s discussion of “cherished beliefs”, beliefs that are “pleasant to hold” (Stebbing 1939: 40), to characterize ideological beliefs as beliefs that are difficult to rationally revise in the light of new evidence because they encode identity-related expectations about the payout of social interactions (Stanley 2015: chap. 6). While Stanley’s account of ideology is descriptive, it bears comparison with critical concerns about group polarization and “echo chamber” effects, in which in-group signaling overrides normal processes of belief revision.

Many epistemic critiques of ideology combine these modes of fault-finding. For instance, Aytac and Rossi have argued that ideas that are not readily challenged tend to be epistemically inferior, and the self-justificatory discourses of the powerful tend to be shielded from contestation by the very power they justify (2023). Therefore, these ideologies suffer from a fault of reflective endorsement—being adhered to only because they are shielded from criticism—but they will also typically exhibit faults of both modality and scope. Similarly, if being subject to uncontrolled power incentivizes strategies of dissimulation and feigned ignorance, then the dominant will tend to form, on this basis, naturalized stereotypes about the untrustworthiness and foolishness of the dominated, beliefs which will shape the expectations of the dominant in hard-to-revise ways (Roberts 2022). These epistemic faults will likely have negative consequences for both the interest-seeking activity of the dominant and for the justice of the society in question, but the basis of the critique in both of these cases is the identification of epistemically faulty partisanship or partiality.

1. The Philosophy of Creativity: Past and Present
Given the significance creativity has in our lives and the deep philosophical questions it raises, one might expect creativity to be a major topic in philosophy. Curiously, it isn’t.

To be sure, some of the most prominent figures in the history of Western philosophy have been fascinated with creativity—or what we now call “creativity”. According to some scholars, the abstract noun for creativity did not appear until the nineteenth century—but the phenomenon certainly existed and many philosophers took an interest in it (McMahon 2013; Nahm 1956; Murray 1989; Tatarkiewicz 1980: chapter 8).

To name just a few examples: Plato (4th century BCE) had Socrates say, in certain dialogues, that when poets produce truly great poetry, they do it not through knowledge or mastery, but rather by being divinely “inspired” by the Muses, in a state of possession that exhibits a kind of madness (Ion and Phaedrus). Aristotle (3rd century BCE), in contrast, characterized the work of the poet as a rational, goal-directed activity of making (poeisis), in which the poet employs various means (such as sympathetic characters and plots involving twists of fate) to achieve an end (of eliciting various emotions in the audience). Margaret Cavendish (1623–1673) and Émilie du Châtelet (1706–1749) championed the creative use of the imagination to pursue freedom, overcome prejudice, and cultivate natural abilities even despite social and political oppression. Immanuel Kant (1724–1804) conceived of artistic genius as an innate capacity to produce original works through the free play of the imagination, a process which does not consist in following rules, can neither be learned nor taught, and is mysterious even to geniuses themselves. Schopenhauer (1788–1860) stressed that the greatest artists are distinguished not only by the technical skill they employ in the production of art, but also by the capacity to “lose themselves” in the experience of what is beautiful and sublime (Schopenhauer 1859: Vol. I: 184–194 and Vol. II: 376–402). Friedrich Nietzsche (1844–1900) argued that the greatest feats of creativity, which he took to be exemplified by the tragic poetry of ancient Greece, was being born out of a rare cooperation between the “Dionysian” spirit of ecstatic intoxication, which imbues the work with vitality and passion, and the “Apollonian” spirit of sober restraint, which tempers chaos with order and form (Nietzsche 1872 [1967]). William James (1842–1910) theorized about creative genius exerts the causal power to change the course of history (Simonton 2018). This is just a glimpse of what each of these philosophers had to say about creativity, and many other figures could be added to their number.

Nevertheless, while some of the topics explored by earlier thinkers have come to occupy a central place in philosophy today—such as freedom, justice, consciousness, and knowledge—creativity is not among them. Indeed, “philosophy of creativity” is still a neologism in most quarters, just as, for example, “philosophy of action” and “philosophy of gender” were not too long ago. However, philosophical work on creativity has been picking up steam over the last two decades (as shown, for example, in a few important collections of essays: B. Gaut & Livingston 2003; Krausz, Dutton, & Bardsley 2009; Paul & Kaufman 2014; B. Gaut & Kieran 2018). We’ll now dive into those contributions, along with earlier work, beginning with what is perhaps the most basic question one can ask in this field.

2. What is Creativity?
As we noted at the outset, the term “creative” can be applied to three kinds of things: a person, a process, or a product (where a product could be an idea, performance, or physical artifact).

Most definitions focus on the product. According to one common approach, persons or processes are creative to the extent that they produce creative products, and a product is creative if it meets two conditions: in addition to being new it must also be valuable. Many theorists argue that novelty is not sufficient, because something can be new but worthless (e.g., a meaningless string of letters), in which case it doesn’t merit the compliment of being called “creative”. Immanuel Kant is often cited as anticipating this definition of creativity in his discussion of (artistic) genius. According to a common interpretation, Kant defines (artistic) genius as the ability to produce works that are not only “original”—since “there can be original nonsense”—but also “exemplary” (Kant 1790: §§43–50 [2000: 182–197]). (Hills & Bird [2018] challenge this reading of Kant.) This definition is so widely accepted among psychologists that it has come to be known as “the standard definition” of creativity in psychology. In practice, “creativity is often not defined” (J.C. Kaufman 2009: 19) in psychological experiments—more on this in §5 below. When psychologists do explicitly adopt a definition, however, they usually say that creative products are not only new, but also valuable in some way, though they variously express the product’s value in terms of its being “useful”, “effective”, “worthwhile”, “fit”, or “appropriate to the task at hand” (Bruner 1962: 18; A. J. Cropley 1967: 67; Jackson & Messick 1965: 313; Kneller 1965: 7; Cattell & Butcher 1968; Heinelt 1974; J.C. Kaufman 2009: 19–20; S.B. Kaufman & Gregoire 2016; Stein 1953; Sternberg & Lubart 1999: 3—for an overview, see Runco & Jaeger 2012). A few psychologists have suggested that the standard definition doesn’t fully capture the concept of creativity (Amabile 1996; Simonton 2012b). As for philosophers, at least one of them defends the standard definition with qualifications (Klausen 2010), but many of them challenge it, as we’ll soon see.

While it is uncontroversial that novelty is required for creativity, philosophers have refined that point. Certain examples may seem, at first, to suggest that novelty isn’t really necessary for creativity. Newton’s discovery of calculus was creative even if, unbeknownst to him at the time, Leibniz got there first—one of many examples of what are called “multiples” in the history of science (Simonton 2004). A beginning student’s idea that freedom is compatible with causal determinism might be creative even if, as she will soon learn, philosophers have been defending such “compatibilist” theories for millennia. However, examples like these do not force us to abandon the novelty requirement, but only to qualify it. Newton’s calculus and the student’s compatibilism were not new in all of history, but they were new to their respective creators, and that is enough for them to count as creative. In the terminology of philosopher Margaret Boden, these ideas are “psychologically creative” (P-creative) even though they are not “historically creative” (H-creative). Notice that P-creativity is more fundamental. Anything that is new in all of history (H-creative) must also be new to its creator (P-creative). Thus, creativity always exhibits psychological novelty, though it doesn’t always exhibit historical novelty.

Again, no one denies that a creative product must be new, at least to its creator. But as we’ll now see, some philosophers depart from the standard definition of creativity by rejecting the value condition (§2.1), or by proposing some further condition(s) (§2.2), or by doing both.

2.1 Challenges to the value condition
Some theorists have argued that although creative things are valuable, we shouldn’t build value into the definition of creativity, because doing so is not informative or explanatory:

Knowing that something is valuable or to be valued does not by itself reveal why or how that thing is. By analogy, being told that a carburetor is useful provides no explanatory insight into the nature of a carburetor: how it works and what it does. (Stokes 2008: 119; Stokes 2011: 675–76)

Those who maintain that value is required for creativity might reply that it doesn’t need to be informative or explanatory. Being a man is required for being a bachelor even though it’s not informative or explanatory to say that bachelors are men. Stokes notes that “creative” is a term of praise, and uses this point to argue that what is creative must be produced intentionally (since we don’t rightly praise what is unintentional or accidental)—an idea we’ll return to below. But the same point also seems to imply that what is creative must also have value (since we don’t rightly praise what doesn’t have value). And while the concept “carburetor” is value-neutral, as shown by the fact that a carburetor can be worthless or useless (if it’s broken), “creative”, one might argue, is a value-laden concept, like “progress”. Progress necessarily involves novelty or change, but we don’t praise change as progress unless it’s good change. Likewise, defenders of the value condition urge, creativity necessarily involves novelty, but we don’t praise novelty as creative unless it’s good novelty.

Other critics use counterexamples to argue that value isn’t necessary for creativity, the most prominent cases being ones of immoral creativity. (For a collection of essays by psychologists on the phenomenon of immoral or so-called “dark” creativity’, see D. Cropley et al. 2010). Putative cases of immoral creativity include creative accounting to cheat investors or creative testimony to mislead jurors, and the stock example in the literature is creative torture or murder. One can imagine novel and well-designed murders, as Thomas De Quincey once did in a satirical essay:

[S]omething more goes to the composition of a fine murder than two blockheads to kill and be killed—a knife—a purse—and a dark lane. Design, gentlemen, grouping, light and shade, poetry, sentiment, are now deemed indispensable to attempts of this nature. Mr. Williams has exalted the ideal of murder to all of us […] Like Æschylus or Milton in poetry, like Michael Angelo in painting, he has carried his art to a point of colossal sublimity. (De Quincey 1827; see also discussion in Battin et al. 1989)

Innovative ways of inflicting needless agony and craftily designed murders are not good (they have no value), and yet they can be creative. If this is right, then it seems to follow that creativity doesn’t require value.

One way of trying to save the value condition is by flatly denying that torture methods can be creative, and by denying more generally that creative things can be bad (Novitz 1999). But such denial seems ad hoc and implausible—“evil creativity” is not a contradiction in terms—and some have argued that this denial faces other problems besides (Livingston 2018).

Other theorists revise or qualify the value condition in order to accommodate examples of immoral creativity. Paisley Livingston (2018) proposes that a creative product only needs to be instrumentally valuable or “effective” as means to its intended end, regardless of whether that end is morally good, bad, or indifferent. Berys Gaut (2018) distinguishes between something’s being good (or good, period) versus being good of its kind. In his view, a new way of wielding blades and pulleys may be creative if it’s a good of its kind—good as a method of torture—even though it isn’t good. In order for something to count as creative, Gaut says, it doesn’t need to be good; it just needs to be good of its kind.

Alison Hills and Alexander Bird (2018) are unconvinced by such qualifications. They contemplate an elaborate torture device that ends up killing its victims immediately, “without enough suffering on the way”. The device may still be creative, they hold, even though “as a method of torture, it’s no good” (2018: 98). Indeed, they argue, a creative item needn’t be good in any way at all, not even for its creator. The ineffective torture device just described doesn’t satisfy its creator’s preferences, it doesn’t give him pleasure, it isn’t an achievement, it doesn’t contribute at all to his well-being—and yet, they contend, it may be creative, provided that it’s new and was produced in the right way. Exactly what “the right way” amounts to is the topic we turn to next.

2.2 Other proposed conditions
With or without the value condition, some theorists argue that a product must satisfy one or more further conditions, beyond being new, in order to count as creative. The four most prominent proposals are that the product must be (i) surprising, (ii) original (i.e., not copied), (iii) spontaneous, and/or (iv) agential. Each of these is a condition on the process of creativity. To be clear, we are still concerned with what it means for a product to be creative, but the proposals we’ll now consider say that in order for a product to count as creative, it must be brought about in the right way.

2.2.1 Surprise
Margaret Boden holds that a creative product must be “new, surprising, and valuable” (2004: 1; cf. Boden 2010; 2014). It is perhaps most natural to assume that being surprising—like being new and valuable—is a feature of a product. But while Boden does think of creative products as surprising, her interest is more fundamentally in the underlying generative process, in how a creator manages to make something surprising. In her view, there are “three types of creativity”—combinatorial, exploratory, and transformative—“which elicit different forms of surprise, [and] are defined by the different kinds of psychological processes that generate the new structures” (2010: 1, italics added).

Combinatorial creativity occurs when old ideas are combined in new ways. Obvious examples include fictional hybrid creatures or chimeras: add wings to a horse (Pegasus), add the tail of a fish to a woman’s head and upper-body (a mermaid), add a lion’s body to a woman’s head and torso (Sphinx), and so on. Other combinations are found in analogies, such as when Niels Bohr compared an atom to the solar system. The term “combination” can refer either to the product of things combined or to the process of combining them, but Boden’s focus is on the process here, on the fact that one way to generate new ideas is to begin with old ideas and combine them in new ways.

To explain her other two kinds of creativity, Boden invokes the notion of a “conceptual space”, which is roughly a system comprising a set of basic elements (e.g., basic ideas or representations) as well as rules or “constraints” for manipulating or re-combining those elements. A conceptual space is not a painting, song, or poem, for example; it’s a way of creating a painting, song, poem, or theory. The rules or constraints are “the organizing principles that unify and give structure to a given domain of thinking”. And so a conceptual space is

the generative system that underlies that domain and defines a certain range of possibilities: chess moves, or molecular structures, or jazz melodies. (1994: 79)

We could think of a conceptual space as not just a set of thoughts but also a style of thinking defined by rules for generating new thoughts.

“Within a given conceptual space”, Boden observes, “many thoughts are possible, only some of which may have been actually thought” (2004: 4). Some conceptual spaces contain more possibilities than others. Consider different games. Tic-tac-toe is such a simple game that all of its possible moves have already been made many times over. The same is not true in chess, by contrast, which allows for a mind-boggling number of possible moves. The range of possible ideas is also practically inexhaustible in literature, music, the visual and performing arts, as well as the various domains of theoretical inquiry. And within those pursuits, there are various “structured styles of thought”—genres, paradigms, methodological orientations—which Boden thinks of as conceptual spaces.

Boden argues that the elements as well as the operating rules of a conceptual space can be, and in some cases have been, captured in computer programs. She has used this point not only to argue that computers can be creative (a topic we’ll return to below in §5), but also to suggest that we should employ the computational model of the mind in order to explain how humans create.

With her notion of conceptual spaces in hand, Boden says that exploratory creativity occurs within a given conceptual space. The new idea that emerges is one that was already possible within that space, because it was permitted by its rules. “When Dickens described Scrooge as ‘a squeezing, wrenching, grasping, scraping, clutching, covetous old sinner,’” Boden writes, “he was exploring the space of English grammar” in which “the rules of grammar allow us to use any number of adjectives before a noun” (Boden 1994: 79). Dickens’s description may strike us somewhat surprising, unexpected, or improbable, but it doesn’t have an air of impossibility about it.

By contrast, Boden argues, another form of creativity does. In this kind of case, the creative result is so surprising that it prompts observers to marvel, “But how could that possibly happen?” (2004: 6). Boden calls this transformational creativity because it cannot happen within a pre-existing conceptual space; the creator has to transform the conceptual space itself, by altering its constitutive rules or constraints. Schoenberg crafted atonal music, Boden says, “by dropping the home-key constraint”, the rule that a piece of music must begin and end in the same key. Lobachevsky and other mathematicians developed non-Euclidean geometry by dropping Euclid’s fifth axiom. Kekulé discovered the ring-structure of the benzene molecule by negating the constraint that a molecule must follow an open curve (Boden 1994: 81–3). In such cases, Boden is fond of saying that the result was “downright impossible” within the previous conceptual space (Boden 2014: 228).

Boden’s definition of creativity has perhaps been most influential among researchers who share her intertest in computer creativity (e.g., Halina 2021; Miller 2019: ch. 3; du Sautoy 2019). In a variation of Boden’s account, one philosopher proposes that what makes a mental process creative is not that it actually involves “the recombination of old ideas or the transformation of one’s conceptual space”, but rather that the creator experiences the process as having one of those features (Nanay 2014).

2.2.2 Originality
Maria Kronfeldner (2009; 2018) argues that the process of making something creative must exhibit originality. As she uses the term “original”, it does not simply mean “new”; instead, it has to do with the kind of causal process the creator must employ. She motivates her view by asking why it’s the case that, as we noted earlier, psychological novelty is required for creativity while historical novelty is not. Why is it, for example, that Newton’s invention of calculus was creative even if Leibniz invented it first? The answer, of course, is that it’s because Newton didn’t copy his calculus from Leibniz. Insofar as Newton came up with calculus independently, on his own, then he exhibited originality in his discovery, even though someone else got there first. This originality, Kronfeldner argues, is essential to creativity.

2.2.3 Spontaneity
Kronfeldner (2009; 2018) also argues that spontaneity is required for creativity. An idea occurs spontaneously to the extent that it is produced without foresight or intentional control. If you were to foresee the output of the creative process at the beginning of that process, then you wouldn’t need any further process to come up with it. So if an idea is creative, you cannot have fully seen it coming. To that extent, insight comes as a surprise, hence the common phenomenological observation that creative breakthroughs feel like they come unbidden or out of the blue: “Eureka!”, “Aha!”, a lightbulb turns on.

Gaut (2018: 133–137) agrees that creativity requires spontaneity, and he points out, as Kronfeldner does, that it comes in degrees. He explains that you do something spontaneously to the extent that do it without planning it in advance. If you are going to act creatively, he argues, you cannot set out to follow an “exact plan”—a mechanical procedure, routine, or algorithmic rule—which would give you advance knowledge of exactly what the outcome will be and exactly the means you'll take to achieve it. At the outset of a creative act, you have to be to some extent ignorant of the end, or the means, or both. That ignorance opens up room for spontaneity and creativity.

2.2.4 Agency
Some philosophers argue that an item does not count as creative unless it has been produced by an agent. Consider a unique snowflake with an intricate shape, a distinctive sunset with stunning layers of red-orange hues, a novel patterning of dunes across a wind-blown desert. All of these things are aesthetically valuable and new. None of them are creative, however, insofar as they all occurred naturally and were not made by an agent. Gaut uses examples like these to argue that creative things must be created by agents (B. Gaut 2018: 129–30; cf. B. Gaut 2010, and B. Gaut 2014b) and several other philosophers agree (Carruthers 2006, 2011; Kieran 2014a, 2014b; Stokes 2008, 2011, 2014; Paul & Stokes 2018).

Of course, many theists would maintain that everything in nature is the handiwork of an agent—namely, God—and so arguably it would make sense for them to regard a natural phenomenon as creative if it is valuable and new. For theists, the unparalleled beauty of nature is a reason to praise the Creator. But this only supports the conceptual point that creativity, by definition, requires agency. We may coherently regard valuable new things as creative if we attribute them to a creative agent, as the theist does with the natural world; otherwise, we can’t. So again, it seems, creativity requires agency.

This leaves open the question of exactly how a creator’s agency must be exercised in order for the result to count as creative. Some philosophers argue that the agent’s act of creation must be intentional. Suppose you are snowboarding on a powder day and, unbeknownst to you, the tracks from your board result in a pleasing new pattern as viewed from high above. The new pattern has aesthetic value, but it isn’t creative. And that is because you didn’t intend to make it. Underlying this intuition, as well as our intuitions about the natural phenomena above, is the fact that “creative” is a term of praise, and we do not extend praise (or blame) for things that are not done by an agent, or for things that an agent doesn’t do in some sense intentionally.

While a number of philosophers endorse some version of the agency requirement for creativity, many theorists make no mention of it, whether to endorse it or reject it, including all of the psychologists cited above. Further, at least two philosophers are willing to attribute creativity to natural phenomena like trees and evolutionary processes: Arnheim (2001) and, in recent work, Boden (2018). These latter theorists don’t discuss agency as such, but insofar as the natural phenomena they call creative are not the result of agency, their view would imply that agency isn’t required for creativity.

The four proposals we’ve just considered all say that a product must arise from a certain kind of process—a process that exhibits surprise, originality, spontaneity, or agency—in order to count as creative. While there is wide agreement among philosophers that creativity requires some special kind of process, not just a special product, there is no consensus on what is required of the process. Of the four process conditions described here, the agency condition seems to be the one that is explicitly endorsed by the greatest number of philosophers thus far, though even they are still just a handful. And as we’ve seen, the other proposed conditions have serious arguments in their favor as well.

Some philosophers argue that if any process requirement is correct, this has an intriguing corollary for judgements about creativity: Even when we are explicitly judging only that a product is creative, we are implicitly assuming something about the process by which it was made. Suppose, for illustration, that the agency requirement is correct—that being generated through an agential process is built into the very concept of a creative product. Suppose further that you are applying that concept competently. It follows that if you come across a captivating arrangement of stones on the beach and you judge it to be creative, you are at least implicitly assuming that it was created through an agential process. If someone later persuades you that the stones happened to be moved into place by the wind and waves, not by any agent but just by chance, then you may still regard the result as aesthetically interesting but you would have to rescind your judgement that it is creative. So if the agency condition is correct, whenever you point to some item and say, “This is creative”, what you are saying, in part is, “This resulted from a creative process”. Furthermore, on this view, analogous implications follow if any other process condition is correct (Paul & Stokes 2018).

2.3 Is creativity a virtue?
Having considered what is required for something to count as a creative product, and whether it must be produced by a certain kind of process, we now turn to analysis of the creative person.

Some theorists suggest that creativity, as an attribute of persons, is an ability to perform creative acts or produce creative things (Boden 2004). Others argue, however, that creativity isn’t merely an ability. An ability is something you can possess without ever putting it to use. You might have the ability to learn Swahili, for example, without ever making the effort to learn that language, despite having ample opportunities to do so. Creativity is different in this regard. If someone has the ability to be creative but never uses that ability when given numerous chances to do so, we would not call that person creative. Creative people are not merely able to act creatively. They are, moreover, disposed to exercise that ability, such that they do act creatively, at least some of the time, when the occasion arises. On this view creativity is a disposition, also referred to as a trait (Grant 2012; cf. B. Gaut 2014b, 2018).

Philosophers have long distinguished virtues as a special subclass of dispositions or traits. In Western philosophy, the tradition of theorizing about virtues goes back to the ancient Greeks, and over the last half-century it has enjoyed a renaissance in ethics (see entry on virtue ethics) and, more recently, in epistemology (see entry on virtue epistemology) and aesthetics (Lopes 2008; Roberts 2018; Hills 2018). Traditional examples of virtues include wisdom, justice, temperance, and courage. Should creativity be added to the list?

The answer depends, of course, on what it means for a trait to be a virtue. At the very least, a virtue is a trait that is good or valuable. So whether creativity counts as a virtue in this minimal sense depends on whether creativity is necessarily valuable, a point which is contested, as we saw in the previous section. In fact, those who contend that creativity isn’t necessarily valuable often do so in order to prove that it isn’t a virtue.

But let’s suppose for the sake of argument that creativity is indeed a valuable trait. Is it also a virtue in some more robust sense? Virtue theorists commonly take their cue from Aristotle’s classic discussion in the Nichomachean Ethics. Citing justice and temperance as paradigm virtues, Aristotle asserts that a trait must meet at least three conditions to count as a virtue:

For actions in accord with the virtues to be done temperately or justly it does not suffice that they themselves have the right qualities. Rather, the agent must also be in the right state when he does them. First, he must know [that he is doing virtuous actions]; second he must decide on them, and decide on them for themselves; and thrid, he must also do them from a firm and unchanging state. (EN II.4, 1105a28–1105a33)

So, for example, if you return something you’ve borrowed, that act exhibits the virtue of justice if and only if (1) you know that you’re returning what you borrowed, (2) you choose to do so because it is the just thing to do, and for no other reason, and (3) you are disposed to do the just thing across the range of circumstances when the opportunity arises. In addition to justice and temperance, Aristotle enumerates other ethical virtues like prudence, generosity, and courage, as well as the intellectual virtue of theoretical wisdom. In his view, each of these traits requires one to meet the three conditions above. While he does not consider whether creativity is a virtue, we may ask whether creativity also has these three criteria. Does one have to meet these three requirements in order to count as creative?

We’ll begin with the third requirement to set it to one side. Does a person’s act count as creative only “if he does it from a fixed and permanent disposition of character”? Examples suggest otherwise. Consider the poet Arthur Rimbaud, who abandoned poetry at the age of 21 to pursue a life of adventure. The fact that he never produced another poem after that does not count against the fact that he was a creative poet in his youth (B. Gaut 2014b). Unlike the Aristotelian virtues, then, creativity does not have to be a permanent disposition.

Even so, it would still be significant if creativity turned out to be like an Aristotelian virtue in meeting the first two requirements. And arguably, creativity does meet the first requirement. A person doesn’t count as doing something creative unless “he knows what he is doing”. This was already implied by the agency condition for creativity discussed earlier.

Where things get interesting is with Aristotle’s second criterion for virtue. In order for your action to count as virtuous, he says, you have to do it “for its own sake”—i.e., you have to do it because you value virtue as an end itself, and not as a means to some external reward like praise, money, status, fame, or winning a competition. Consider the virtue of generosity, for instance. If you give money to someone in need merely because it will make you look good in the eyes of your friends, then you aren’t really being generous. Your act may outwardly look like generosity, but it’s not the real thing. To exhibit real generosity, you have to pursue generosity as an end in itself; you have to help others just for the sake of helping others. Now contrast being generous with being polite. If you compliment your colleague on the good work she’s done, then even if you’re doing this in order to manipulate her, you are being polite to her. You can have an ulterior motive for being polite. So politeness is not a virtue the way generosity is.

Is creativity a virtue in this respect? That is, does being creative require acting creatively for its own sake? Matthew Kieran’s (2014a, 2014b, 2018) answer is a qualified yes. While he grants that you can be motivated by external rewards to exhibit “minimal creativity” in producing valuable new things, he maintains that “exemplary creativity” requires you to be motivated by the value of creativity itself. Thus, in his view, exemplary creativity is a virtue.

To support this claim, Kieran points to a research program in psychology which purports to show that creativity is driven by “intrinsic motivation” rather than “extrinsic motivation”. A classic experiment in this program is “the magic markers study”, in which kids end up producing less creative drawings when they are offered a prize (Lepper et al. 1973). Many other studies have reported similar results, which lead Teresa Amabile to conclude, at first without qualification, that creativity is enhances by intrinsic motivation and hampered by extrinsic motivation (Amabile 1983: 107).

Further research introduced complications. In some studies, subjects were given “immunization techniques” whereby they were first primed or trained to focus on intrinsically motivating factors like the pleasure or aesthetical value of engaging in artistic activities, and it was found that when they engaged in those activities afterward, external rewards actually enhanced their creativity.

As researchers interpreted these findings, offering reward can support one’s intrinsic motivation, provided that the reward works either to boost one’s sense of agency or to provide useful feedback about what’s working and what isn’t. Intrinsic motivation is still what fuels creativity, on this interpretation; rewards help only indirectly, when they reinforce intrinsic motivation. This lead Amabile to revise her hypothesis as the Intrinsic Motivation Principle (IMP):

Intrinsic motivation is conducive to creativity; controlling extrinsic motivation is detrimental to creativity, but informational or enabling extrinsic motivation can be conducive, particularly if initial levels of intrinsic motivation are high. (1996: 107)

Kieran takes this as evidence for his claim that creativity, or at least what he calls exemplary creativity, requires intrinsic motivation and is therefore a virtue in that respect.

Objecting to this proposal, Gaut cites evidence that extrinsic motivation is not always detrimental to creativity. In one study, students in an introductory psychology class came up with more creative short story titles if they were offered a financial reward (Eisenberger & Rhodes 2001). In the studies where immunization techniques were used, proponents of IMP argue that rewards enhance creativity only indirectly, by buttressing intrinsic motivation. But in this case no such techniques were used, and so it seems the prospect of a reward enhanced creativity directly.

Further, Gaut argues that this point coheres with the role that rewards seem to play in so many real-world cases of creative achievement. In their quest to discover the structure of the DNA molecule, Watson and Crick were driven “to imitate Linus Pauling and beat him at his own game” (Watson 1968 [1999: 46]). Picasso and Matisse were both spurred on by their rivalry with each other (Flam 2003: 37). Paul McCready says he was driven to invent his award-winning human-powered glider in 1977 because he needed the prize-money to pay off his debts:

I felt that I didn’t have the time to mess with such things, but I had this strong economic motivation to take an interest in man-powered flight, so I charged around trying to figure out a way to solve it. (quoted in Sternberg & Lubart 1995: 242)

One historian argues that in World War II the Poles beat the French in cracking the Germans’ Enigma Code because they were more terrified of German invasion (Singh 1999: ch. 4). Gaut quips: “Fear of death is a more powerful motivator than the intrinsic satisfactions of code breaking” (Gaut 2014b: 196).

Finally, Gaut points out that even if IMP is true, it is only a causal, probabilistic claim: intrinsic motivation is “conducive” to creativity; extrinsic motivation is “detrimental”. But for a trait to be a virtue, intrinsic motivation must be conceptually necessary for the exercise of that trait. If we learn that someone gave to charity just to enhance his reputation, we conclude that he wasn’t really being generous. By contrast, if we discover that someone created gorgeous artwork just for the fame and glory, we may then lose some of our admiration for her creativity, but we do not deny that she was being creative.

Kieran could remind us that, in his view, intrinsic motivation is not required for all creativity, but only for the special form of it that he calls exemplary creativity. Anticipating this reply, Gaut says that to distinguish between two forms of creativity is just to concede his point. There are not two forms of generosity, one that requires intrinsic motivation and another that does not. If your act of giving isn’t motivated by the right kind of reason, then it doesn’t count as an act of generosity at all. Thus, Gaut argues, to grant the possibility of non-exemplary creativity is to grant that, unlike generosity, creativity isn’t a virtue in the traditional Aristotelian sense.

Another way to examine relations between creativity and virtue is through the lens of virtue epistemology. Linda Zagzebksi defines a virtue

as a deep and enduring acquired excellence of a person, involving a characteristic motivation to produce a certain desired end and reliable success in bringing about that end. (1997: 137, italics added)

While there is a lot packed into this definition, what we’ll pinpoint here is the idea that virtue involves reliable success in achieving a desired end, and that the agent who is epistemically virtuous, in particular, is one who is reliably successful in achieving knowledge. Knowledge requires truth, of course, so an epistemic virtue is a trait that is “truth-conducive”. Epistemologists typically regard a process as truth-conducive to the extent that the beliefs it produces are more often true than false. But Zagzebksi proposes that a process or trait may be truth-conducive in a different sense, insofar as it is necessary for advancing knowledge in some area, even if it produces a very small proportion of true beliefs. Creativity, she claims, is truth-conducive in this sense, and thus it qualifies as an epistemic virtue (1997: 182). Also note the emphasis on agency. In contrast to contemporary western epistemology, virtue epistemology identifies the agent (rather than, say her beliefs) as the essential locus of epistemic valence; it is the agent who is epistemically good (or not). This emphasis comports well with the proposal, discussed above, that the creator’s agency is necessary for genuine creative achievement. A virtue-theoretic approach thus illuminates what may (as we will discuss again later) be essential to creativity, namely, a process that non-trivially involves a responsible agent.

We’ve seen that even after we fix a specific referent for the term “creative”—whether it be a person, process, or product—there are lively disagreements about what it means. These debates often seem to presuppose that the term always expresses the same concept, for which we can seek necessary and sufficient conditions. But we’ve also seen that some theorists distinguish between different concepts of creativity, corresponding to different senses of the term “creative”. In future work we may see theorists develop such pluralistic approaches in more detail. The trick, though, will be to give principled reasons for multiplying different concepts of creativity so that the analyses do not simply reduce to saying that anything goes.

3. Can Creativity be Learned?
There is a long tradition of thinkers who answer no to the question above. Two of the most influential are from the eighteenth century—Edward Young and Immanuel Kant—who were concerned specifically with genius, the capacity for achieving the very highest levels of creativity. In Conjectures on Original Composition (1759), Young says,

An Original may be said to be of a vegetable nature; it rises spontaneously from the vital root of genius; it grows, it is not made …. (1759 [1966: 7])

His idea is that originality emerges naturally from something implanted in us by nature, and it can only be hindered by learning. Young seems to think of learning as proceeding either through imitation or through the following of rules, and both, he thinks, are detrimental to originality. Regarding imitation he writes,

Born Originals, how comes it to pass that we die Copies? That meddling ape Imitation… destroys all mental individuality…. (1759 [1966: 20])

And insofar as learning is “a great lover of rules”, he warns that it “sets rigid bounds to that liberty, to which genius often owes its supreme glory” (1759 [1966: 13]).

Kant makes similar claims in his Critique of Judgment (1790). Like Young, he takes genius to be a natural capacity, though a very rare one:

such a skill cannot be communicated, but is apportioned to each immediately from the hand of nature and dies with him. (1790: §47 5:309 [2000: 188])

It certainly cannot be learned through imitation:

genius is entirely opposed to the spirit of imitation. Now since learning is nothing but imitation, even the greatest aptitude for learning, facility for learning (capacity) as such, still does not count as genius. (1790: §47 5:308 [2000: 187])

Nor can it be learned through rules, Kant holds, for genius is

the talent (natural gift) that gives the rule to art … the inborn predisposition of the mind (ingenium) through which nature gives the rule to art. (1790: §46 5:307 [2000: 186])

For Kant, a genius does not follow rules; a genius invents the rules, indirectly, by creating exemplary works from which other artists might extract rules and undertake “a methodical instruction in accordance with rules” (1790: §49 5:318 [2000: 196]).

Young and Kant are concerned with genius, specifically, but if we extend their reasoning to creativity in general, as Berys Gaut (2014a) has noted, we can discern two lines of argument:

The imitation argument

All learning is a form of imitation.
Imitating someone or something is incompatible with being creative.
So, one cannot learn to be creative.
The rules argument

All learning consists in the following of rules.
Following rules is incompatible with being creative.
So, one cannot learn to be creative. (2014a: 266)
Gaut points out, first of all, that both arguments are invalid. In both cases, what the premises would entail is that learning cannot be creative, that, in other words, you cannot learn creatively (a claim about how you can learn). But even if that were true, it wouldn’t follow that you cannot learn to be creative (a claim about what you can learn). If you absorb the advice of a creative writing manual then this act of learning may not itself be creative. But if the manual is effective—and we’ll see in a moment how it can be—then what you will learn is how to become more creative.

Gaut also challenges the premises of these arguments. To start with the first premise of the imitation argument, it simply isn’t true that all learning proceeds through imitation, as we learn many things through direct experience, trial and error, and many other means.

The second premise is also suspect. Something superficially close to it is true: mere copying is incompatible with being creative. But to the extent that we learn from others by imitating them, this is not merely a matter of copying them. When a child learns to speak the language of those around her, she doesn’t simply parrot the exact same sentences she hears; she absorbs the vocabulary and underlying grammar in a way that enables her to form new sentences of her own devising.

Now for the rules argument. Contrary to the first premise, it cannot be the case that all learning consists in following rules, Gaut argues, because for any given rule there will be hard cases where it is unclear whether or how the rule applies to them, and so an individual still has to use her own judgment in applying the rule.

The second premise is false too. Recall the distinction from §3 above between two kinds of rules. An algorithm serves as an exact plan, specifying both the outcome and the path for getting to it in exact detail. In contrast, a heuristic is a looser “rule of thumb” that leaves room for an agent to exercise her own judgment, choice, and creativity in determining whether, when, and how to follow the rule. While algorithms, in this sense, may preclude creativity, heuristics do not, which is why, as we’ll see below, the teaching of creativity so often takes the form of heuristics.

There is a sense in which the question at hand can be answered empirically: We can show that creativity can be taught simply by pointing to cases where it has been taught. Gaut himself discusses such examples as they occur in mathematics and fiction writing, which we’ll turn to below. But while such cases may suffice to show that creativity can be taught, Gaut further enriches our understanding by explaining how this is possible. He does so partly by articulating and then debunking the imitation and rules arguments to the contrary. But in addition, he offers the following positive argument to show that creativity can be taught and learned. He calls it “the constitutive argument” because it begins with his view of what constitutes or defines creativity itself.

The constitutive argument

Creativity is a disposition—involving both the ability and the motivation—to produce things that are new and valuable, and to do so in ways that express one’s agency through “the exercise of choice, evaluation, understanding, and judgment” (Gaut 2014a: 273).
At least some people can learn to enhance their creative motivation.
At least some people can learn to enhance their creative abilities.
So, at least some people can learn to become more creative.
Premise 1 recapitulates the point we’ve already seen Gaut and others defend (in §2.3 above), that creativity is not merely an ability but a disposition or trait, whereby the creative person is disposed or motivated to exercise that ability when given the opportunity.

In support of premise 2, Gaut argues that you can strengthen both your intrinsic motivation to be creative (when you take pleasure in your creative activities), as well as your extrinsic motivation to be creative (when you are rewarded with praise, grades, pay, etc. for your creative efforts).

Defending premise 3, Gaut points out that you can develop your ability to produce valuable new things by practising and strengthening the relevant skills. And this development can be substantially aided by learning certain heuristics.

Heuristics are indeed a staple of education in creative pursuits from mathematics (draw the figure; consider special cases; consider extreme cases; generalize the problem; look for a related problem, etc.—see Pólya 1945; Schoenfeld 1982, 1987a, 1987b) to creative writing (write what you know; be specific and detailed in describing sensory experiences; practice seeing similarities between dissimilar things; show, don’t tell, etc.—see Bell & Magrs 2001; Anderson 2006; Maybury 1967; S. Kaufman & J. Kaufman 2009). Gaut also identifies several heuristics that might be used to foster creativity in philosophy, even among children (cf. M. Gaut 2010; B. Gaut & M. Gaut 2011).

With this last theme, Gaut has a kindred spirit in Alan Hájek (2014, 2016, 2017, 2018), who has independently proposed that by using various heuristics, philosophers can enhance their abilities to make valuable contributions to their field, including ideas that are distinctively creative. It has been said that anyone of average talent can become a strong chess player by learning and internalizing certain chess heuristics: “castle early”, “avoid isolated pawns”, etc. Analogously, Hájek suggests, philosophy has a wealth of heuristics—philosophical heuristics—although they have not been as well documented and studied. Sometimes these take the form of useful heuristics for generating counterexamples, such as “check extreme cases”. Sometimes they suggest ways of generating new arguments out of old ones, as in “arguments involving possibility can often be recast as arguments involving time, or space”. Sometimes they provide templates for positive arguments (e.g., ways of showing that something is possible). Hájek offers a catalogue of such philosophical heuristics to show that, contrary to a common assumption, creativity, even in philosophy, can be compatible with, and enhanced by, following rules.

4. Can Creativity be Explained?
Upon observing the work of creative people, it is natural to wonder: How do they do that? How do people create? The issue we turn to now is whether we could, at least in principle, answer this question scientifically, using the methods of modern empirical psychology and other cognitive and behavioral sciences. Those who take a negative stance on this matter are not merely saying that, in practice, it would be exceedingly difficult for science to explain creativity. They are saying that it’s altogether impossible that science could ever explain creativity.

Hospers (1985) defends this kind of pessimism based on the variety and complexity of creativity, given that creativity occurs not only in art, but in science, theorizing of any sort, engineering, business, medicine, sport, gaming, and so on. At least two worries may follow. First, given the complexity of any one of these individual domains, one might worry that there are simply too many variables to allow for a clear explanation. Art provides a paradigmatic example. Consider an artwork that you judge to be masterful (a sculpture, a painting, a film). Now imagine attempting to describe or identify all the reasons for which you think it is masterful. Take as much time as you like but, the skeptic will urge, any long description you construct will invariably strike you as woefully incomplete by comparison to the artwork, and the experience thereof. So, if the creative achievements of artists, in all of their complexity, cannot even be adequately described, we have little reason to think that such achievements can be explained.

How can theorists respond to these skeptical worries? Both the complexity and generalizability worries might be partially disarmed by noting analogies between creativity and other phenomena. For instance, consider the range of bodily movement involved in some of the very domains of activities listed above: art, science, engineering, medicine, sport. The kinds of bodily action specific to these domains are complex and vary dramatically: the relevant physical movements of the surgeon are much different from the tennis player. However, it is not plausible that this complexity and variety precludes explanation of bodily action in those domains. It simply implies that some features of the explanation will be context-sensitive, that is, specific to that domain of activity. And further to the analogy: the fact that the long description of, say, the tennis serve is incomplete does not preclude it from being apt and explanatory. If this line of reasoning is sound for bodily action, why not also for creative action?

At this point, one might argue that while complexity and generalizability worries would only show that creativity is difficult to explain in practice, the very nature of creativity implies, more strongly, that it could never be explained, not even in principle. Resources to support this kind of pessimism may be adduced from various past philosophers. We need to tread carefully, however, since most of the figures we are about to consider were writing long before the rise of the relevant sciences, so they could not have made any explicit claim either way as to whether creativity could be explained by those sciences. Nevertheless, some of them did make claims which entail, or seem to entail, that creativity simply isn’t the kind of thing that could be explained through scientific inquiry as we understand it today.

The classic expression of such a view comes from Plato. In his dialogues, Plato features his teacher Socrates as a spokesperson for his own views, and in the Ion he has Socrates argue that poets do not produce poetry through knowledge or skill. When you exercise a skill (technē), you apply techniques, rules, or methods to perform a given activity, like charioteering, fishing, or commanding an army. In principle, one could explain these activities by identifying the techniques they involve, and a student or apprentice could learn these activities by applying and practicing those techniques. But poetry is not like that, in Socrates’ view. A poet can only imitate the application of rules or techniques, mimicking the surface appearance of skill. Voicing an idea that was familiar in Ancient Greek culture, Socrates suggests that poetry emerges instead through divine inspiration, whereby a human being is inspired—literally “filled with a spirit”, with a god or goddess, with a muse:

You know, none of the epic [or lyric] poets, if they’re good, are masters of their subject; they are inspired, possessed, and that is how they utter all those beautiful poems. … [They] are not in their right minds when they make those beautiful lyrics, but as soon as they sail into harmony and rhythm they are possessed by Bacchic frenzy. […] For a poet is an airy thing, winged and holy, and he is not able to make poetry until he becomes inspired and goes out of his mind and his intellect is no longer in him. As long as a human being has his intellect in his possession he will always lack the power to make poetry or sing prophecy. […] You see, it’s not mastery [technē] that enables them to speak those verses, but a divine power. That’s why the god takes their intellect away from them when he uses them as his servants, as he does prophets and godly diviners, so that we who hear should know that they are not the ones who speak those verses that are of such high value, for their intellect is not in them: the god himself is the one who speaks, and he gives voice through them to us. In this more than anything, then, I think, the god is showing us, so that we should be in no doubt about it, that these beautiful poems are not human, not even from human beings, but are divine and from gods; that poets are nothing but representatives of the gods, possessed by whoever possesses them. (Ion 534a-d)

Socrates repeats this view in the Phaedrus: “Some of the greatest blessings come by way of madness, indeed madness that is heaven-sent” (244a). He adds that while a poet may have some kind of skill, anyone who aspires to make poetry purely by skill, without the madness or the muse, will fail (245a).

It’s important to note that “madness”, for Plato, is a supernatural affair. From the vantage of contemporary behavioral science, we think of madness—or rather, mental illness—as a pathology arising from some combination of genetic and environmental factors, and those factors can be studied scientifically. So even if creativity is linked to mental illness—a highly controversial proposition—it could still be entirely within the scope of science. However, Plato’s talk of “madness” does not refer to any naturally occurring pathology, but rather to the result of divine intervention: the poet is taken over or “possessed” by the muse and that is precisely why he is “out of his mind”. Plato’s poet suffers divine madness.

According to this story, then, the person we call a poet isn’t really a creator of poetry, but is merely the vessel through which a divine being delivers poetry. If it is literally true that the source of poetry is supernatural, then poetic creativity could never be explained by science, which is limited to the investigation of natural causes. (For more on Plato, see Asmis 1992.)

This kind of supernaturalism has enjoyed a long afterlife in Western thought. In ancient Rome, the Latin term “genius” referred to a guiding spirit that was thought to accompany each person throughout their lives. The genius of an artist would occasionally deliver art through that person in the manner of Platonic inspiration.

Conceptions of the artist take a new turn when the idea of genius is transformed in the eighteenth century. As we saw above, Immanuel Kant defines genius as a natural capacity that a certain kind of artist possesses innately and which partly constitutes that artist’s identity. So rather than saying that a gifted artist “has a genius”, Kant says that such a person “is a genius”. What distinguishes the genius is fundamentally an imaginative capacity—an ability to engage in a “free play” of imagination to produce artworks of “exemplary originality”. These works are exemplary not only in the sense that they have artistic or aesthetic value, unlike “original nonsense”; they are also exemplary in the more radical sense of providing an exemplar—a new paradigm and precedent—for lesser artists to follow. A work of genius sets a new standard of artistic value, and, looking to that exemplar, lesser artists may then extract techniques or rules for their own craft. The genius therefore “gives the rule to art”. In creating such works, the genius does not follow any rules or methods. Instead the genius creates art through a “free play of imagination”—where the terms “free” and “play” characterize the nature of an activity unconstrained by any pre-established methods or rules:

[G]enius … is a talent for producing that for which no determinate rule can be given, not a predisposition of skill for that which can be learned in accordance with some rule …. (1790: §46 5:307–8; 2000 trans., 186)

Kant thought that genius, so conceived, is limited to the fine arts, poetry being chief among them. Meanwhile, in Kant’s view, there is no room for genius in science, for example, where good theories and hypotheses must emerge from the careful application of scientific method, and so he said that even Isaac Newton, “that great man of science”, was not a genius. We’ll soon consider why this view might seem to entail that creativity is inexplicable, but first it will be helpful to bring another figure, Arthur Schopenhauer, who was deeply influenced both by Kant and by Plato.

Like Kant, Schopenhauer thought of genius as a natural capacity that is limited to the fine arts. He also echoes Plato’s sentiments about madness, famously stating that “genius and madness have a side where they touch and even pass over into each other” (The World as Will and Representation, 1859, WWV I: 190), and that “Genius lives only one storey above madness” (Parerga and Paralipomena, SW 2:53, PP 2:49). In a state of madness, Schopenhauer’s genius is like Plato’s poet in experiencing a momentary loss of self, but what displaces the self is not any divine being but rather a pure Idea which seizes the author’s being and becomes the object of both his fascination and his artistic expression:

We lose ourselves entirely in this object, to use a pregnant expression; in other words, we forget our individuality, our will, and continue to exist only as pure subject, as clear mirror of the object, so that it is as though the object alone existed without anyone to perceive it, and thus we are no longer able to separate the perceiver from the perception, but the two have become one, since the entire consciousness is filled and occupied by a single image of perception. (World WWV I: 178–179, §34).

With their focus on genius construed as a natural capacity, figures like Kant and Schopenhauer abandon the supernaturalism of the Platonic muse. Nevertheless, they retain the idea that creativity—specifically genius-level creativity in the fine arts—is not a matter of exercising a skill or applying given rules, methods, or techniques.

As we noted earlier, these figures did not and could not have explicitly denied that creativity could be explained by the sciences of the twentieth and twenty-first centuries, but they are commonly taken to represent such a denial (Kronfeldner 2018). Why?

Perhaps figures like Kant and Schopenhauer seem to make creativity, or at least creative genius, inexplicable insofar they suppose it to be innate and as they have no story to tell about how one came to acquire an innate capacity except to say that it was either an accident of chance (which is no explanation at all) or a gift from God (which again is not a scientific explanation). But while these figures seemed to think of artistic genius as being endowed entirely by nature with no contribution from nurture, modern genetic theory rejects that dichotomy. Instead of positing all-or-nothing natural abilities, behavioral scientists today think in terms of genetically inherited predispositions. In order for a genetic predisposition to develop into a trait with an observable phenotype, it needs to be triggered and shaped through a complex interaction between an organism’s genes and certain kinds of stimuli or environmental conditions. There are still open questions about exactly how, and how much, genes and environment feed into the development of any given trait, but it’s misguided to pose the binary nature-versus-nurture question as if the two were mutually exclusive (see Tabery 2014). Many researchers agree that some people have a stronger natural predisposition toward creativity than others, and that genius-level creativity partly stems from such a predisposition. Even so, the predisposition itself can be understood scientifically in terms of genetic heritability. (For a sampling of the relevant studies, see the essays collected in S.B. Kaufman 2013.)

Perhaps creativity seems inexplicable according to these accounts because it doesn’t follow rules or methods. In order to explain how to do something—how to build a boat or lead an army etc.—perhaps I need to be able to identify the rules or methods you should follow in order to practice and apply those skills. How-to explanations are instructions. But scientific explanations needn’t be instructions. A lot of good science explains how something happens—e.g., how heat melts ice or how a bat navigates its environment by echolocation—without explaining how to do it yourself.

Perhaps creativity seems inexplicable according to these accounts because creators themselves do not know how they create. But a scientific explanation needn’t be available through introspection. Most people cannot explain how their own digestive, circulatory, or perceptual systems work, but scientists who study those systems can.

Another line of thought is perhaps implicit in Kant but comes to the fore in Schopenhauer, who says that “the nature of genius consists precisely in the preeminent ability” to

consider things independently of the principle of sufficient reason, in contrast to the way of considering which proceeds in exact accordance with this principle, and is the way of science and experience. (World WWV: I: 192, §36)

The principle of sufficient reason says that for every fact there is a cause which completely explains that fact. So the defining ability of genius is to see things in a way that transcends the causal order and defies all explanation.

A version of this view is defended more recently by Carl Hausman (1975 [1984], 1979, 1985) who frames it in terms of novelty that creativity involves. Hausman asserts that if a product is creative, it must be metaphysically novel (or in his terms, “genuinely novel”) in the sense that it cannot be predicted from, or explained by, prior events—not even in principle. Creativity is therefore incompatible with causal determination and causal explanation: “A causal view of explanation sets a framework for ways of denying that there is anything new under the sun” (Hausman 1984: ix). If something can be explained by prior causes, it is not metaphysically novel, and is therefore, in Hausman’s view, not truly creative.

Against Hausman’s skeptical charge, Maria Kronfeldner (2009) argues that creativity is compatible with causal determination. First, causal determinism does not preclude novelty or change. Determinism says the emergence of new kinds of things can at least in principle be predicted in advance. Importantly, though, when this prediction becomes true, then something new is added to the world. Of course, not all novelty instantiates creativity. The question is whether the kind of novelty involved in creativity must be metaphysical novelty, which is by definition incompatible with causal determination. This is doubtful. Notice that, by definition, metaphysical novelty defies natural laws. The production of something metaphysically novel would therefore require supernatural powers. Traditional Western religions conceive of God as performing the miracle of creation ex nihilo. But are we positing a miracle every time we describe a human artifact or achievement as creative? Surely not. As noted above, human creativity is manifest in things that are novel relative to the agent producing them or new to human history, but both of those kinds of novelty (psychological and historical) are perfectly compatible with causal determination. As Kronfeldner explains, creativity does not preclude causes in general; it only precludes certain kinds of causes. A creative product, she argues, must be original—which means that it cannot be produced through a process of copying something prior. And it must be spontaneous (not produced through a routine or mechanical procedure)—which means that it is to some extent independent of the agent’s intentional control and previously acquired knowledge. (For more on originality and spontaneity, recall §2.2 above). Intuitively, the causes of something creative cannot simply be a matter of copying or following a routine. But it may have causes nonetheless, and cognitive science can investigate those causes, at least in principle. Indeed, as we’ll see next, it is doing so in practice.

5. The Cognitive Science of Creativity
Although creativity has been relatively understudied by contemporary philosophers, as we noted in §1, it has been receiving a great deal of attention from psychologists over the past few decades. In 1950, J. P. Guilford gave a presidential address at the American Psychological Association calling for research on the topic, and the field soon took off with waves of research investigating the traits and dispositions of creative personalities; the cognitive and neurological mechanisms at play in creative thought; the motivational determinants of creative achievement; the range of institutional, educational, and environmental factors that enhance or inhibit creativity; and more. Today, the blossoming of this field can be seen in the flurry of popular writing on its results; an official division of the American Psychological Association for the psychology of aesthetics, creativity, and the arts (Division 10); numerous academic conferences; dedicated peer-reviewed journals (Psychology of Aesthetics, Creativity and the Arts; Creativity Research Journal; Journal of Creative Behavior; International Journal of Creativity and Problem Solving); special issues of journals (Current Opinion in Behavioral Sciences, Takeuchi & Jung 2019); literature surveys (Hennessey & Amabile 2010; Runco & Albert 2010; Runco 2017; Glaveanu 2014; Williams et al. 2016); textbooks (J.C. Kaufman 2009; Sawyer 2012; R. W. Weisberg 1986, 2006); and a comprehensive encyclopedia (Runco & Pritzker 2020). According to one overview, creativity has been studied by nearly all of the most eminent psychologists of the twentieth century, and “the field can only be described as explosive” (Albert & Runco 1999: 17). There is also a groundswell of new work on creativity in the fields of computer science, artificial intelligence (AI), and robotics.

The present section surveys empirical work in psychology along with some related work in neuroscience, while the next section (§6) covers research in computing, AI, and robotics. Throughout, we’ll see that philosophers are actively in dialogue with these fields under the broad, interdisciplinary umbrella of cognitive science.

The vast body of empirical research of creativity can be seen as addressing a variety of issues, but the central question that concerns us here is the one we identified above as the challenge for explaining creativity: How are people creative? This question is analogous to a number of other questions in cognitive science: How do people perceive through sense modalities such as vision? How do they form concepts? How do they acquire a language? How do they make inferences? Just as psychologists investigate the psychological and neurological processes, systems, and mechanisms at work in these other mental operations, as well as the internal and external factors that either enhance or hinder these operations, they are doing the same for creativity. There is no pretension to achieving a complete explanation which would include each and every causal factor, and provide the basis for perfectly predicting creative outcomes in advance. But to the extent that we identify some of the relevant causal factors involved in creativity we thereby make progress in explaining creativity, just as we do with other features of the mind.

As we noted in §2, the standard definition of creativity in psychology says that a product (idea or artefact) is creative to the extent that it is both new and valuable (“effective”, “useful” or “appropriate”), and, in turn, people and processes are creative to the extent that they produce new and valuable things. As we also noted, many psychologists do not actually employ this, or any, definition of creativity in conducting their research. In one sampling of studies of creativity published in peer-reviewed psychology journals, only 38% of them included an explicit definition of creativity (Plucker, Beghetto, & Dow 2004), as they rely in one way or another on the assumption that we know it when we see it. For example, many studies use the Consensual Assessment Technique (CAT), whereby experimental subjects produce things that are then rated for how creative they are by a panel of experts in the relevant field; so paintings are rated by professional painters, stories by published authors, etc. Many other research methodologies are used, as we’ll see below.

Empirical research on creativity departs in several ways from the traditional approaches that seemed to place creativity outside the scope of science. For starters, in stark contrast to Plato’s supernaturalism, empirical psychologists take creativity to be a completely natural phenomenon. Creative people may of course be “inspired” in the sense of feeling energized or filled with ideas, but rather than being literally “breathed into” by some god or muse, their thoughts and behaviors are presumed to have causes that are perfectly natural. While it is difficult in practice to identify these causes, they are not in principle beyond the reach of science.

Further, the range of phenomena that contemporary researchers countenance within the ambit of creativity is far broader and more diverse than the traditional focus on poetry and the fine arts, as creativity can be manifest in any kind of art or craft, as well as in the sciences, technology, entrepreneurship, cooking, humor, or indeed in any domain where people come up with ideas or things that are novel and valuable in some way or another. Departing from Kant, genius, the highest echelon of creativity, may be acknowledged in virtually any of these domains, not just in the fine arts. And while a few researchers (e.g., Simonton 1984, 1994, 1997, 2009; Root-Bernstein & Root-Bernstein 1999) venture to examine genius (so-called “Big-C” creativity), most of them focus instead on relatively ordinary creative feats (“little-c” creativity) including the kinds of story-making, drawing, and problem-solving that can be elicited on command from regular people in experimental settings. Some researchers propose that in order to understand how the mind generates new ideas, we should begin with even more rudimentary phenomena. For example, philosopher Jesse Prinz and psychologist Lawrence Barsalou focus on how we form new concepts to categorize the things we perceive, a process which they claim is creative, albeit in a “mundane” rather than “exceptional” way (Prinz & Barsalou 2002; Barsalou & Prinz 1997; cf. Child 2018).

Of course, many feats of human creativity, and the ones that are most interesting, go far beyond the basic formation of concepts. A major step toward explaining those feats is to recognize that what we call “the creative process”, as if it were a single, homogenous phenomenon, is in fact an assembly of multiple stages or operations. The simplest recognition of this fact is the Geneplore model which distinguishes just two stages: generating ideas and exploring ideas (Finke 1996; Smith, Ward, & Finke 1995). This distinction may be seen as echoing one made by philosophers of science in the early twentieth century, between the context of discovery and the context of justification (Popper 1934). Other theorists posit up to eight stages of creativity (for a summary of proposals, see Sawyer 2012: 89). But the most influential stage-theory traces back to Henri Poincaré’s lecture, “Mathematical Creation” (1908 [1913: 383–394]), in which he identifies four phases in his own innovative work as a mathematician:

conscious hard work or preparation,
unconscious incubation,
illumination, and
verification.
In his book, The Art of Thought (1926), the psychologist Graham Wallas endorses Poincaré’s four stages with corroborating evidence from the personal reports of other eminent scientists like Hermann von Helmholtz. Wallas’s scheme, as a development of Poincaré’s, is still the one that is most widely cited, and we employ a version of it here with some slightly different terminology and with two more substantive alterations: instead of “incubation”, we identify the second operation more generally as the “generation” of ideas, which may include unconscious incubation but may also occur in conscious, deliberate thought; and we add “externalization” for a total of five operations:

Preparation—You invest a great deal of effort learning and practicing in order to acquire the knowledge, skills, and expertise required for work in a given domain.
Generation—You produce new ideas, whether through conscious reflection or unconscious incubation.
Insight—You consciously experience the emergence of a new idea, which would strike you with a feeling of surprise: “Aha!”, “Eureka!”
Evaluation – You assess the idea to determine whether it should be discarded, retained, revised, or amended.
Externalization—You express your idea in a concrete, observable form.
Artists provide compelling examples (though not the only ones) of each of these five operations. Such examples can be especially illustrative since they come straight from the artists’ mouths, as they reflect upon, and share, their creative process. The twentieth century painter Jacob Lawrence was known for painting in the style of visual narratives. Lawrence developed a system, much like a filmmaker’s storyboard, for the preparation of these paintings. He would lay as many as 60 wood panels on the studio floor, each with individual scenes and sometimes with captions. From these storyboards, Lawrence would generate and evaluate ideas and insights for a visual narrative, culminating in the paintings such as those in his Migration Series (see Whitney Museum, 2002, in Other Internet Resources). Toni Morrison, the Nobel prize winning novelist, remarks on the labors and sustained effort required at the preparation, generation, evaluation, and externalization stages of a creative writing process. Commenting on her novel Jazz, she says,

I thought of myself as like the jazz musician—someone who practices and practices and practices in order to be able to invent and to make his art look effortless and graceful. I was always conscious of the constructed aspect of the writing process, and that art appears natural and elegant only as a result of constant practice and awareness of its formal structures.

She further notes that insight does not always come in a flash,

[I]t’s a sustained thing I have to play with. I always start out with an idea, even a boring idea, that becomes a question I don’t have any answers to. (T. Morrison 1993)

Writer Ishmael Reed claims that insight can come unexpectedly and in various contexts:

One can find inspiration from many sources. The idea of Japanese by Spring originated in a news item that claimed the endowment to a major university was traced to Japanese mob, the Yakuza. Flight to Canada began as a poem. The Terrible series began when I heard someone at party mention that there was a black figure, Black Peter, in the Dutch Christmas, and by coincidence I was invited to the Netherlands shortly afterwards, where I witnessed the arrival of Saint Nicholas and Peter on a barge that floated into Amsterdam with crowds looking on. I took photos of the ceremony …. (Howell 2020: 91)

And with signature profundity, James Baldwin suggested that all elements of the creative artistic process, from preparation to externalization, require a basic enabling condition: being (and willing to be) alone (Baldwin 1962).

As Wallas recognized (1926: 81), and as the above examples suggest, the “stages” of the creative process are not necessarily discrete steps that follow one another in a tidy sequence. Creative work is messy: over time you have numerous ideas, keeping some and abandoning others in multiple rounds of trial-and-error; you incubate new ideas for one problem while you’re busy externalizing your ideas for another; and your moments of insight, evaluation, and externalization trigger further generative processes that send you cycling through these operations many times over. It’s still important to distinguish these operations, however, because, as researchers are confirming, they are enabled and influenced by different causal factors.

Among the additional stages that researchers have posited, one of the most widely discussed is known as problem-finding. Psychologists often conceptualize creative thought in terms of problem-solving: the ideas generated within the creative process are seen as candidate solutions to a given problem—where “problems” are broadly construed to include any creative aim, like that of producing a particular kind of artwork or proving a particular theorem, etc. (Flavell & Draguns 1957: 201; Newell, Shaw, & Simon 1962). But following some early work by Mihalyi Csikszentmihalyi (1965), many researchers came to appreciate that a lot of creative work is done not just in solving problems but in finding the right problem to begin with (Abdulla et al. 2020; Csikszentmihalyi & Getzels 1970; Getzels 1965; Getzels & Csikszentmihalyi 1975). While we agree that problem-finding often plays a key role in creativity, we have not assigned it to a separate stage, for the following reasons. Consider that you might settle on a problem to work on in either of two ways. On one hand, you might choose a problem to work on from a pre-existing menu of options. In that case, your choice would fall under the evaluation phase; it’s just that the idea you select is a problem that calls for the pursuit of further ideas. If, on the other hand, you develop a new problem, you would thereby be engaging in the generation of a new idea—the new problem—which may emerge in a moment of insight. Einstein and his colleague celebrated the novelty in such problem-finding:

The formulation of a problem is often more essential than its solution, which may be merely a matter of mathematical or experimental skill. To raise new questions, new possibilities, to regard old problems from a new angle, requires creative imagination and marks real advance in science. (Einstein & Infeld 1938: 92)

Either way—whether you “find” a problem by picking a pre-existing one or by coming up with a new one yourself—problem-finding, though important, does not need to be seen as an additional operation beyond the five listed above; it’s just a special case of generation, insight, or evaluation.

The next five sub-sections will respectively examine the five operations of creative work. Notice that three of them—preparation, evaluation, and externalization—are uncontroversially ordinary activities that involve no apparent mystery; it’s a challenge to explain them but no one is tempted to regard them as inexplicable or as violating the laws of nature. As we saw in §4, traditional skepticism about the possibility of explaining creativity is really focused on the two remaining phenomena: the generation of new ideas (§5.2) and the experience of insight whereby an idea seems to come out of the blue, as if from a god (§5.3).

5.1 Preparation
It’s myth that outsiders are more creative. To put yourself in a position to create anything of value, you have to spend a great deal of time and effort acquiring the relevant knowledge, skills, and expertise. In what has come to be called “the ten-year rule”, Howard Gardner (1993) found that, on average, people spend about 10 years learning and being immersed in a domain before they make any significant creative contribution to it.

Though a certain amount of rote learning is required, gaining mastery in a field is not simply a matter of passively absorbing information. Much of it involves what Anders Ericsson calls deliberate practice, where you focus on tasks which are a little beyond your current abilities, but which you eventually conquer through feedback and repetition. Across a variety of domains—including physics, medicine, programming, dance, and music—Ericsson found that, on average, world-class performance becomes possible for people only after 10,000 hours of deliberate practice in their chosen activity. This finding also converges on the ten-year rule, because if you engage in deliberate practice four hours a day, five days a week, that would add up to 10,000 hours in ten years (Ericsson, Krampe, & Tesch-Römer 1993; Ericsson et al. 2006).

However, there seems to be a point at which too much formal training can dampen creativity. Simonton (1984: 70–73) has reported that the relationship between creativity and education level is an inverted-U, as too much schooling can reinforce familiar, pre-established styles of thought. Even so, the point remains that, before you run into diminishing returns, years of preparatory learning and practice are required for exceptional creativity.

5.2 Generation
In this section we discuss four kinds of mental capacities or processes that researchers have posited for generating new ideas.

5.2.1 Blind Variation
Psychologist Donald T. Campbell (1960, 1965) proposed that creative thought proceeds through “blind variation and selective retention (BVSR)”. The “variations” he refers to are the various ideas that might occur to a creator, and the process of generating them is “blind” to the extent that it is not guided or directed by prior knowledge of how valuable or useful they will be: “Real gains must have been the products of explorations going beyond the limits of foresight or prescience, and in this sense blind” (Campbell 1960: 92, emphasis added). Once ideas have been generated, however, there is a subsequent stage where the creator selectively retains some of those ideas while discarding others, and Campbell says this stage is “sighted” rather than blind since it is guided by the creator’s judgments as to which ideas are valuable. While there is little debate that selective retention is sighted in this sense, there has been more controversy over whether the initial production of ideas is, by contrast, blind.

In his prolific body of work, Dean Keith Simonton has extended and refined Campbell’s proposal. His work nicely illustrates the interdisciplinary nature of creativity research as he, like Campbell, is a psychologist who engages with philosophers, some of whom are broadly sympathetic to the BVSR theory (Briskman, 2009; Nickles, 2003), while others are skeptical (Kronfeldner 2010, 2011, 2018). In earlier writings Simonton suggested, in a way Campbell did not, that BVSR is to be understood on the model of Darwinian evolution (Simonton 1999a, 1999b). But Simonton (forthcoming: 2–3) has come to rescind the Darwinian framing of BVSR, conceding that it is misleading. Reprising Campbell’s core idea, he says that a process of generating an idea is blind to the extent that it is not guided by “the creator’s prior knowledge of the variation’s utility” (Simonton forthcoming: 5; cf. Simonton 2011, 2012a, 2012b, 2018). He stresses that blindness is not all-or-nothing; it comes in degrees. An example of a highly sighted process is that of using the quadratic formula to find the roots of a quadratic equation: you know in advance that if you apply the formula correctly, it will yield the correct answer. Examples of relatively blind processes include remote association and mind wandering.

5.2.2 The Default-Mode Network
Despite the foregoing criticism of BVSR, recent neuroscientific studies suggest a network of brain activity that may serve the blind variation role. Brain activity doesn’t cease when one is not focusing on a task, when one is at rest, daydreaming, and so on. Following this insight, researchers have used neuroimaging methods to identify what is now called the default mode network (DMN). The precise anatomy of this network is still a matter of investigation, but it is supposed to be less active when one is focused on an external task (say a problem in the real world or in the lab) and more active when one is not so focused (Raichle et al. 2001; Buckner & DiNicola 2019). Notice then, that while this network is not creativity-specific—it is supposed to be active during memory recall, imagining future events, daydreaming, and so on—it does seem especially well-suited for creativity, and particularly for the random idea generation hypothesized by the BVSR (Jung et al. 2013). Creativity researchers in these fields often refer to this more “free” production of ideas as “divergent thinking”, and some argue on the basis of neuroimaging studies that creative thought requires cooperation between this mode of thought as well as that under “executive control”. As one team puts the point,

In general, we contend that the default network influences the generation of candidate ideas, but that the control network can constrain and direct this process to meet task-specific goals via top-down monitoring and executive control.. (Beaty, Benedek, et al. 2016; see also Mayseless, Eran, & Shamay-Tsoory 2015; Beaty, Seli, & Schacter 2019; Chrysikou 2019)

Notice how well this comports with both the Geneplore and the BVSR frameworks, perhaps identifying a way to keep some of the insights of both without commitment to a special creativity mechanism after all.

5.2.3 Imagination
At least since Kant, theorists have identified an important link between creativity and imagination; indeed, the two are sometimes unfortunately conflated. Construed broadly, imagination can take various forms: sensory imagery, propositional imagination, supposition, free association. Berys Gaut (2003, 2009, 2010) and Stokes (2014, 2016) have both recently argued that, although imagination and creativity are distinct, imagination is especially well-suited to creative thought because of its characteristic flexibility. They both agree that imagination is decoupled from action (Gaut 2003) and “non-truthbound” (Stokes 2014) in the sense that, unlike belief, imagination is not limited by the proper function of accurately representing (some part of) the world. This freedom or playfulness of imagination is crucial to generating new ideas, since it allows one to safely “try out” hypotheses, conceptual combinations, strategies for solutions, and so on, without epistemic or behavioral commitment.

A series of studies illustrates both the need for non-truthbound capacities in creative thought, as well as the difficulty of employing them. When people—children and adults alike—are asked to imagine and draw non-existent houses, people, or animals, they depict things that are strikingly similar to their familiar counterparts in the real world: imagined people, for example, were generally drawn with some version of a head, limbs, eyes, and so forth. (Karmiloff-Smith 1990, 1992: 155–61; Cacciari et. al 1997; Ward 1994, 1995). This suggests that we are highly constrained in our creativity by the concepts we already have. Concepts of existing things are truth-bound: your concept of an animal, for example, has the proper function of accurately representing the range of things that are in fact animals. When you try to envision a new, fictional kind of animal, you begin with a mental image that exemplifies your existing concept of animal, which is why you are constrained by that concept. You then have to manipulate your initial image, varying its features in ways that abandon the aim of accuracy, using a capacity that isn’t truthbound. Generalizing this point yields the cognitive manipulation thesis, according to which creative thought requires cognitive manipulation, which involves thinking in ways that are not bound to the truth (Stokes 2014: 167). Plausibly, imagination is the mental capacity which is best suited to serve in this cognitive manipulation role. In the studies just cited, subjects must use their imagination to manipulate their existing concepts so as to form new ideas.

Recent empirical research on visual imagery seems to corroborate this claim. Various studies have identified positive correlations between creative problem solving and visual image generation, image transformation, and vividness of imagery (Finke 1990, 1996; Zemore 1995; R. Morrison & Wallace 2001; Pérez-Fabello and Campos 2007). A more recent study highlights the importance of image transformation ability—the ability to mentally manipulate a given image—and the ability to achieve high degrees of visual creativity. Further, the results of this study suggest that although vividness negatively correlates with the practicality of images created, vividness positively correlates with novel idea generation (Palmiero et al. 2015). The novelty involved is minimal, but again it appears that imagination, here in the form of imagery, well serves the role of cognitive manipulation.

Stokes observes further that we can voluntarily control imaginative states (in contrast with other non-truthbound states, like desires and wishes). And because imagination connects in important ways with inferential systems, as well as affective systems, the thoughts it produces can often be integrated with knowledge and skills to formulate an innovative strategy or solution to a problem. Finally, this role for imagination in creativity is not exclusive to the rich creativity of artists and scientists, but indeed seems to characterize the minimally creative behavior that we all enjoy. This claim is partly motivated by the empirical research just discussed. Here, as in the more radical cases, instances of novel achievement or learning by subjects requires more than rote memorization; it requires cognitive manipulation of the information in the relevant conceptual space (e.g., combining concepts about houses and persons). This kind of cognitive activity is best done by using the imagination.

Peter Carruthers has argued that imagination is important to creativity on evolutionary grounds (2002, 2006; see also Picciuto & Carruthers 2014). Like the above analyses, he focuses on the playfulness of imagination. Pretend play typically develops early in childhood in humans. And imagination in adults provides the right mechanisms for generating and exploring ideas (just as required by the Geneplore model). Carruthers argues that imagination evolves under adaptive advantage as a kind of practice for adult creativity—and may have been accordingly selected for, aligning with the putative creativity explosion of 40,000 years ago (Mithen 1996, 1998; Harris 2000). This, he argues, is the most parsimonious explanation of both the emergence and the ubiquity of creativity in the human species. See B. Gaut (2009) for a critique of Carruthers’ analysis.

5.2.4 Incubation
While we may generate ideas consciously in imagination, we may also do so during a period of unconscious incubation, when we are focused on something else. This point is illustrated by any number of famous stories, though some are probably embellished after years of retelling. Isaac Newton witnessed an apple fall from a tree (on some accounts, falling upon Newton’s head) and thereby found the insight for his laws of gravity. August Kekulé is reported to have discovered the structure of the benzene molecule while daydreaming of a serpent circling upon and seizing its own tail. Henri Poincaré alleged that, while boarding a bus, he enjoyed a needed flash of insight that led to his discovery of non-Euclidian geometry. Richard Feynman, the Nobel prize winning physicist, claimed to find inspiration while sipping soda and doodling at adult clubs. And Einstein reported:

I was sitting in a chair in the patent office at Bern when all of a sudden a thought occurred to me. “If a person falls freely he will not feel his own weight”. I was startled. This simple thought made a deep impression on me. It impelled me toward a theory of gravitation. (Einstein, “Kyoto Lecture”, translated and quoted in Pais 1982: 179)

In each case, someone is suddenly struck with a flash of insight about one thing while engaged with something else entirely. The empirically-minded theorist rejects the notion that such ideas arise ex nihilo or through divine possession. So how are they explained in terms of natural mental phenomena?

Arthur Koestler, partly inspired by the work of Henri Poincaré (1908 [1913]), hypothesized that during creative thought processing, ideas are combined in novel ways, and this combination is performed largely unconsciously, by what Poincaré called the subliminal self (Koestler 1964: 164–5). For Poincaré there are only two ways we might think of the unconscious. One, we might think of the unconscious in Freudian terms, as a self capable of careful and fine discernment and, importantly, distinctions and combinations that the conscious self fails to make. Alternatively (and this is the option favored by both Poincaré and Koestler), we can think of the unconscious as a sub-personal automaton that mechanically runs through various combinations of ideas. Importantly, this unconscious process (or, if one likes, automaton) generates random conceptual associations and ideas. And these can then be further considered, examined, explored, and revised.

In the context of creativity in particular, there is precedent, or at least overlap, in Colin Martindale’s cortical arousal theory. This theory centers around the nature of focuses of attention (Martindale 1977, 1981, 1995, 1999; Martindale & Armstrong 1974; Martindale & Hines 1975). Martindale proposes a multi-stage model of problem solving, which if the right mechanism is possessed, leads to creative thought. In the initial stages, information is gathered, various approaches are taken to the problem, and there is a high level of cortical arousal with a narrow focus of attention. As information increases and the problem remains unsolved, two kinds of responses may occur. The first kind of response is to keep attempting the same solutions to the problem such that the arousal and attention focus stay high and narrow, respectively. Alternatively, some persons experience a decrease in cortical arousal coupled with a wider range of attention focus. Information then enters what Martindale calls primary processing: a kind of subconscious cognition not under the complete control of the agent. It is this kind of processing, and the arousal mechanisms that enable it, that distinguish creative insight or achievement from non-creative ones. The first kind of response typically results in frustration and failure (fixation), while the second often results in creative insight.

Some early studies on these phenomena centered around a familiar observation. Consider the tip-of the-tongue phenomenon, when you know that you know some bit of information (an actor’s name or the title of a song) but, try as you may, you just can’t recall it. It often helps to give up for a moment and allow the memory to surface without effort. Researchers found that the same approach—forgetting about a problem—works well to overcome fixation on ineffective ideas so as to allow the actual solution to pop up. Smith and Blankenship primed two groups of subjects with inappropriate or misleading solutions to problems. They left one group to continue struggling with the same problem, while they distracted the second group with a distinct but cognitively demanding task. The second group thereby overcame fixation and outperformed the first group when returning attention to the original target problem (Smith & Blankenship 1989, 1991; see also Smith, Ward, & Finke 1995).

These behavioral methods can be combined with contemporary understanding of neural plasticity and the effects of cognitive effort and attention. Neuroscientists have long recognized that the human brain is plastic—stable in genetic material but constantly undergoing functional change and development in neural networking in response to external stimuli, with the work of Donald Hebb in the middle of the twentieth century being one important early precedent. As Hebb put it, neural cells that “fire together, wire together”. Cell assemblies thus form as a result of the synchrony and proximity of the firing of individual cells.

[A]ny two cells or systems of cells that are repeatedly active at the same time will tend to become “associated”, so that activity in one facilitates activity in the other. (Hebb 1949 [2002: 70])

And continued attention to a problem, what some have called cerebral effort, causes changes in the networking of the brain’s cortex (Donald 2001: 175–8). Importantly, these changes can continue to take place, to “reverberate” even after one has removed attention from that problem. This motivates a simple (and somewhat unsurprising) hypothesis: attending to and performing cognitive tasks affects neural networking (Posner et al. 1997; Posner & Raichle 1994; see also Kami et al. 1995), and those changes can involve strengthening of synaptic connectivity (which correlate with conceptual connections and associations). These changes, again, can occur both when one is attending to a task and after one has diverted attention elsewhere. And, finally, the latter goes some way to explain a moment of insight after incubation (the so-called incubation effect): when one returns attention to the target problem, new or newly strengthened neural connectivity (as a result of previous cognitive effort) can give rise to a new idea. And because that neural process is not in any sense done by you, the emergence of the new idea can feel like a burst of insight (see Stokes 2007; Thagard & Stewart 2011; Ritter & Dijksterhuis 2014; and Heilman 2016).

There are also various recent studies on closely related topics: on mindwandering and spontaneous thought (Christoff et al. 2016; Irving & Thompson 2018; Murray et al. forthcoming), on so-called “divergent thinking” (Mekern et al. 2019), and more on the neural basis of insight (Jung-Beeman et al. 2004; Bowden et al. 2005; Limb & Braun 2008; Dietrich & Kanso 2010; Kounios & Beeman 2014).

5.3 Insight
It should be intuitive that creativity often involves solving problems and doing so in interesting or surprising ways. In exceptional cases, the individual identifies a problem solution that perhaps no one (including the creator) anticipated. But there are countless examples of more mundane instances of problem solving, where the solution may be surprising (or especially interesting) to only a few individuals, perhaps even only to the problem solver. One broad, standard experimental method used by researchers thus focuses on insight in problem solving. Some problems (thankfully!) can be solved by straightforward appeal to memory, or by applying some technique or method of calculation in a mechanical way. Solving the problem may still take time and effort, but the solution will come so long as one executes the appropriate strategy or applies the relevant knowledge from memory. An insight problem, by contrast, typically requires something new on the part of the individual, and one must often “change views” of the structure of the very problem. Predictably, there are a variety of definitions or characterizations of “insight” in the literature. Here are two recent, representative examples. Bowden et al. suggest that insight occurs

when a solver breaks free of unwarranted assumptions, or forms novel, task-related connections between existing concepts or skills. (Bowden et al. 2005: 322)

More recently, Kounios and Beeman write,

we define insight as any sudden comprehension, realization, or problem solution that involves a reorganization of the elements of a person’s mental representation of a stimulus, situation, or event to yield a nonobvious or nondominant interpretation. (2014: 74)

There are at least two, separable components of insight thus understood. First, an insight problem requires non-mechanical or non-algorithmic solution, and this in turn requires some kind of conceptual reorganization. A hackneyed phrase may come to mind here: one has to “think outside the box”.

The second element of insight as understood here is subjective or phenomenological. An insightful problem solution is often described as occurring suddenly and with little or no apparent effort. It is an aha moment, even if less dramatic than the traditionally romanticized Eureka moment. One way researchers have tested for this subjective feature is to ask subjects to report nearness or “warmth” relative to solving a problem. They find that for insight problems, by contrast to non-insight problems, subjects report that as they near solution they experience abrupt changes in the sense of warmth for solving the problem (Metcalfe & Wiebe 1987; see also Dominowski 1995; Laukkonen & Tangen 2018). More recently, researchers have begun to employ neuroimaging techniques to study insight and insightful problem solving (Luo & Niki 2003; Mai et al. 2004).

First, researchers have developed methods for using subjective report, where subjects rate whether they felt that they used insight in solving a designated problem (Bowden et al. 2005). And second, and coupled with those report methods, researchers have developed simple problems that can be solved with insight. One such example is the “Compound remote associates problem” (CRA). Here is an example of a CRA problem:

Each of the three words in (a) and (b) below can form a compound word or two-word phrase with the solution word. The solution word can come before or after any of the problem words.

french, car, shoe
boot, summer, ground[1]
(Bowden et al. 2005: 324)

Because of their simplicity, these problems can be solved unambiguously and quickly, and with this speed comes better potential for neuroimaging study. In instances where subjects report insight solutions to these kinds of problems,

EEG shows a burst of high-frequency (gamma-band) EEG activity over the right temporal lobe, and fMRI shows a corresponding change in blood flow in the medial aspect of the right anterior superior temporal gyrus (Jung-Beeman et al. 2004). (Kounios & Beeman 2014: 78)

The question for neuroscientists is whether this convergence of evidence is sufficient to establish neural correlates of insight.

5.4 Evaluation
A moment of “insight” can be misleading, as what initially strikes you as a promising idea may ultimately turn out to be a dead end. You may have countless ideas in the course of undertaking a complex creative project, while only a few of them will make the final cut. A crucial part of your creative work therefore consists in evaluating your ideas. For any idea that occurs to you, you might have to ask: Will this work? Is it new? How does it fit in with other parts of your project? Do you have the resources and abilities to bring it to fruition? Is it worth the time and effort?

Much of the research on this phase of the creative process is concerned to identify and categorize the range of factors that people take into consider as they evaluate their ideas (Blair & Mumford 2007; Dailey & Mumford, 2006). Unsurprisingly, those factors vary from one domain to another. New culinary dishes are judged by factors like aroma, taste, texture, color, presentation (Horng & Lin 2009), whereas improved musical performances are judged according to their complexity, originality, and technical virtuosity (Eisenberg & Thompson 2003), and so on. Your understanding of the relevant factors is part of your internalized model of the domain (Bink & Marsh, 2000; Csikszentmihalyi & Sawyer 1995). And since you acquired and refined that model through years of preparation, your capacity for evaluation is largely a consequence of your efforts from that initial stage.

Somewhat more surprisingly, there is some evidence that people who are good at evaluating ideas are also good at generating them (Runco 1991; Runco & Dow 2004; Runco & Chand 1994; Runco & Vega 1990).

Other studies support what Sawyer calls Sawyer (2012: 131) calls the productivity theory, which says that the best way to get good ideas is to have lots of ideas and just throw away the bad ones. In historiometric studies, Simonton found that creators who yielded the greatest number of works over their lifetimes were mostly likely to produce works that were significant and stood the test of time. Even more striking, he discovered that, from year to year, the periods when creators were most productive were also the ones in which they were most likely to do exceptional work (Simonton 1988a, 1988b). Linus Pauling, who won the Nobel Prize in Chemistry in 1954 as well as the Nobel Peace Prize in 1962, summed up the productivity theory in a famous remark:

If you want to have good ideas you must have many ideas. Most of them will be wrong, and what you have to learn is which ones to throw away. (quoted by Crick 1995 [time 34:57])

5.5 Externalization
The final operation of the creative process—externalizing ideas—may involve any number of disparate activities, which Keith Sawyer sums up as follows:

Creativity research has tended to focus on the early stages of the eight-stage creative process—particularly on the idea-generating stage. But a lot has to happen to make any idea a reality. Successful creators are skilled at executing their ideas, predicting how others might react to them and being prepared to respond, identifying the necessary resources to make them successful, forming plans for implementing the ideas, and improvising to adjust their plans as new information arrives. These activities are important in all creativity, but are likely to be even more important in practical domains such as technological invention and entrepreneurship (Mumford, 2003; Policastro & Gardner, 1999). (Sawyer 2012: 133–4)

It may be tempting to assume that the real creative work is finished once a new idea emerges in the moment of insight, and that externalization is just the uncreative, mechanical chore of making the idea public. But a closer look at the phenomenon reveals that externalization is often integral to creativity itself.

Vera John-Steiner (1985) interviewed, and examined the notebooks of, over 70 exceptional creators (ranging from author Anaïs Nin to composer Aaron Copland), and consulted the notebook of another 50 eminent historical creators such as Leo Tolstoy and Marie Curie. A recurring theme throughout was that at the beginning of each creative endeavor and continually throughout its development, creators manipulate and build upon their impressions, inklings, and tentative hunches using sketches, outlines, and other external representations.

Perkins (1981) corroborated this finding by analyzing the 61 sketches Picasso made en route to painting his famous work, Guernica, as well as Beethoven’s musical drafts and Darwin’s notebooks. In each case, the artist progressed by engaging with external representations.

Other studies found that people discovered and solved more problem when they used sketches during a task (Verstijnen 1997), and that people come up with better ideas for improving inventions when they work with visual diagrams (Mayer 1989).

One reason externalization is so vital to substantial creative work is because of our limited capacity to consciously hold and manipulate information in our minds. It helps to offload ideas and store them in the form of physical symbols and expressions in order to free up space for the mind to examine those ideas at arm’s length while entertaining new ones. Thus research shows that internal strategies like mental visualization can help with relatively simple tasks, but for more complex projects externalization is key (Finke et al. 1992: 60).

5.6 Worries and future directions
We close our survey of the cognitive science of creativity with a brief discussion of some general worries about current work, and some prescriptions for future research.

Some have worried about the validity of the psychometric measures employed in neuroimaging studies. One such concern regards the confidence that we should have that the tests employed are really tracking creative behavior. This is of course a general problem, partly symptomatic of the challenges that come with defining creativity (like other phenomena) and with the special challenges that attach to features such as insight and incubation. But there are particular challenges that come with using neuroimaging technologies such as fMRI scanning to attempt to study naturally occurring phenomena. Use of this technology is almost invariably ecologically invalid—one cannot run an fMRI in the artist’s studio. And because of the cost and sensitivity of these imaging systems, the correlative behavioral tests are often significantly abbreviated. This may impose constraints on space for occurrence of the target phenomena—novel thinking and insight—during the imaging session. As one researcher worries,

Too often single tests are used—or even single items! This is contrary of psychometric theory in general (where longer tests allow errors to cancel themselves out and are thus more reliable) and true of the research on creativity assessment in particular, where differences among items and even tests are common (Richards, 1976; Runco, Mohamad, & Paek, 2016 [sic should be Runco, Abdulla et al. 2016). Results from any one test will not generalize to other tests. Results from a single item of course have even less generalizability. (Runco 2017: 309–310; see also Abraham 2013)

Another empirical researcher criticizes what he sees as “the wild goose chase” in the neuroscience of creativity. Arne Dietrich (2019) recapitulates the above worries about validity of psychometric measures and their abbreviated and piecemeal application. He further worries about the now dominant emphasis on divergent thinking, and the default mode network (as well as the now mostly abandoned emphasis on notions such as madness, the right brain, and REM sleep). Dietrich’s concern in each case is that the research emphasis is unhelpfully myopic, and that while the imaging methods are sound and state of the art, the characterization of creativity is not. He decries the temptation to identify what may be a feature of creativity with the whole of the phenomenon. Divergent thinking, he suggests, is likely a cluster of various mental phenomena rather than a singular one, and

there is no effort underway to dissect divergent thinking and link it to the kinds of cognitive processes we use to operationalize all other psychological phenomena, such as working memory, cognitive control, semantic memory, perceptual processes, or executive attention. (2019: 37)

Notice, then, that the “wild goose” for Dietrich is to hastily conclude and then center studies around a singular, special creativity mechanism.

Dietrich also offers various prescriptions for remedy. To combat myopia, he suggests (as some have in other disciplines, e.g., Boden 2004) a plurality of types of creativity (and/or features of creativity). He cautions,

Since different types of creativity contain opposing brain mechanisms—focused versus defocused attention, for instance—any all-encompassing claim about creativity in the brain will almost certainly qualify as phrenology. (2019: 39)

He pairs this with a prescription for a more interdisciplinary approach to the topic. Others in the field have made the same prescription, advocating a “systems” approach sensitive both to the multi-faceted nature of creativity and the value of theorizing at multiple levels of explanation (Hennessy & Amabile 2010).

These directives for future research seem hard to resist. At the very least, it would seem advantageous to ensure that the full range of empirical method across the behavioral and brain sciences is communicated across the relevant sub-disciplines. This would ideally lead to better collaboration amongst such researchers. What’s interesting is that a cousin to this prescription is not well heeded by the same researchers advancing it here. However little crossover there is between, say, behavioral psychologists and neuroscientists in studies of creativity, there is comparatively even less crossover (almost none) between the psychological sciences and computational approaches to creativity. The next section thus begins by highlighting this “gap”, and identifying some of the potentially fruitful areas for interdisciplinary work on that front. It then continues with a discussion, generally, of research on creativity in the fields of computing science, artificial intelligence, and robotics.

6. Creativity and Artificial Intelligence
Just as we find in psychology and neuroscience, there is a rich research literature on creativity in artificial intelligence and computer science, with devoted journals, special issues, and conferences (The Journal of Artificial Creativity, The Journal of Creative Music Systems, Digital Creativity, Minds and Machines special issue on Computational Creativity [Gervás et al. 2010], The International Conference on Computational Creativity). The question we focus on here is whether a computer could be creative. As background, it is worth considering how theorists approached the analogous question as to whether a computer could think.

Although theorists of various kinds have asked whether machines can think since at least the early modern period, the most important conceptual innovations on the topic came from Alan Turing, centering around his 1950 paper “Computing machinery and intelligence”. Here Turing provided a number of groundbreaking insights. Perhaps most familiar is Turing’s “imitation game”, now commonly known as “the Turing Test”. In brief, the test involved an unknowing interrogator who could ask an open-ended series of questions of both a human and a computer. If the interrogator could not distinguish computer from human, Turing postulated that this would suffice to illustrate genuine intelligence. There is no shortage of controversy regarding the aptness of the test for intelligence, and arguably no computer has yet passed it. (For more thorough discussion of Turing and the Turing test see entries on Alan Turing, Turing machines, and the Turing test).

Successful performance in Turing’s game would require remarkable behavioral flexibility. And it is highly operational: specify a threshold for imitation, and then simply allow the interrogator to ask questions, then assess performance. If the behavior is sufficiently flexible to fool the interrogator, Turing claimed, the behavior was intelligent and, therefore, the computer intelligent.

With this background in mind, what are some of the cases in AI research lauded as success cases, and how do they align with some of Turing’s criteria?

Many of the familiar success cases are highly specialized. Deep Blue defeated chess master Garry Kasparov (Kasparov & Greengard 2017); some language processing systems managed to navigate social contexts such as ordering from a menu at a restaurant (Schank & Abelson 1977); AlphaGo more recently defeated the world champion Go player. This specialization is both a virtue and a limitation. On the one hand, achievement in such a specialized domain implies an exceptional amount of detailed memory and skill. On the other hand, this knowledge and skill does not generalize. Neither Deep Blue nor Alpha Go could successfully order from a menu, along with countless other basic human tasks. Put in terms of Turing’s imitation game, these systems would fail miserably to fool a human, or even remotely imitate one (except for their performance in a very narrow domain). What about systems such as IBM’s Watson, which famously won (against humans) on the television game show Jeopardy! This performance is more general, since topics on the show vary widely, and seemed to require both language comprehension and some minimal reasoning skills (see entry on artificial intelligence for extended discussion). Even so, Watson’s capabilities are still quite limited: it cannot make fluid conversation “in real time” and is largely insensitive to temporal and other factors that come with context.

There are many, many more examples of computational systems that display sophisticated behavior, from the highly specialized to the more general. On the language processing front, very recent AI systems such as OpenAI’s ChatGPT and Google’s LaMDA significantly outperform the systems described above. To be clear, these are remarkable achievements that display substantial complexity and, it appears in some cases, significant flexibility—features Turing highlighted in characteristically human behaviors. But this also underscores a distinction, often invoked by critics of artificial intelligence research. There is a difference between a computer’s displaying or merely imitating an intelligent behavior, and a computer’s instantiating intelligence through such behavior. And the critic will say, even if a computer behaves as if it is intelligent, this is just modeling or simulating intelligence. The greater ambition, though, is “genuine artificial intelligence”, a system that actually thinks. John Searle refers to this as the distinction between “weak AI” and “strong AI”, respectively.

Weak AI: Could a computer behave as if it thinks?
Strong AI: Could a computer genuinely think?
The general worry here is that however sophisticated a system’s behavior may appear “from the outside”, for all we know it may just be a “hollow shell” (Haugeland 1981 [1997]; Clark 2001). The worry has then been fleshed out in various ways by specifying what is missing from the shell, as it were. Here are three standard such candidates. And, again, in each case however sophisticated the computer’s behavior may appear it still may be lacking in any or all of the following. First, the computer may lack consciousness. Second, the computer may lack any understanding of the symbols over which it computes (Searle 1980). Finally, the computer may operate without caring about its own behavior or, as John Haugeland colorfully puts it, without “giving a damn”. In each case, any kind of response from the ambitious AI researcher encounters the substantial challenges that come with theorizing mental phenomena such as consciousness, understanding, linguistic competence, and emotion. (Turing 1950, for instance, recognized but largely eschewed these kinds of topics).

It’s one thing to ask whether computers could think, and another to ask whether they could be creative. And just as the prospect of artificial intelligence or thinking divides into two questions—of weak AI and strong AI—we may distinguish two analogous questions about artificial creativity, which we’ll refer to as the questions of “weak AC” and “strong AC”, respectively. To begin with the former:

Weak AC: Could a computer behave as if it’s creative?
Something behaves as if it’s creative if it produces things which are psychologically new (new to that thing) and valuable. Arguably, a number of computers have already done that.

In the 1970s, Harold Cohen began using computational technologies to produce new drawings and paintings. The work of his computer painter, Aaron, has exhibited at galleries such as the Tate and the Victoria and Albert Museum in London. David Cope’s “EMI” (Experiments in Musical Intelligence) has composed musical works in the style of various known composers and styles, even a full-length opera. Some of these works have been recorded and produced by bona fide record labels. Just search “Emily Howell” on Spotify or Apple Music and give it a listen (Cope 1996, 2006). Simon Colton’s The Painting Fool is an ongoing project, involving a software that abstracts phrases, images, and other items from newspaper articles and creates collage-style pieces. It has also produced portraits, based on images of film characters, of the same individual in different emotional states (see Painting Fool in Other Internet Resources; see Colton 2012 for theoretical discussion). Even more recently, there have been explosive developments in generative art systems like DALL•E, Midjourney, Stable Diffusion, VQGAN+CLIP. (For discussion see Paul & Stokes 2021). In all of these cases, the relevant outputs of the computer program are new relative to its past productions—so they are psychologically (or behaviorally) novel, which again is all the novelty that creativity requires. And although historical novelty isn’t required for creativity, it’s worth noting that these products appear to be to be new in all of history as well.

What about value? As noted above in §2.1, some theorists reject the value condition, but even if value is required for creativity, that too is a condition these computer artworks seem to meet. Assessments of value can be controversial, but that is no less true for the outputs of human creativity. The fact that these works are critically acclaimed, showcased in prestigious galleries, and commissioned by selective record labels testifies to their artistic merit, and viewers find them pleasing, interesting, and appealing, even before being apprised of their unusual origin. So it is reasonable to conclude computer programs like the ones just described exhibit at least weak AC insofar as they produce works of valuable novelty, and one could cite many more examples in the same vein.

Some theorists have noted that, whether or not the original Turing test is a good test for intelligence or thinking, we might adopt an analogous test for creativity: If a computer can fool human observers into thinking that it is a human creator, then it is in fact creative (Pease & Colton 2011; see also Chen 2020 for useful discussion of artificial creativity, including many additional examples of particular cases, and so-called Dartmouth-based Turing tests). If we employ this test, we might find ourselves with an unexpected conclusion: computers can be creative; in fact, some of them already are. But one might reasonably worry that the test is inadequate and the conclusion is too quick (Berrar & Schuster 2014; Bringsjord et al. 2001). From the fact that a computer operates as if it’s creative, one might argue, it doesn’t follow that it really is. Which brings us to our next question:

Strong AC: Could a computer genuinely be creative?
This obviously returns us to the question of what conditions something must meet in order to count as being genuinely creative. And here we need go beyond the outwardly observable product-features of novelty and value to consider the underlying processes of genuine creativity. As we saw in §2.2, theorists have variously proposed that in order for a process to count as creative, it must be surprising, original, spontaneous, and/or agential. There is no consensus to appeal to here, but if any one of these conditions is indeed required for genuine creativity, then a computer could be genuinely creative only to the extent that it executes processes which satisfy that condition.

The classic statement of skepticism regarding the possibility of computer creativity is due to Lady Ada Lovelace who had this to say while remarking on “the Analytical Engine” designed by her friend Charles Babbage:

It is desirable to guard against the possibility of exaggerated ideas that might arise as to the powers of the Analytical Engine. The Analytical Engine has no pretensions whatever to originate anything. (Lovelace 1843, italics added)

Though Lovelace does not frame her comments in terms of “creativity” as such, she explicitly denied that a computer could satisfy at least one condition that is plausibly required for creativity, namely originality. A computer cannot be the originator, the author, or the creator of anything new, she contends; it can only do what it is programmed to do. We cannot get anything out of a computer that has not already been programmed into it. Further, Lovelace may also be interpreted as expressing or implying doubt about whether a computer could satisfy the three other proposed requirements for genuine creativity. Insofar as a computer’s outputs cannot be original, one might also suspect that they cannot be surprising. The image of a machine strictly following rules invokes precisely the kind of mechanical procedure that is the antithesis of spontaneity. And it may seem that such a machine could not be a genuine agent either. The problem isn’t just that a computer can’t produce anything original; it’s that it deserves no credit for whatever it does produce. Any praise or blame for the outputs of a computer rightly go to the engineers and programmers who made the machine, not to the machine itself. While these points may be intuitive, at least some of them are being challenged by modern technologies, which have come a long way since Babbage’s invention.

Consider AlphaGo again. This is a “deep learning” system, which involves two neural networks: a Policy network and a Value network. Very briefly: The system is trained using a vast number of legitimate moves made in actual games of Go played by professional human players (28.4 million moves from 160,000 games, to be precise; see Silver et al. 2016 and Halina 2021). The network is further trained, again using learning algorithms, by playing many games (some 100 million) against previous versions of itself (in the sense of a differently weighted neural network). The weights of nodes in the network are then adjusted by a learning algorithm that favors moves made in winning games. The value network is trained over a subset of these many games, with node weighting adjustments resulting in reliable probability assignments to moves vis-à-vis their potential to contribute to a win. Finally, the system employs a Monte Carlo search tree (MCT). Generally, this kind of algorithm is designed to simulate a decision process to optimize success given chosen parameters. In this case, the search algorithm selects a given path of moves, then adds some valid moves to this path, and then if this process does not terminate (end in win/loss), the system performs a “rollout”. A rollout essentially plays the game out for both players (using samples of possible moves) to its conclusion. The information that results from the MCT and processing by the value network are then fed back (back propagated) into the system. This entire process (once the system is trained) is rapid and determines how AlphaGo “decides” to move in any given game.

Here are some things to note. AlphaGo’s style of play is surprising. As commentators have noted, it is starkly unconventional relative to standards of human play (Halina cites Baker and Hui 2017 [Other Internet Resources]). Indeed, Lee Sodol, the world champion Go player defeated by AlphaGo in 2016, remarked that AlphaGo’s play revealed that much of human play is, contrary to prior common opinion, not creative after all—intimating that at least some of the play of AlphaGo is. Note further that this system is flexible. While there are learning algorithms and rules that adjust network weights, the system is not mechanical or predictable in the same fashion as earlier, classical systems (including Deep Blue, for example). In a recent paper, Marta Halina has made this argument (Halina 2021). She explicitly invokes Boden’s characterization, which requires novelty, value, and surprise of creativity. Again, the novelty and value should be plausibly attributed in this case. Regarding surprise, Halina suggests that it is AlphaGo’s employment of MCT that enables a kind of “insight”, flexibility, and unpredictable results. She writes,

It is the exploration parameter that allows AlphaGo to go beyond its training, encouraging it to simulate moves outside of those recommended by the policy network. As the search tree is constructed, the system starts choosing moves with the highest “action value” to simulate, where the action value indicates how good a move is based on the outcome of rollouts and value-network evaluations. (Halina 2021: 324)

Halina grants that given its domain-specificity, as we have already noted, this system’s particular abilities do not generalize in a way that may be required to properly attribute genuine intelligence. But she suggests that the complex use of the MCT search may amount to “mental scenario building” or, we might say, a kind of imagination. And insofar as this search algorithm technology can be applied to other systems in other domains, and imagination is a general component of intelligence, perhaps here lies space for generalizability. AlphaGo also affords at least some reply to the traditional Lovelace worry.

Artificial systems do not act only according to preprogrammed rules hand-coded by engineers. Moreover, current deep-learning methods are capable of producing systems that are superhuman in their abilities to discover novel and valuable solutions to problems within specific domains. (Halina 2021: 327)

If this is right, then AlphaGo exhibits originality. Finally, the flexibility with which this system operates may also satisfy Kronfeldner’s spontaneity requirement.

Some of these same features are found in a related approach in AI, namely research in evolutionary robotics. These systems also involve various forms of machine learning but in this case the learning is distributed, as it were, across a population of individuals rather than one individual. This approach can be understood, albeit imperfectly, as analogous to natural evolution. One begins, typically in computer simulation, with a population of agents. These agents are typically identified with individual neural networks, the connections and weightings of which are random to start. Relative to some task—for instance, avoiding obstacles, collecting objects, performing photo or phonotaxis—a genetic algorithm assigns a fitness value to each individual agent after a certain period of time or number of trials. Fitter agents are typically favored and used to generate the next population of agents. Also included in this generation are random mutation and genetic crossover (digital breeding!). Although it can take hundreds of generations, this is a discovery approach to engineering or constructing a system that successfully performs a task; it is “gradient descent learning” (Clark 1996). In this bottom-up approach, no single individual, nor even an entire population, are in any strict sense programmed. Rather, successful agents have “learned” as a result of generations of randomness, crossover, and small fitness improvements (and lots and lots of failures). Early success cases evolved robots that can follow trails (Koza 1992), locomote in insect-fashion (Beer & Gallagher 1992), guide themselves visually (Cliff, Husbands, & Harvey 1993), and collect garbage (Nolfi & Floreana 2000). See Bird and Stokes (2006, 2007) and Stokes and Bird (2008) for analysis and study of creativity in the context of evolutionary robotics.

These systems most certainly produce novelty. Later, fit individuals achieve novelty at their aimed task relative to whole generations and populations of previous agents. And this novelty is often surprising to the engineers and programmers that build them, indeed sometimes even unpredictably independent of any relevant task for individuals in the population. There are many examples in the literature. Indeed Lehman and others (2020) catalog a large range of cases where digital evolution surprises its creators, categorizing them in four representative groups: “mis-specified fitness functions”, “unintended debugging”, “exceeded experimenter expectations”, and “convergence with biology”. Here is one now relatively famous example of the first type of case. In early research in artificial life (A-Life), Karl Sims (1994) designed virtual creatures that were supposed to learn to walk (as well as swim and jump) in a simulated environment. The fitness function assessed individual agents on their average ground velocity across 10 seconds. Some of the fittest individuals to evolve were surprising: they grew tall and rigid and when they would fall over they would achieve high ground velocity, thus maximizing fitness given the (mis)specified parameters in unpredicted ways.

This is but one example of how systems like these can evolve in unpredictable or surprising ways. This unpredictability has occurred not just in simulated robotics, but in embodied robotics as well. In using a genetic algorithm to attempt to evolve oscillating sensors, researchers unintentionally evolved a radio antenna (Bird & Layzell 2002). This unexpected result arose from a combination of the particular algorithm used (which was intended) and various physical features of the space such as proximity to a PC monitor (which the researchers had presumably deemed irrelevant but which the evolved system, in a sense, did not). And one might be further inclined to describe some of these achievements as creative (and not just in the trivial sense that they are original instances of robotic success), since they also produce value, at least insofar as they are useful at performing a task, whether it is locomoting or locating a source of light or sensing radio waves.

Some theorists in this domain might argue that these systems achieve spontaneity as well. Given the substantial inclusion of randomness in the system’s development—both at the outset when the individual’s neural networks are randomized and more importantly with random mutation across populations—it is intuitive to describe the system’s as not following a mechanical procedure. Indeed, the way in which systems exploit fitness functions and data patterns further underscores this point. (Again, see the rich catalog of cases offered by Lehman et al. 2020).

On the face of it, then, recent technologies in AI, evolutionary robotics, and artificial life, seem to fulfill many of the conditions proposed for genuine creativity. These systems produce things that are novel and valuable, and do so through computational processes that are plausibly surprising, original, and spontaneous. The one requirement we have yet to address, however, is agency. Recall the suggestion, implicit in Lovelace’s remarks, that whatever a computer produces is to the credit of the programmer, not the computer. Notice that as sophisticated as current technologies in artificial creativity may be, presumably they are still not subject to praise or blame for what they do. If any beings are responsible for the work of these programs, it still seems to be the programmers and engineers who make them, not the programs themselves. The programs themselves do not seem to “give a damn”. So, if the creative process requires agency, arguably we have not yet created, programmed, or evolved a computational system that is really creative, however much they might appear to be. In the pursuit of strong AC, agency might be the final frontier (Paul & Stokes 2021).

7. Conclusion
It should be clear from the above discussions that there are rich and lively research programs, across a range of scientific disciplines, studying human creativity. These approaches substantiate the view that, contrary to the romantic tradition, creativity can be explained. Psychological functions and neural correlates have been identified, and remarkable advances are being made with computational and robotics technologies. What may be less clear is that, despite these advances, the distinct research programs in question are largely disjoint or siloed.

In a recent paper, Geraint Wiggins and Joydeep Bhattacharya (2014) highlight this “gap” between scientific studies of creativity. Their particular emphasis is on the gaps between research in neuroscience and research in computer science, and they advocate a bridge in the form of a neurocomputational approach. This kind of bridging may be called for even beyond what these authors prescribe, since there are gaps not just between these disciplines, but also between these and behavioral psychology, AI and A-Life research, and philosophical analysis. Creativity is a deeply complex and deeply important phenomenon. Fully understanding it will require us to integrate a variety of theoretical perspectives, and, as this survey reveals, philosophy has a vital role to play in that endeavor.

The primary distinguishing feature of Darwin’s theory that separates it from previous explanations of species change centers on the causal explanation he offered for how this process occurred. Prior theories, such as the theory of Jean-Baptiste Lamarck (see entry on evolutionary thought before Darwin), relied on the inherent dynamic properties of matter. The change of species was not, in these pre-Darwinian efforts, explained through an adaptive process. Darwin’s emphasis after the composition of Notebook D on the factors controlling population increase, rather than on a dynamic theory of life grounded in vital forces, accounts for many of the differences between Darwin’s theory and those of his predecessors and contemporaries.

These differences can be summarized in the concept of natural selection as the central theoretical component of Darwinian theory. However, the exact meaning of this concept, and the varying ways he stated the principle in the Origin over its six editions (1859–1872), has given rise to multiple interpretations of the meaning of this principle in the history of Darwinism, and the different understandings of his meaning deeply affected different national and cultural receptions of his theory (see below, Section 3.1).

One way to see the complexity of Darwin’s own thinking on these issues is to follow the textual development of this concept from the close of the Notebook period (1839) to the publication of the Origin of Species in 1859. This period of approximately twenty years involved Darwin in a series of reflections that form successive strata in the final version of his theory of the evolution of species. Understanding the historical sequence of these developments also has significance for subsequent controversies over this concept and the different readings of the Origin as it went through its successive revisions. This historical development of the concept also has some bearing on assessing Darwin’s relevance for more general philosophical questions, such as those surrounding the relevance of his theory for such issues as the concept of a more general teleology of nature.

The earliest set of themes in the manuscript elaboration of natural selection theory can be characterized as those developed through a particular form of the argument from analogy. This took the form of a strong “proportional” form of the analogical argument that equated the relation of human selection to the development of domestic breeds as an argument of the basic form: human selection is to domestic variety formation as natural selection is to natural species formation (White, Hodge and Radick 2021, chps. 4–5). This makes a direct analogy between the actions of nature with those of humans in the process of selection. The specific expressions, and changes, in this analogy are important to follow closely. As this was expressed in the first coherent draft of the theory, a 39-page pencil manuscript written in 1842, this discussion analogized the concept of selection of forms by human agency in the creation of the varieties of domestic animals and plants, to the active selection in the natural world by an almost conscious agency, a “being infinitely more sagacious than man (not an omniscient creator)” who acts over “thousands and thousands of years” on “all the variations which tended towards certain ends” (Darwin 1842 in Glick and Kohn 1996, 91). This agency selects out those features most beneficial to organisms in relation to conditions of life, analogous in its action to the selection by man on domestic forms in the production of different breeds. Interwoven with these references to an almost Platonic demiurge are appeals to the selecting power of an active “Nature”:

Nature’s variation far less, but such selection far more rigid and scrutinizing […] Nature lets <<an>> animal live, till on actual proof it is found less able to do the required work to serve the desired end, man judges solely by his eye, and knows not whether nerves, muscles, arteries, are developed in proportion to the change of external form. (Ibid., 93)

These themes were continued in the 230 page draft of his theory of 1844. Again he referred to the selective action of a wise “Being with penetration sufficient to perceive differences in the outer and innermost organization quite imperceptible to man, and with forethought extending over future centuries to watch with unerring care and select for any object the offspring of an organism produced” (Darwin 1844 in ibid., 101). This selection was made with greater foresight and wisdom than human selection. As he envisions the working of this causal agency,

In accordance with the plan by which this universe seems governed by the Creator, let us consider whether there exist any secondary means in the economy of nature by which the process of selection could go on adapting, nicely and wonderfully, organisms, if in ever so small a degree plastic, to diverse ends. I believe such secondary means do exist. (Ibid., 103).

Darwin returned to these issues in 1856, following a twelve-year period in which he published his Geological Observations on the Volcanic Islands (1844), the second edition of his Journal of Researches (1845), Geological Observations on South America (1846), the four volumes on fossil and living barnacles (Cirripedia) (1851, 54, 55), and Geological Observations on Coral Reefs (1851). In addition, he published several smaller papers on invertebrate zoology and on geology, and reported on his experiments on the resistance of seeds to salt water, a topic that would be of importance in his explanation of the population of remote islands.

These intervening inquiries positioned Darwin to deal with the question of species permanence against an extensive empirical background. The initial major synthesis of these investigations takes place in his long manuscript, or “Big Species Book”, commenced in 1856, known in current scholarship as the “Natural Selection” manuscript. This formed the immediate background text behind the published Origin. Although incomplete, the “Natural Selection” manuscript provides insights into many critical issues in Darwin’s thinking. It was also prepared with an eye to the scholarly community. This distinguishes its content and presentation from that of the subsequent “abstract” which became the published Origin of Species. “Natural Selection” contained tables of data, references to scholarly literature, and other apparatus expected of a non-popular work, none of which appeared in the published Origin.

The “Natural Selection” manuscript also contained some new theoretical developments of relevance to the concept of natural selection that are not found in earlier manuscripts. Scholars have noted the introduction in this manuscript of the “principle of divergence”, the thesis that organisms under the action of natural selection will tend to radiate and diversify within their “conditions of life”—the contemporary name for the complex of environmental and species-interaction relationships (Kohn 1985b, 2009). Although the concept of group divergence under the action of natural selection might be seen as an implication of Darwin’s theory from his earliest formulations of the 1830s, nonetheless Darwin’s explicit definition of this as a “principle”, and its discussion in a long late insertion in the “Natural Selection” manuscript, suggests its importance for Darwin’s mature theory. The principle of divergence was now seen by Darwin to form an important link between natural variation and the conditions of existence under the action of the driving force of population increase.

Still evident in the “Natural Selection” manuscript is Darwin’s implicit appeal to some kind of teleological ordering of the process. The action of the masculine-gendered “wise being” of the earlier manuscripts, however, has now been given over entirely to the action of a selective “Nature”, now referred to in the traditional feminine gender. This Nature,

…cares not for mere external appearance; she may be said to scrutinise with a severe eye, every nerve, vessel & muscle; every habit, instinct, shade of constitution,—the whole machinery of the organisation. There will be here no caprice, no favouring: the good will be preserved & the bad rigidly destroyed.… Can we wonder then, that nature’s productions bear the stamp of a far higher perfection than man’s product by artificial selection. With nature the most gradual, steady, unerring, deep-sighted selection,—perfect adaption [sic] to the conditions of existence.… (Darwin 1856–58 [1974: 224–225])

The language of this passage, directly underlying statements about the action of “natural selection” in the first edition of the published Origin, indicates the complexity in the exegesis of Darwin’s meaning of “natural selection” when viewed in light of its historical genesis (Ospovat 1981). The parallels between art and nature, the intentionality implied in the term “selection”, the notion of “perfect” adaptation, and the substantive conception of “nature” as an agency working toward certain ends, all render Darwin’s views on teleological purpose more complex than they are typically interpreted from the standpoint of contemporary Neo-selectionist theory (Lennox 1993, 2013). As will be discussed below, the changes Darwin subsequently made in his formulations of this concept over the history of the Origin have led to different conceptions of what he meant by this principle.

The hurried preparation and publication of the Origin between the summer of 1858 and November of 1859 was prompted by the receipt on June 18 of 1858 of a letter and manuscript from Alfred Russel Wallace (1823–1913) that outlined his remarkably similar views on the possibility of continuous species change under the action of a selection upon natural variation (Wallace 1858 in Glick and Kohn 1996, 337–45). This event had important implications for the subsequent form of Darwin’s published argument. Rapidly condensing the detailed arguments of the unfinished “Natural Selection” manuscript into shorter chapters, Darwin also universalized several claims that he had only developed with reference to specific groups of organisms, or which he had applied only to more limited situations in the manuscript. This resulted in a presentation of his theory at the level of broad generalization. The absence of tables of data, detailed footnotes, and references to the secondary literature in the published version also resulted in predictable criticisms which will be discussed below in Section 3.2.

2.2. The Central Argument of the Published Origin
The Origin of Species by Means of Natural Selection, or the Preservaton of Favoured Races in the Struggle for Life was issued in London by the publishing house of John Murray on November 24, 1859 (Darwin 1859 [1964]). The structure of the argument presented in the published Origin has been the topic of considerable literature and can only be summarized here. Although Darwin himself described his book as “one long argument”, the exact nature of this argument is not immediately transparent, and alternative interpretations have been made of his reasoning and rhetorical strategies in formulating his evolutionary theory. (Prestes 2023; White, Hodge and Radick 2021; Hodge 2013b, 1977; Hoquet 2013; Hull 2009; Waters 2009; Depew 2009; Ruse 2009; Lennox 2005; Hodge 1983b).

The scholarly reconstruction of Darwin’s methodology employed in the Origin has taken two primary forms. One approach has been to reconstruct it from the standpoint of currently accepted models of scientific explanation, sometimes presenting it as a formal deductive model (Sober 1984). Another, more historical, approach interprets his methodology in the context of accepted canons of scientific explanation found in Victorian discussions of the period (see the entry on Darwinism; Prestes 2023; White, Hodge and Radick 2021; Hodge 2013b, 1983b, 1977; Hoquet 2013; Hull 2009; Waters 2009; Depew 2009; Lennox 2005). The degree to which Darwin did in fact draw from the available methodological discussions of his contemporaries—John Herschel, William Whewell, John Stuart Mill—is not fully clear from available documentary sources. The claim most readily documented, and defended particularly by White, Hodge and Radick (2021) and M. J. S. Hodge (1977, 1983a), has emphasized the importance of John Herschel’s A Preliminary Discourse on the Study of Natural Philosophy (1830 [1987]), which Darwin read as a young student at Cambridge prior to his departure on the HMS Beagle in December of 1831.

In Herschel’s text he would have encountered the claim that science seeks to determine “true causes”—vera causae—of phenomena through the satisfaction of explicit criteria of adequacy (Herschel, 1830 [1987], chp. 6). This concept Newton had specified in the Principia as the third of his “Rules of Reasoning in Philosophy” (see the entry on Newton’s philosophy, Section 4). Elucidation of such causes was to be the goal of scientific explanation. Vera causae, in Herschel’s formulation, were those necessary to produce the given effects; they were truly active in producing the effects; and they adequately explained these effects.

The other plausible methodological source for Darwin’s mature reasoning was the work of his older contemporary and former Cambridge mentor, the Rev. William Whewell (1794–1866), whose three-volume History of the Inductive Sciences (Whewell 1837) Darwin read with care after his return from his round-the-world voyage (Ruse 2013c, 1975). On this reading, a plausible argument has been made that the actual structure of Darwin’s text is more closely similar to a “Whewellian” model of argument. In Whewell’s accounts of his philosophy of scientific methodology (Whewell 1840, 1858), the emphasis of scientific inquiry is, as Herschel had also argued, to be placed on the discovery of “true causes”. But evidence for the determination of a vera causa was to be demonstrated by the ability of disparate phenomena to be drawn together under a single unifying “Conception of the Mind”, exemplified for Whewell by Newton’s universal law of gravitation. This “Consilience of Inductions”, as Whewell termed this process of theoretical unification under a few simple concepts, was achieved only by true scientific theories employing true causes (Whewell 1840: xxxix). It has therefore been argued that Darwin’s theory fundamentally produces this kind of consilience argument, and that his methodology is more properly aligned with that of Whewell.

A third account, related to the Whewellian reading, is that of David Depew. Building on Darwin’s claim that he was addressing “the general naturalist public,” Darwin is seen as developing what Depew has designated as “situated argumentation”, similar to the views developed by contemporary Oxford logician and rhetorical theorist Richard Whately (1787–1863) (Depew 2009). This rhetorical strategy proceeds by drawing the reader into Darwin’s world by personal narration as it presents a series of limited issues for acceptance in the first three chapters, none of which required of the reader a considerable leap of theoretical assent, and most of which, such as natural variation and Malthusian population increase, had already been recognized in some form in the literature of the period.

As Darwin presented his arguments to the public, he opens with a pair of chapters that draw upon the strong analogy developed in the manuscripts between the action of human art in the production of domestic forms, and the actions of selection “by nature.” The resultant forms are presumed to have arisen through the action of human selection on the slight variations existing between individuals within the same species. The interpretation of this process as implying directional, and even intentional, selection by a providential “Nature” that we have seen in the manuscripts was, however, downplayed in the published work through the importance given by Darwin to the role of “unconscious” selection, a concept not encountered in the Natural Selection manuscript. Such selection denotes the practice even carried out by aboriginal peoples who simply seek to maintain the integrity and survival of a breed or species by preserving the “best” forms.

The domestic breeding analogy is, however, more than a decorative rhetorical strategy. It repeatedly functions for Darwin as the principal empirical example to which he could appeal at several places in the text as a means of visualizing the working of natural selection in nature, and this appeal remains intact through the six editions of the Origin.

From this model of human selection working on small individual natural variations to produce the domestic forms, Darwin then developed in the second chapter the implications of “natural” variation, delaying discussion of the concept of natural selection until Chapter IV. The focus of the second chapter introduces another important issue. Here he extends the discussion of variation developed in Chapter I into a critical analysis of the common understanding of classification as grounded on the definition of species and higher groups based on the possession of essential defining properties. It is in this chapter that Darwin most explicitly develops his own position on the nature of organic species in relation to his theory of descent. It is also in this chapter that he sets forth the ingredients for his attack on one meaning of species “essentialism”.

Darwin’s analysis of the “species question” involves a complex argument that has many implications for how his work was read by his contemporaries and successors, and its interpretation has generated a considerable literature (see the entries on species and Darwinism; Mallet 2013; R. A. Richards 2010; Wilkins 2009; Stamos 2007; Sloan 2009b, 2013; Beatty 1985).

Prior tradition had been heavily affected by eighteenth-century French naturalist Buffon’s novel conception of organic species in which he made a sharp distinction between “natural” species, defined primarily by fertile interbreeding, and “artificial” species and varieties defined by morphological traits and measurements upon these (see the entry on evolutionary thought before Darwin, Section 3.3). This distinction was utilized selectively by Darwin in an unusual blending of two traditions of discussion that are conflated in creative ways in Darwin’s analysis.

Particularly as the conception of species had been discussed by German natural historians of the early nineteenth-century affected by distinctions introduced by philosopher Immanuel Kant (1724–1804), “Buffonian” species were defined by the material unity of common descent and reproductive continuity. This distinguished them by their historical and material character from the taxonomic species of the “Linnean” tradition of natural history. This distinction between “natural” and “logical” species had maintained a distinction between problems presented in the practical classification of preserved specimens, distinguished by external characters, and those relating to the unity of natural species, which was grounded upon reproductive unity and the sterility criterion (Sloan 2009b).

Remarkable in Darwin’s argument is the way in which he draws selectively in his readings from these two preexistent traditions to undermine the different grounds of species “realism” assumed within both of these traditions of discourse. One framework—what can be considered in his immediate context the “Linnean” tradition—regarded species in the sense of universals of logic or class concepts, whose “reality” was often grounded on the concept of divine creation. The alternative “Buffonian” tradition viewed species more naturalistically as material lineages of descent whose continuity was determined by some kind of immanent principle, such as the possession of a conserving “internal mold” or specifying vital force (see evolutionary thought before Darwin 3.3). The result in Darwin’s hands is a complex terminological interweaving of concepts of Variety, Race, Sub-species, Tribe, and Family that can be shown to be a fusion of different traditions of discussion in the literature of the period. This creative conflation also led to many confusions among his contemporaries about how Darwin actually did conceive of species and species change in time.

Darwin addresses the species question by raising the problems caused by natural variation in the practical discrimination of taxa at the species and varietal levels, an issue with which he had become closely familiar in his taxonomic revision of the Sub-class Cirripedia (barnacles) in his eight-year study on this group. Although the difficulty of taxonomic distinctions at this level was a well-recognized problem in the literature of the time, Darwin subtly transforms this practical problem into a metaphysical ambiguity—the fuzziness of formal taxonomic distinctions created by variation in preserved specimens is seen to imply a similar ambiguity of “natural” species boundaries.

We follow this in reading how natural variation is employed by Darwin in Chapter Two of the Origin to break down the distinction between species and varieties as these concepts were commonly employed in the practical taxonomic literature. The arbitrariness apparent in making distinctions, particularly in plants and invertebrates, meant that such species were only what “naturalists having sound judgment and wide experience” defined them to be (Origin 1859 [1964], 47). These arguments form the basis for claims by his contemporaries that Darwin was a species “nominalist”, who defined species only as conventional and convenient divisions of a continuum of individuals.

But this feature of Darwin’s discussion of species captures only in part the complexity of his argument. Drawing also on the tradition of species realism developed within the “Buffonian” tradition, Darwin also affirmed that species and varieties are defined by common descent and material relations of interbreeding. Darwin then employed the ambiguity of the distinction between species and varieties created by individual variation in practical taxonomy to undermine the ontological fixity of “natural” species. Varieties are not simply the formal taxonomic subdivisions of a natural species as conceived in the Linnaean tradition. They are, as he terms them, “incipient” species (ibid., 52). This subtly transformed the issue of local variation and adaptation to circumstances into a primary ingredient for historical evolutionary change. The full implications to be drawn from this argument were, however, only to be revealed in Chapter Four of the text.

Before assembling the ingredients of these first two chapters, Darwin then introduced in Chapter Three the concept of a “struggle for existence”. This concept is introduced in a “large and metaphorical sense” that included different levels of organic interactions, from direct struggle for food and space to the struggle for life of a plant in a desert. Although described as an application of Thomas Malthus’s parameter of geometrical increase of population in relation to the arithmetical increase of food supply, Darwin’s use of this concept in fact reinterprets Malthus’s principle, which was formulated only with reference to human population in relation to food supply. It now becomes a general principle governing all of organic life. Thus all organisms, including those comprising food for others, would be governed by the tendency to geometrical increase.

Through this universalization, the controls on population become only in the extreme case grounded directly on the traditional Malthusian limitations of food and space. Normal controls are instead exerted through a complex network of relationships of species acting one on another in predator-prey, parasite-host, and food-web relations. This profound revision of Malthus’s arguments rendered Darwin’s theory deeply “ecological” as this term would later be employed. We can cite two thought experiments employed by Darwin himself as illustrations (ibid., 72–74). The first concerns the explanation of the abundance of red clover in England. This Darwin sees as dependent on the numbers of pollinating humble bees, which are controlled in turn by the number of mice, and these are controlled by the number of cats, making cats the remote determinants of clover abundance. The second instance concerns the explanation of the abundance of Scotch Fir. In this example, the number of fir trees is limited indirectly by the number of cattle.

With the ingredients of the first three chapters in place, Darwin was positioned to assemble these together in his grand synthesis of Chapter Four on “natural” selection. In this long discussion, Darwin develops the main exposition of his central theoretical concept. For his contemporaries and for the subsequent tradition, however, the meaning of Darwin’s concept of “natural” selection was not unambiguously evident for reasons we have outlined above, and these unclarities were to be the source of several persistent lines of disagreement and controversy.

The complexities in Darwin’s presentation of his central principle over the six editions of the published Origin served historically to generate several different readings of his text. In the initial introduction of the principle of natural selection in the first edition of Darwin’s text, it is characterized as “preservation of favourable variations and the rejection of injurious variations” (ibid., 81). When Darwin elaborated on this concept in Chapter Four of the first edition, he continued to describe natural selection in language suggesting that it involved intentional selection, continuing the strong art-nature analogy found in the manuscripts. For example:

As man can produce and certainly has produced a great result by his methodical and unconscious means of selection, what may not nature effect? Man can act only on external and visible characters: nature cares nothing for appearances, except in so far as they may be useful to any being. She can act on every internal organ, on every shade of constitutional difference, on the whole machinery of life. Man selects only for his own good; Nature only for that of the being which she tends. Every selected character is fully exercised by her; and the being is placed under well-suited conditions of life. (Ibid., 83)

The manuscript history behind such passages prevents the simple discounting of these statements as mere rhetorical imagery. As we have seen, the parallel between intentional human selectivity and that of “nature” formed the proportional analogical model upon which the concept of natural selection was originally constructed.

Criticisms that quickly developed over the overt intentionality embedded in such passages, however, led Darwin to revise the argument in editions beginning with the third edition of 1861. From this point onward he explicitly downplayed the intentional and teleological language of the first two editions, denying that his appeals to the selective role of “nature” were anything more than a literary figure. Darwin then moved decisively in the direction of defining natural selection as the description of the action of natural laws working upon organisms rather than as an efficient or final cause of life. He also regrets in his Correspondence his mistake in not utilizing the designation “natural preservation” rather than “natural selection” to characterize his principle (letter to Lyell 28 September 1860, Burkhardt Correspondence 8, 397; also see Darwin Correspondence Project in Other Internet Resources). In response to criticisms of Alfred Russel Wallace, Darwin then adopted in the fifth edition of 1869 his contemporary (1820–1903) Herbert Spencer’s designator, “survival of the fittest”, as a synonym for “natural selection” (Spencer 1864, 444–45; Darwin 1869, 72). This redefinition further shifted the meaning of natural selection away from the concept that can be extracted from the early texts and drafts. These final statements of the late 1860s and early 70s underlie the tradition of later “mechanistic” and non-teleological understandings of natural selection, a reading developed by his disciples who, in the words of David Depew, “had little use for either his natural theodicy or his image of a benignly scrutinizing selection” (Depew 2009, 253). The degree to which this change preserved the original strong analogy between art and nature can, however, be questioned. Critics of the use of this analogy had argued since the original formulations that the comparison of the two modes of selection actually worked against Darwin’s theory (Wallace 1858 in Glick and Kohn 1997, 343). This critique would also be leveled against Darwin in the critical review of 1867 by Henry Fleeming Jenkin discussed below.

The conceptual synthesis of Chapter Four also introduced discussions of such matters as the conditions under which natural selection most optimally worked, the role of isolation, the causes of the extinction of species, and the principle of divergence. Many of these points were made through the imaginative use of “thought experiments” in which Darwin constructed possible scenarios through which natural selection could bring about substantial change.

One prominent way Darwin captured for the reader the complexity of this process is reflected in the single diagram to appear in all the editions of the Origin. In this illustration, originally located as an Appendix to the first edition, but thereafter moved into Chapter Four, Darwin summarized his conception of how species were formed and diverged from common ancestral points. This image also served to depict the frequent extinction of most lineages, an issue developed in detail in Chapter Ten. It displayed pictorially the principle of divergence, illustrating the general tendency of populations to diverge and fragment under the pressure of population increase. It supplied a way of envisioning relations of taxonomic affinity to time, and illstrated the persistence of some forms unchanged over long geological periods in which stable conditions prevail.

Graph labeled on the horizontal-axis with the letters A to L and on the vertical-axis with Roman numerals I to XIV. From A branch up several dashed lines; all but two stop before reaching vertical-level I; from those two branch up several more dashed lines, some stop before the next vertical-level those that don't sprout up more lines, repeat though in some cases no line from a particular branch reaches the next vertical-level. Further description in the text following.
Figure: Tree of life diagram from Origin of Species (Origin 1859:“Appendix”.

Remarkable about Darwin’s diagram of the tree of life is the relativity of its coordinates. It is first presented as applying only to the divergences taking place in taxa at the species level, with varieties represented by the small lower-case letters within species A–L of a “wide ranging genus”, with the horizontal lines representing time segments measured in terms of a limited number of generations. However, the attentive reader could quickly see that Darwin’s destructive analysis of the distinction between “natural” and “artificial” species in Chapter Two, implied the relativity of the species-variety distinction, this diagram could represent eventually all organic relationships, from those at the non-controversial level of diverging varieties within fixed species, to those of the relations of Species within different genera. Letters A–L could also represent taxa at the level of genera, families or orders. The diagram can thus be applied to relationships between all levels of the Linnaean hierarchy with the time segments representing potentially vast expanses of time, and the horizontal spread of branches the degree of taxonomic divergence over time. In a very few pages of argument, the diagram was generalized to represent the most extensive group relations, encompassing the whole of geological time. Extension of the dotted lines at the bottom could even suggest, as Darwin argues in the last paragraph of the Origin, that all life was a result of “several powers, having been originally breathed into a few forms or into one” (Darwin 1859 [1964], 490). This could suggest a single naturalistic origin of all original forms either by material emergence, or through the action of a vitalistic power of life. Darwin’s use of Biblical language could also be read as allowing for the action of a supernatural cause.

In response to criticisms concerning this latter point, Darwin quickly added to the final paragraph in the second edition of 1860 the phrase “by the Creator” (1860: 484), which remained in all subsequent editions. as did the quotations on the frontispiece from familiar discussions in British natural theology concerning creation by secondary causation. Conceptual space was thereby created for the reading of the Origin by some contemporaries, notably by the Harvard botanist Asa Gray (1810–88), as compatible with traditional natural theology (Gray 1860).

The sweep of the theoretical generalization that closed the natural selection chapter, one restated even more generally in the final paragraph of the book, required Darwin to deal with several obvious objections to the theory that constitute the main “defensive” chapters of the Origin (Five–Nine), and occupy him through the numerous revisions of the text between 1859 and 1872. As suggested by David Depew, the rhetorical structure of the original text developed in an almost “objections and response” structure that resulted in a constant stream of revisions to various editions of the original text as Darwin engaged his opponents (Depew 2009; Peckham 2006). Anticipating at first publication several obvious lines of objection, Darwin devoted much of the text of the original Origin to offering a solution in advance to predictable difficulties. As Darwin outlined these main lines of objection, he discussed, first, the apparent absence of numerous slight gradations between species, both in the present and in the fossil record, of the kind that would seem to be predictable from the gradualist workings of the theory (Chps. Six, Nine). Second, the gradual development of organs and structures of extreme complexity, such as the vertebrate eye, an organ which had since Antiquity served as a mainstay of the argument for external teleological design (Chp. Six). Third, the evolution of the elaborate instincts of animals and the puzzling problem of the evolution of social insects that developed sterile neuter castes, proved to be a particularly difficult issue for Darwin in the manuscript phase of his work and needed some account (Chp. Seven). As a fourth major issue needing attention, the traditional distinction between natural species defined by interfertility, and artificial species defined by morphological differences, required an additional chapter of analysis in which he sought to undermine the absolute character of the interbreeding criterion as a sign of fixed natural species (Chp. Eight).

In Chapter Ten, Darwin developed his interpretation of the fossil record. At issue was the claim by Lamarckian and other transformists, as well as Cuvierian catastrophists such as William Buckland (1784–1856) (see the entry on evolutionary thought before Darwin, Section 4.1), that the fossil record displayed a historical sequence beginning with simpler plants and animals, arriving either by transformism or replacement, at the appearance of more complex forms in geological history. Opposition to this thesis of “geological progressionism” had been made by none other than Darwin’s great mentor in geology, Charles Lyell in his Principles of Geology (Lyell 1832 [1990], vol. 2, chp. xi; Desmond 1984; Bowler 1976). Darwin defended the progressionist view against Lyell’s arguments in this chapter.

To each of the lines of objection to his theory, Darwin offered his contemporaries plausible replies. Additional arguments were worked out through the insertion of numerous textual insertions over the five revisions of the Origin between 1860 and 1872, including the addition of a new chapter to the sixth edition dealing with “miscellaneous” objections, responding primarily to the criticisms of St. George Jackson Mivart (1827–1900) developed in his Genesis of Species (Mivart 1871).

For reasons related both to the condensed and summary form of public presentation, and also as a reflection of the bold conceptual sweep of the theory, the primary argument of the Origin could not gain its force from the data presented by the book itself. Instead, it presented an argument from unifying simplicity, gaining its force and achieving assent from the ability of Darwin’s theory to draw together in its final synthesizing chapters (Ten–Thirteen) a wide variety of issues in taxonomy, comparative anatomy, paleontology, biogeography, and embryology under the simple principles worked out in the first four chapters. This “consilience” argument might be seen as the best reflection of the impact of William Whewell’s methodology (see above).

As Darwin envisioned the issue, with the acceptance of his theory, “a grand untrodden field of inquiry will be opened” in natural history. The long-standing issues of species origins, if not the explanation of the ultimate origins of life, as well as the causes of their extinction, had been brought within the domain of naturalistic explanation. It is in this context that he makes the sole reference in the text to the claim that “light will be thrown on the origin of man and his history”. And in a statement that will foreshadow the important issues of the Descent of Man of 1871, he speaks of how “Psychology will be based on a new foundation, that of the necessary acquirement of each mental power and capacity by gradation” (ibid., 488)

3. The Reception of the Origin
3.1 The Popular Reception of Darwin’s Theory
The broad sweep of Darwin’s claims, the brevity of the empirical evidence actually supplied in the Origin, and the implications of his theory for several more general philosophical and theological issues, opened up a controversy over Darwinian evolution that has waxed and waned over more than 160 years. The theory was inserted into a complex set of different national and cultural receptions the study of which currently forms a scholarly industry in its own right. European, Latin American and Anglophone receptions have been most deeply studied (Bowler 2013a; Gayon 2013; Largent 2013; Glick 1988, 2013; Glick and Shaffer 2014; Engels and Glick 2008; Gliboff 2008; Numbers 1998; Pancaldi, 1991; Todes 1989; Kelly 1981; Hull 1973; Mullen 1964). To these have been added analyses of non-Western recptions (Jin 2020, 2019 a,b; Yang 2013; Shen 2016; Elshakry 2013; Pusey 1983). These analyses display common patterns in both Western and non-Western readings of Darwin’s theory, in which these receptions were conditioned, if not determined, by the pre-existing intellectual, scientific, religious, social, and political contexts into which his works were inserted.

In the anglophone world, Darwin’s theory fell into a complex social environment that in the United States meant into the pre-Civil War slavery debates (Largent 2013; Numbers 1998). In the United Kingdom it was issued against the massive industrial expansion of mid-Victorian society, and the development of professionalized science. To restrict focus to aspects of the British reading public context, the pre-existing popularity of the anonymous Vestiges of the Natural History of Creation of 1844, which had reached 11 editions and sold 23,350 copies by December of 1860 (Secord “Introduction” to Chambers 1844 [1994], xxvii]), with more editions to appear by the end of the century, certainly prepared the groundwork for the general notion of the evolutionary origins of species by the working of secondary natural laws. The Vestiges’s grand schema of a teleological development of life, from the earliest beginnings of the solar system in a gaseous nebula to the emergence of humanity under the action of a great “law of development”, had also been popularized for Victorian readers by Alfred Lord Tennyson’s epic poem In Memoriam (1850). This Vestiges backdrop provided a context in which some could read Darwin as supplying additional support for the belief in an optimistic historical development of life under teleological guidance of secondary laws with the promise of ultimate historical redemption. Such readings also rendered the Origin seemingly compatible with the progressive evolutionism of Darwin’s contemporary Herbert Spencer (see the entry on Herbert Spencer). Because of these similarities, Spencer’s writings served as an important vehicle by which Darwin’s views, modified to fit the progressivist views expounded by Spencer, were first introduced in non-Western contexts (Jin 2020, 2019 a,b; Lightman [ed.] 2015; Pusey 1983). Such popular receptions ignored or revised Darwin’s concept of evolution by natural selection to fit these progressivist alternatives.

Outside the United Kingdom, the receptions of Darwin’s work display the importance of local context and pre-existent intellectual and social conditions. Three examples—France, Germany, and China—can be elaborated upon. In France, Darwin’s theory was received against the background of the prior debates over transformism of the 1830s that pitted the theories of Lamarck and Etienne Geoffroy St. Hilaire against Cuvier (Gayon 2013; entry on evolutionary thought before Darwin, 4.1). At least within official French Academic science, these debates had been resolved generally in favor of Cuvier’s anti-transformism. The intellectual framework provided by the “positive philosophy” of Auguste Comte (1798–1857) also worked both for and against Darwin. On one hand, Comte’s emphasis on the historical progress of science over superstition and metaphysics allowed Darwin to be summoned in support of a theory of the progress of science. The Origin was so interpreted in the preface to the first French translation of the Origin made by Clémence Royer (Harvey 2008). On the other hand, the Comtean three stages view of history, with its claim of the historical transcendence of speculative and metaphysical periods of science by a final period of experimental science governed by determinate laws, placed Darwinism in a metaphysical phase of speculative nature philosophy. This view is captured by the assessment of the leading physiologist and methodologist of French Science, Claude Bernard (1813–78). As he stated this in his 1865 treatise on scientific methodology, Darwin’s theory was to be regarded with those of “a Goethe, an Oken, a Carus, a Geoffroy Saint Hilaire”, locating it within speculative philosophy of nature rather than granting it the status of “positive” science (Bernard 1865 [1957], 91–92]).

In the Germanies, Darwin’s work entered a complex social, intellectual and political situation in the wake of the failed efforts to establish a liberal democracy in 1848. It also entered an intellectual culture strongly influenced by the pre-existent philosophical traditions of Kant, Schelling’s Naturphilosophie, German Romanticism, and the Idealism of Fichte and Hegel (R. J. Richards 2002, 2008, 2013; Gliboff 2007, 2008; Mullen 1964). These factors formed a complex political and philosophical environment into which Darwin’s developmental view of nature and theory of the transformation of species was quickly assimilated, if also altered. Many readings of Darwin consequently interpreted his arguments against the background of Schelling’s philosophy of nature. The marshalling of Darwin’s authority in debates over scientific materialism were also brought to the fore by the enthusiastic advocacy of Darwinism in Germany by University of Jena professor of zoology Ernst Heinrich Haeckel (1834–1919). More than any other individual, Haeckel made Darwinismus a major player in the polarized political and religious disputes of Bismarckian Germany (R. J. Richards 2008). Through his polemical writings, such as the Natural History of Creation (1868), Anthropogeny (1874), and Riddle of the Universe (1895–99), Haeckel advocated a materialist monism in the name of Darwin, and used this as a stick with which to beat traditional religion. Much of the historical conflict between religious communities and evolutionary biology can be traced back to Haeckel’s polemical writings, which went through numerous editions and translations, including several English and American editions that appeared into the early decades of the twentieth century.

To turn to a very different context, that of China, Darwin’s works entered Chinese discussions by a curious route. The initial discussions of Darwinian theory were generated by the translation of Thomas Henry Huxley’s 1893 Romanes Lecture “Evolution and Ethics” by the naval science scholar Yan Fu (1854–1921), who had encountered Darwinism while being educated at the Royal Naval College in Greenwich from 1877 to 1879. This translation of Huxley’s lecture, published in 1898 under the name of Tianyan Lun, was accompanied with an extensive commentary by Yan Fu that drew heavily upon the writings of Herbert Spencer which Yan Fu placed in opposition to the arguments of Huxley. This work has been shown to have been the main vehicle by which the Chinese learned indirectly of Darwin’s theory (Jin 2020, 2019 a, b; Yang 2013; Pusey 1983). In the interpretation of Yan Fu and his allies, such as Kan Yuwei (1858–1927), Darwinism was given a progressivist interpretation in line with aspects of Confucianism.

Beginning in 1902, a second phase of Darwinian reception began with a partial translation of the first five chapters of the sixth edition of the Origin by the Chinese scientist, trained in chemistry and metallurgy in Japan and Germany, Ma Junwu (1881–1940). This partial translation, published between 1902 and 1906, again modified the text itself to agree with the progressive evolutionism of Spencer and with the progressivism already encountered in Yan Fu’s popular Tianyan Lun. Only in September of 1920 did the Chinese have Ma Junwu’s full translation of Darwin’s sixth edition. This late translation presented a more faithful rendering of Darwin’s text, including an accurate translation of Darwin’s final views on natural selection (Jin 2019 a, b). As a political reformer and close associate of democratic reformer Sun Yat-Sen (1866–1925), Ma Junwu’s interest in translating Darwin was also was involved with his interest in revolutionary Chinese politics (Jin 2019a, 2022).

3.2 The Professional Reception of Darwin’s Theory
The reception of the Origin by those who held positions of professional research and teaching positions in universities, leadership positions in scientific societies, and employment in museums, was complex. These individuals were typically familiar with the empirical evidence and the technical scientific issues under debate in the 1860s in geology, comparative anatomy, embryology, biogeography, and classification theory. This group can usually be distinguished from lay interpreters who may not have made distinctions between the views of Lamarck, Chambers, Schelling, Spencer, and Darwin on the historical development of life.

If we concentrate attention on the reception by these professionals, Darwin’s work received varied endorsement (Hull 1973). Many prominent members of Darwin’s immediate intellectual circle—Adam Sedgwick, William Whewell, Charles Lyell, Richard Owen, and Thomas Huxley—had previously been highly critical of Chambers’s Vestiges in the 1840s for its speculative character and its scientific incompetence (Secord 2000). Darwin himself feared a similar reception, and he recognized the substantial challenge facing him in convincing this group and the larger community of scientific specialists with which he interacted and corresponded widely. With this group he was only partially successful.

Historical studies have revealed that only rarely did members of the scientific elites accept and develop Darwin’s theories exactly as they were presented in his texts. Statistical studies on the reception by the scientific community in England in the first decade after the publication of the Origin have shown a complicated picture in which there was neither wide-spread conversion of the scientific community to Darwin’s views, nor a clear generational stratification between younger converts and older resisters, counter to Darwin’s own predictions in the final chapter of the Origin (Hull et al. 1978). These studies also reveal a distinct willingness within the scientific community to separate acceptance of Darwin’s more general claim of species descent with modification from common ancestors from the endorsement of his explanation of this descent through the action of natural selection on slight morphological variations.

Of central importance in analyzing this complex professional reception was the role assigned by Darwin to the importance of normal individual variation as the source of evolutionary novelty. As we have seen, Darwin had relied on the novel claim that small individual variations—the kind of differences considered by an earlier tradition as merely “accidental”—formed the raw material upon which, by cumulative directional change under the action of natural selection, major changes could be produced sufficient to explain the origin and subsequent differences in all the various forms of life over time. Darwin, however, left the specific causes of this variation unspecified beyond some effect of the environment on the sexual organs. Variation was presented in the Origin with the statement that “the laws governing inheritance are quite unknown” (Darwin 1859 [1964], 13). In keeping with his commitment to the gradualism of Lyellian geology, Darwin also rejected the role of major “sports” or other sources of discontinuous change in this process.

As critics focused their attacks on the claim that such micro-differences between individuals could be accumulated over time without natural limits, Darwin began a series of modifications and revisions of the theory through a back and forth dialogue with his critics that can be followed in his revisions to the text of the Origin. In the fourth edition of 1866, for example, Darwin inserted the claim that the continuous gradualism depicted by his branching diagram was misleading, and that transformative change does not necessarily go on continuously. “It is far more probable that each form remains for long periods unaltered, and then again undergoes modification” (Darwin 1866, 132; Peckham 2006, 213). This change-stasis-change model allowed variation to stabilize for a period of time around a mean value from which additional change could then resume. Such a model would, however, presumably require even more time for its working than the multi-millions of years assumed in the original presentation of the theory.

The difficulties in Darwin’s arguments that had emerged by 1866 were highlighted in a lengthy and telling critique in 1867 by the Scottish engineer Henry Fleeming Jenkin (1833–1885) (typically Fleeming Jenkin). Using an argument previously raised in the 1830s by Charles Lyell against Lamarck, Fleeming Jenkin cited empirical evidence from domestic breeding that suggested a distinct limitation on the degree to which normal variation could be added upon by selection (Fleeming Jenkin 1867 in Hull 1973). Using a loosely mathematical argument, Fleeming Jenkin argued that the effects of intercrossing would continuously swamp deviations from the mean values of characters and result in a tendency of the variation in a population to return to mean values over time. It is also argued that domestic evidence does not warrant an argument for species change. For Fleeming Jenkin, Darwin’s reliance on continuous additive deviation was presumed to be undermined by these arguments, and only more dramatic and discontinuous change—something Darwin explicitly rejected—could account for the origin of new species.

Fleeming Jenkin also argued that the time needed by Darwin’s theory to account for the history of life under the gradual working of natural selection was simply unavailable from scientific evidence, supporting this claim by an appeal to the physical calculations of the probable age of the solar system presented in publications by his mentor, the Glasgow physicist William Thompson (Lord Kelvin, 1824–1907) (Burchfield 1975). On the basis of Thompson’s quantitative physical arguments concerning the age of the sun and solar system, Fleeming Jenkin judged the time since the presumed first beginnings of life to be insufficient for the Darwinian gradualist theory of species transformation to have taken place.

Jenkin’s multi-pronged argument gave Darwin considerable difficulties and set the stage for more detailed empirical inquiries into variation and its causes by Darwin’s successors. The time difficulties were only resolved in the twentieth-century with the discovery of radioactivity that could explain why the sun did not lose heat in accord with Newtonian principles.

As a solution to the variation question, Darwin developed his “provisional hypothesis” of pangenesis, which he presented the year after the appearance of the Fleeming Jenkin review in his two-volume Variation of Plants and Animals Under Domestication (Darwin 1868; Olby 2013). Although this theory had been formulated independently of the Jenkin review (Olby 1963), in effect it functioned as Darwin’s reply to Jenkin’s critique. The pangenesis theory offered a causal theory of variation and inheritance through a return to a theory resembling Buffon’s theory of the organic molecules proposed in the previous century (see entry on evolutionary thought before Darwin section 3.2). Invisible material “gemmules” were presumed to exist within the cells. According to theory, these were subject to external alteration by the environment and other external causes. The gemmules were then shed continually into the blood stream (the “transport” hypothesis) and assembled by “mutual affinity for each other, leading to their aggregation either into buds or into the sexual elements” (Darwin 1868, vol. 2, 375). In this form they were then transmitted—the details were not explained—by sexual generation to the next generation to form the new organism out of “the modified physiological units of which the organism is built” (ibid., 377). In Darwin’s view, this hypothesis united together numerous issues into a coherent and causal theory of inheritance and explained the basis of variation. It also explained how use-disuse inheritance, a theory which Darwin never abandoned, could work.

The pangenesis theory, although not specifically referred to, seems to be behind an important distinction Darwin inserted into the fifth edition of the Origin of 1869 in his direct reply to the criticisms of Jenkin. In this textual revision, Darwin distinguished “certain variations, which no one would rank as mere individual differences”, from ordinary variations (Darwin1869, 105; Peckham 2006, 178–179). This revision shifted Darwin’s emphasis away from his early reliance on normal slight individual variation, and gave new status to what he now termed “strongly marked” variations. The latter were now the forms of variation to be given primary evolutionary significance. Presumably this strong variation was more likely to be transmitted to the offspring, although details are left unclear, and in this form major variation could presumably be maintained in a population against the tendency to swamping by intercrossing as Fleeming Jenkin had argued.

Darwin’s struggles over this issue defined a set of problems that British life scientists in particular were to deal with into the 1930s. These debates over the role of somatic variation in the evolutionary process placed Darwinism in a defensive posture that forced its supporters into major revisions in the Darwinian research program (Gayon 1998; Vorzimmer 1970). The consequence was a complex period of Darwinian history in which natural selection theory was rejected by many research, or defended in modified form by others (Bowler 1983, 2013a; Largent 2009).

4. Human Evolution and the Descent of Man
4.1 The Genesis of Darwin’s Descent
Darwin had retained his own conclusions on human evolution quietly in the background through the 1860’s while the defense of his general theory was conducted by advocates as diverse as Thomas Henry Huxley (1825–95) in England, Asa Gray (1810–88) in the United States, and Ernst Haeckel (1834–1919) in the emerging new Germany. Darwin’s own position on the “human question” remained unclear to the reading public, and his rhetorical situating of the Origin within a tradition of divine creation by secondary law, captured in the frontispiece quotations from William Whewell and Francis Bacon, allowed many before 1871 to see Darwin as more open to religious interpretations of human origins than those of some of his popularizers.

Darwin’s interest in developing his insights into the origins of human beings and the explanation of human properties through descent with modification was, however, evident in his correspondence as early as January of 1860 when he began collecting evidence on the expressions of the emotions in human beings (Browne 2002, chp. 9). He then developed a questionnaire specifically intended to gain such information from contacts in Patagonia and Tierra del Fuego (Radick 2018). Further engagement with these issues was then generated by the discussions of Lyell (1863) and A. R. Wallace (1864), both of whom suggested that natural selection could not account for the development of the “higher” rational faculties, language, and ethical motivation (R. J. Richards 1987, chp. 4). It was then in February of 1867 that Darwin decided to remove material from his massive manuscript of the Variation of Plants and Animals Under Domestication to create a “very small volume, ‘an essay on the origin of mankind’” (Darwin to Hooker, 8 February 1867 and CD to Turner, 11 February 1867, Burkhardt, Correspondence 15: 74, 80). At this time he also sent to several international correspondents a more detailed questionnaire asking for information on human emotional expression. Further impetus to develop his views was created by the arguments of William R. Greg (1809–1881) in an essay in Fraser’s Magazine (1868), with further support by arguments of A. R. Wallace in 1869, both of whom drew a sharp distinction between human properties and those of animals (R. J. Richards 1987, 172–184). These arguments denied that natural selection could explain the origins of these “higher powers”.

Darwin’s drafting of his views on human issues, begun in early 1868, expanded into a major enterprise in which he became deeply engaged with the issue of the implications of his theory for ethics. The result of this effort devoted to anthropological topics was two separate works: the Descent of Man and Selection in Relation to Sex, delivered to the publisher in June of 1870 with publication in 1871, and its companion, Expression of the Emotions in Man and Animals, which he commenced in early 1871 with publication in early 1872.

As commentators have noted, these two works differ markedly in their arguments, and reflect different relationships to Darwin’s causal theories of natural and sexual selection, with sexual selection predominting over natural selection for the major portion of the Descent, and both of these causal theories generally missing from the descriptive approach of the Expression (Radick 2018).

Sexual selection—the choosing of females by males or vice versa for breeding purposes—had received a general statement by Darwin in Chapter IV of the Origin, but this played only a minor role in the original argument, and its importance was denied by co-evolutionist A. R. Wallace. In the Descent this was now developed in extensive detail as a major factor in evolution that could even work against ordinary natural selection. Sexual selection could be marshaled to explain sexual dimorphism, and also the presence of unusual characters and properties of organisms—elaborate feeding organs, bright colors, and other seemingly maladaptive structures such as the antlers on the Irish Elk or the great horn on the Rhinoceros beetle—that would appear anomalous outcomes of ordinary natural selection working for the optimal survival of organisms in nature. In a dramatic extension of the principle to human beings, the combination of natural and sexual selection is used to explain the origins of human beings from simian ancestors. It also accounts for the sexual dimorphism in humans, and is a major factor accounting for the origin of human races (E. Richards 2017; R. A. Richards 2013).

4.2 Darwin on Mental Powers
Although the secondary causal role of sexual selection in the development of species generally was to be the main topic of the bulk of the Descent, this plays an ambiguous role initially in the “treatise on man” that occupies the initial chapters, and functions differently in his treatment of the origins of mental powers, the moral sense, and the origin of races in this opening discussion.

In constructing this presentation, Darwin reaches back to the early Notebooks that he had separated out from the “transformist” discussions to deal with his inquiries into ethics, psychology, and emotions (see Section 1.2 above). Of particular importance for the opening discussions of the Descent was the “M” notebook, commenced in July of 1838, and “N”, begun in October of that year. On occasion he also samples the collection of entries now entitled “Old and Useless Notes”, generally written between 1838 and 1840.

The initial topic of focus in the Descent deals with the far-reaching issues concerning the status and origin of human mental properties, faculties presumed traditionally to be possessed uniquely by human beings. These properties Darwin now places on an evolutionary continuum with those features of animal behavior long regarded as instinctual. In this he placed himself in opposition to the long tradition of discourse that had distinguished humans from animals due to the possession of a “rational principle” related to their possession of a rational soul. This tradition had been given a more radical foundation in the revolutionary reflections on the relation of mind and body initiated by René Descartes (1596–1650) in the middle of the seventeenth century. Descartes deepened this distinction with the separation of the two substances—thinking substance, or res cogitans, possessed only by humans, and extended material substance, res extensa that constituted the rest of the natural world, including animals and plants, rendering animals only lifeless machines without rational faculties.

Darwin’s collapse of this Cartesian barrier with his theory of human origins outlined in the Descent continued a discussion that had been a concern of his transformist predecessors, especially Jean Baptiste Lamarck (Sloan 1999). But Darwin took this issue to a new level as he interpreted the human-animal relationship in the context of his novel theory of divergent evolution from common ancestors. Darwin also broke with the view of humans as the summit of a natural teleological process. Darwin instead denies such teleological ordering, and effectively reduces human properties to those of animals—mental as well as physical—by tracing them to their origin in properties of lower organisms.

The warrant for the identification of human and animal mental properties, however, is not supported by substantial argumentation in the Descent. The opening discussions of the treatise summarize the anatomical evidence for “homologies” —true identities—between humans and animals due to descent from common ancestors, claims already set out in Chapter Thirteen of the Origin. But the transferal of this identity of structure to inner non-anatomical “mental” properties rested on premises that are not made explicit in this text, and were not identities drawn by Huxley, Wallace and Lyell, for example, in their treatments of humans in relation to evolutionary theory, although they acknowledged the anatomical continuities.

To understand Darwin’s arguments, it is useful to return to his Notebook discussions on which he was drawing for his reasoning (see above, Section 1.2). In his “C” Notebook, opened in February of 1838, Darwin has a remarkable entry that displays very early on his commitment to a metaphysical “monism”—the thesis that there is only one substance underlying both mind and body. With this goes the thesis of a parallelism of the complexity of mental properties with those of material structure. In this entry in “C” following on Darwin’s reflections on the issue of instinct, and also recording some of his observations on animals at the Regents Park zoological gardens, Darwin comments:

There is one living spirit, prevalent over this wor[l]d, (subject to certain contingencies of organic matter & chiefly heat), which assumes a multitude of forms <<each having acting principle>> according to subordinate laws.—There is one thinking […] principle (intimately allied to one kind of organic matter—brain. & which <prin> thinking principle. seems to be given or assumed according to a more extended relations [sic] of the individuals, whereby choice with memory, or reason? is necessary.—) which is modified into endless forms, bearing a close relation in degree & kind to the endless forms of the living beings.— We see thus Unity in thinking and acting principle in the various shades of <dif> separation between those individuals thus endowed, & the community of mind, even in the tendency to delicate emotions between races, & recurrent habits in animals.— (Barrett 1987, 305)
As we follow these issues into the “M” Notebook, the assumption of a single “thinking principle,” allied to one kind of organic matter, seems then to underlie Darwin’s subsequent reflections on mind and matter. The “M” Notebook cites numerous “mental”properties common to humans and animals that generally parallel levels of material organization, similar to the identities expressed in the later Descent. The range of this universal extension of mental properties is far-reaching in these early discussions: consciousness and “free will” extends to all animals, including invertebrates:

With respect to free will, seeing a puppy playing cannot doubt that they have free will, if so all animals., then an oyster has & a polype (& a plant in some senses […]; now free will of oyster, one can fancy to be direct effect of organization, by the capacities its senses give it of pain or pleasure, if so free will is to mind, what chance is to matter […] (Barrett 1987, 536).

When these themes reappear in Chapter Two of the first edition of the Descent, Darwin seems to draw implicitly upon this matter-mind identity theory as an obvious consequence of his theory of descent from common ancestry. There he enumerates a long list of traditional human mental and emotional properties to claim that each of them are identities with the properties of simpler forms of life. The list is expansive: courage, deceit, play, kindness, maternal affection, self-complacency, pride, shame, sense of honor, wonder, dread, imitation, imagination, and dreaming. All are considered to be represented in a wide range of animals, with “play”and “recognition” found even in the ants.

When he addresses the more complex mental properties that specifically had been considered by a long tradition of discussion to be the distinctive human properties—possession of language, reason, abstract conceptual thinking, self-reflection—these again are treated as having their manifestations in other forms of life, with none of them unique to human beings. Language, the property that Descartes, for example, had considered to be the primary distinguishing character denoting the human possession of mind as distinct from matter, Darwin treats a developing in a gradual process from animal sounds that parallel the differentiation of species, illustrated by the fact that languages “like organic beings, can be classed in groups under groups” (Darwin 1871 [1981], 60). He closes his discussion of mental powers with an analysis of religious belief that derives it from imagination and belief in spirits found in aboriginal peoples. It can even be homologized with the “deep love of a dog for his master, associated with complete submissions, some fear, and perhaps other feelings” (ibid., 68). Darwin’s discussions of the relation of human and animal mental and emotional properties would set the agenda for a complex discussion that would carry into contemporary debates over animal cognition and the relations of human and animal properties (see the entries on animal cognition; methods in comparative cognition; and animal consciousness).

4.3 The Ethical Theory of the Descent of Man
The subsequent treatment of ethical issues in the third chapter of the Descent was for Darwin a topic to be approached “exclusively from the side of natural history” (ibid., 71). This issue also presented him with some of his most difficult conceptual problems (CD to Gray, 15 March 1870, Burkhardt, Correspondence 18, 68). In this discussion he also employs natural selection theory as an explanatory cause.

Under the heading of “Moral Sense”, Darwin offered some innovations in ethics that do not easily map on to standard ethical positions classified around the familiar categories of Rule or Act Utilitarianism, Kantian Deontology, Hedonism, and Emotivism. Darwin’s closest historical affinities are with the Scottish “Moral Sense” tradition of Frances Hutcheson (1694–1746), Adam Smith (1723?–1790), and David Hume (1711–1776). More immediately Darwin drew from the expositions of the moral sense theory by his distant relative, Sir James Macintosh (1765–1832) (R. J. Richards 1987, 114–122, 206–219).

Traditional moral sense theory linked ethical behavior to an innate property that was considered to be universal in human beings, although it required education and cultivation to reach its full expression (see the entry on moral sentimentalism). This inherent property, or “moral” sense, presumably explained such phenomena as ethical conscience, the sense of moral duty, and it accounted for altruistic actions that could not be reduced to hedonic seeking of pleasure and avoiding pain. It also did not involve the rational calculation of advantage, or the maximization of greatest happiness by an individual prior to action, as implied by Utilitarianism. For this reason Darwin criticized John Stuart Mill’s version of Utilitarian theory because it relied on acquired habits and the calculation of advantage (Darwin 1871 [1981], 71n5).

Darwin’s reinterpretation of the moral sense tradition within his evolutionary framework also implied important transfomations of this theory of ethics. The moral sense was not to be distinguished from animal instinct but was instead derived historically from the social instincts and developed by natural selection. From this perspective, Darwin could claim a genuine identity of ethical foundations holding between humans and animals, with the precursors of human ethical behavior found in the behavior of other animals, particularly those with social organization. Natural selection then shaped these ethical instincts in ways that favored group survival over immediate individual benefit (ibid., 98). Human ethical behavior is therefore grounded in a natural property developed by natural selection, with the consequence that ethical actions can occur without moral calculus or rational deliberation.

When moral conflict occurs, this is generally attributed to a conflict of instincts, with the stronger of two conflicting instincts favored by natural selection insofar as it favors group benefit (ibid. 84). In human beings the “more enduring Social Instincts” thus come to override the less persistent “individual” instincts.

The adequacy of evolutionary ethical naturalism as a foundation for ethical realism proved to be a point of contention for Darwin’s contemporaries and successors following the publication of the Descent. For some moral philosophers, Darwin had simply reduced ethics to a property subject to the relativizing tendencies of natural selection (Farber 1994: chp. 5). It was, in the view of Darwin’s philosophical critics, to reduce ethics to biology and in doing so, to offer no way to distinguish ethical goods from survival advantages. Not even for some strong supporters of Darwinism, such as Thomas Huxley and Alfred Russel Wallace, was Darwin’s account adequate (ibid., chp. 4). Much of subsequent development of moral philosophy after Darwin would be grounded upon the canonical acceptance of the “is-ought” distinction, which emerged with new force from the critique of “evolutionary” ethical theory. This critique began with Thomas Huxley’s own break with Darwinian ethical theory in his Romanes Lecture, “Evolution and Ethics”of 1893 (Huxley 1893). This lecture, reflecting Huxley’s views eleven years after Darwin’s death, would play an important role in the Chinese reception of Darwinism (Huxley 1895; see above, section 3.1). This line of critique also received an influential academic expression in G. E. Moore’s (1873–1958) Principia Ethica—itself an attack on Spencer’s version of evolutionary ethics (Moore 1903). Debates over the adequacy of evolutionary ethics continue into the present (see the entries on biological altruism and morality and evolutionary biology; see also, R. J. Richards 2015, 2009, 1999, 1987, Appendix 2; Charmetant 2013; Boniolo and DeAnna (eds.) 2006; Hauser 2006; Katz (ed.) 2000; Maienschein and Ruse (eds.) 1999).

4.4 Reception of the Descent
The international reception of the Descent of Man and Expression of the Emotions is a topic in need of the kind of detailed studies that surround the historical impact of the Origin. These works presented the reading public after 1871 with a more radical and controversial Darwin than had been associated with the author of the popular Journal of Researches or even the Origin itself, and his anthropological works created a watershed in the public reception of Darwin’s views (Radick 2013). The Descent finally made public Darwin’s more radical conclusions about human origins, and seemed to many of his readers, even those previously sympathetic to the Origin, to throw Darwin’s authority behind materialist and anti-religious forces. Public knowledge of Darwin’s own conclusions on human evolution before 1871 had rested on the one vague sentence on the issue in the Origin itself. The Descent made public his more radical conclusions. Even though the question of human evolution had already been dealt with in part by Thomas Huxley in his Man’s Place in Nature of 1863 (Huxley 1863), and by Charles Lyell in the same year in his Geological Evidences of the Antiquity of Man (Lyell 1863), followed by Alfred Russel Wallace’s articles in 1864 and 1870 (Wallace 1864 and online), these authors had either not dealt with the full range of questions presented by the inclusion of human beings in the evolutionary process, or they had emphasized the moral and mental discontinuity between humans and animals. Only Ernst Heinrich Haeckel had drawn out a more general reductive conception of humanity from evolutionary theory and he had not ventured into the specific issues of ethics, social organization, the origins of human races, and the relation of human mental properties to those of animals, all of which are dealt with in the Descent. Darwin’s treatise presented, as one commentator has put it, “a closer resemblance to Darwin’s early naturalistic vision than anything else he ever published” (Durant 1985, 294).

Darwin’s extension of his theory to a range of questions traditionally discussed within philosophy, theology, and social and political theory, has shaped the more general history of Darwinism since the 1870s. It set the agenda for much of the development of psychology of the late nineteenth century (R. J. Richards 1987). It also hardened the opposition of many religiously-based communities against evolutionary theory, although here again, distinctions must be made between different communities (Ellegård 1990, chp. 14). Such opposition was not simply based upon the denial of the literal scriptural account of the origins of humankind, an issue that played out differently within the main religious denominations (Haught 2013; Finnegan 2013; Swetlitz 2013; Artigas, Glick, & Martinez 2006; Moore 1979). The more fundamental opposition was due to the denial of distinctions, other than those of degree, between fundamental human properties and those of animals.

Furthermore, the apparent denial of some kind of divine guidance in the processes behind human evolution and the non-teleological character of Darwin’s final formulations of the natural selection theory in the fifth and sixth editions of the Origin, hardened this opposition. His adoption from Herbert Spencer of designator “survival of the fittest” as a synonym for “natural selection” in the fifth edition of 1869 added to this growing opposition. As a consequence, the favorable readings that many influential religious thinkers—John Henry Newman (1801–1890) is a good example—had given to the original Origin, disappeared. The rhetoric of the Descent, with its conclusion that “man is descended from a hairy quadruped, furnished with a tail and pointed ears” (Darwin 1871 [1981], 389), presented to the public a different Darwin than many had associated with the popular seagoing naturalist.

The new opposition to Darwin is reflected in the many hostile reviews of the Descent to appear in the periodical press (R. J. Richards 1987, 219–230). Particularly at issue were Darwin’s accounts of the origin of ethical principles and intelletual powers, including language, self-reflection, abstract thinking and religious belief as derivations from animal properties (Anon. 1871)

The profound revolution in thought that Darwin created, however, was eventually recognized even by his one-time harsh critics. The once leading British comparative anatomist Richard Owen (1804–1892), who had long been estranged from Darwin since his harsh review of the Origin in 1860, nonetheless could comment on the occasion of Darwin’s burial in Westminster Abbey in a letter to Horace Walpole:

The great value of Darwin’s series of works, summarizing all the evidence of Embryology, Paleontology, & Physiology experimentally applied in producing Varieties of Species, is exemplified in the general acceptance by Biologists of the Secondary Law, by Evolution, of the ‘Origin of Species’ […] In this respect Charles Darwin stands to Biology in the relation which Copernicus stood to Astronomy. […] [Copernicus] knew not how the planets revolved around the sun. To know that required the successive labours of a Galileo, a Kepler and finally a Newton […] Meanwhile our British Copernicus of Biology merits the honour and the gratitude of the Empire, which is manifest by a Statue in Westminster Abbey. (Richard Owen to Horace Walpole, 5 November, 1882, Royal College of Surgeons of England Archives, MS0025/1/5/4).

The subsequent history of the debates surrounding Darwin’s achievement forms a complex story that involves much of the history of life science, as well as ethical theory, psychology, philosophy, theology and social theory since 1870. For a general summary of recent scholarship see Ruse 2013a and articles from this encyclopedia listed below.

5. Summary and Conclusion
This article has intended to give a historical overview of the specific nature of Darwinian theory, and outline the ways in which it differed from the theories of predecessors in the nineteenth century (see the entry evolution before Darwin). The eventual general consensus achieved by the middle of the twentieth century around the so-named “Synthetic” theory of evolution that would combine population genetics with a mathematical analysis of evolutionary change, has formed a successful research program for more than half a century (Smocovitis 1996; Mayr and Provine 1980; Provine 1971). This “synthesis” has been challenged in recent decades by the current movement known as evolutionary developmental theory, or “evo-devo”. This development represents in some important respects a return to presumably discarded traditions and lines of exploration of the nineteenth and early twentieth centuries which sought to link evolution with embryological development, and to a complex understanding of genetics, with re-examination of the effects of external conditions on inheritance (Gilbert 2015; Newman 2015; Laubichler and Maienschein 2007; Gissis and Jablonka 2011; Pigliucci and Müller 2010; Amundson 2005; Gilbert, Opitz and Raff 1996). Where these debates and revisions in evolutionary theory may lead in another fifty years is a matter of speculation (Gayon 2015 in Sloan, McKenny and Eggleson 2015).

More general philosophical issues associated with evolutionary theory—those surrounding natural teleology, ethics, the relation of evolutionary naturalism to the claims of religious traditions, the implications for the relation of human beings to the rest of the organic world—continue as issues of scholarly inquiry. The status of Darwin’s accounts of human mental powers and moral properties continue to be issues of philosophical debate. The adequacy of his reliance on sexual selection to explain sex and gender roles in human society form heated topics in some feminist scholarship. Such developments suggest that there are still substantial theoretical issues at stake that may alter the future understanding of evolutionary theory in important ways (Sloan, McKenny, & Eggleson [eds] 2015).

1. History
As noted in the separate entry, critical theory can be thought of narrowly or broadly. Thought of broadly, ‘critical theory’ picks out philosophical work which combines a moral-political conviction that human flourishing is presently blocked, and a methodological conviction that interrogation of prevailing norms, practices, and concepts is necessary (but perhaps insufficient) to remove this blockage. Thought of narrowly, ‘critical theory’ picks out a specific area of philosophy animated by these twin convictions – the work of philosophers associated with the Frankfurt Institut für Sozialforschung (Institute for Social Research). This group is often referred to as the ‘Frankfurt School’.

This entry will use ‘critical theory’ in the narrow sense. This entry will be further narrowed by focusing almost exclusively on the first generation of the Frankfurt School. For economy, I will use ‘critical theory’ as shorthand for the more unwieldy ‘first-generation critical theory’. Closely related figures – like Walter Benjamin (1892–1940) and Ernst Bloch (1885–1977) – will not be collected under this phrase, but explicitly picked out as and when they are relevant.

The Frankfurt School is often spoken of as having three generations. Core figures in the first generation include Max Horkheimer (1895–1973), Theodor Adorno (1903–1969), and Herbert Marcuse (1898–1979). Horkheimer, who would become a close collaborator with Adorno, took directorship of the Institute in 1930. The Institute was closed by the Nazis in 1933. Horkheimer was instrumental in relocating and re-starting the Institute, eventually settling in America in 1934. Adorno and Marcuse followed. The most famous of their works would be Dialectic of Enlightenment (Adorno & Horkheimer 1944 [2002]), One-Dimensional Man (Marcuse 1964), and Negative Dialectics (Adorno 1966 [2006]). As important as this American context – and exposure to American popular culture – would be to their aesthetics, so too was the German cultural milieu of the 1920s and 1930s. For example, before joining the Institute Adorno had studied musical composition with the atonal composer Alban Berg (later commemorated in Adorno’s Alban Berg: Master of the Smallest Link, 1968 [1991]), befriended philosopher and critic Siegfried Kracauer, and written musical criticism for the journal, Musikblätter des Anbruch (Claussen 2008: 52–56, 102–106, 152). Walter Benjamin’s ideas were broadly disseminated via his friendship with Adorno (for more on their relationship, including with respect to aesthetics, see Rosen 2004, Buck-Morss 1977 & 1991, Nicholsen 1997b). The ideas of Georg Lukács and Ernst Bloch were also influential on the first-generation critical theorists, as we will see.

The second and third generations of critical theory are roughly marked by the coming to prominence of Jürgen Habermas (1929–) and Axel Honneth (1949–), respectively. The three generations are not merely distinguished by their personnel. There are important theoretical differences. The Cambridge Habermas Lexicon begins its ‘Aesthetics’ entry (Duvenage 2019) with the statement that ‘Whereas aesthetics plays an important role among the thinkers of the first generation … this is not so for Habermas.’’ (For an opposing view of Habermas’ relationship to aesthetics, however, see Duvenage 2003, McMahon 2011, Mackin 2022.) Honneth seldom mentions aesthetics (though see Honneth 2007). While there are valuable remarks on aesthetics in the work of both, then, they do not obviously belong in a discussion of the characteristic shape of the interface between critical theory and aesthetics. Critical theory’s distinctive treatment of aesthetics is thus confined largely to the first generation.

2. Why Does Critical Theory Have an Aesthetics?
In 1937’s ‘Traditional and Critical Theory’ Horkheimer offers the following statement about the programme he and his colleagues pursue, and its moral urgency –

If activity governed by reason is proper to man, then existent social practice, which forms the individual’s life down to its least details, is inhuman, and this inhumanity affects everything that goes on in the society …. The goal at which the [critical theorist] aims, namely the rational state of society, is forced upon him by present distress … It is the task of the critical theoretician to reduce the tension between his own insight and oppressed humanity in whose service he thinks …. (Horkheimer 1937 [2002: 210, 217, 221])
It is not surprising that this approach produced books with titles like Eclipse of Reason (Horkheimer 1947), One-Dimensional Man (Marcuse 1964) or Prophets of Deceit (Lowenthal & Guterman 1949). But the emergence of books with titles like Aesthetic Theory (Adorno 1970 [2004]), or The Aesthetic Dimension (Marcuse 1978) is perhaps harder to anticipate. Given the nature of critical theory, and its urgent pursuit of concrete improvement in people’s material and social lives, one might wonder why critical theory has an aesthetics at all.

One answer is that ‘critical theory’ has no need for an aesthetics. Its practitioners simply happened to like to think about art and did so. It is certainly true that many of the members of the first generation had biographical connections to, or a pre-established interest in, the world of art. But critical theory has an aesthetics for theoretical, not biographical, reasons. To make these reasons clear we need to take a detour through the work of Karl Marx (1818–1883), and then Georg Lukács (1885–1971).

2.1 Commodity Fetishism
Critical theory is grounded in a very particular approach to Marx, which emphasizes capitalism’s power to determine the world of individual thought and experience (see Kautzer 2017). This approach rests on a keen interest in commodity fetishism, a phenomenon which Marx discusses relatively briefly in volume one of Capital:

A commodity is … a mysterious thing, simply because in it the social character of men’s labour appears to them as an objective character stamped upon the product of that labour; because the relation of the producers to the sum total of their own labour is presented to them as a social relation, existing not between themselves, but between the products of their labour. This is the reason why the products of labour become commodities, social things …. This I call the Fetishism which attaches itself to the products of labour, so soon as they are produced as commodities, and which is therefore inseparable from the production of commodities. (Marx 1867 [1932: 83])
For Marx, commodity fetishism is an effect produced by the objective structure of social exchange. The buying and selling of goods draws equivalences between the goods exchanged. But this equivalence is performed not by comparing the labour time expended on those goods, but by comparing what appear to be properties of the commodities themselves. Price indices are driven by social facts about exchange and production; but these social facts vanish from view, and the prices themselves appear to be properties of objects. (“This gold bar is worth £__, and thus equivalent in value to 1,000 coats, 2,000 bricks and so on” – not “The socially necessary labour time involved in gold mining is such that a gold bar’s exchange value is expressed as …”). Capitalism thus presents itself as natural. This conceals the fact that exchange value arises not from the nature of things, but rather from the way we organize our labour. In turn, this conceals the fact that the way we organize our labour is not natural but in fact open to change.

On Marx’s account commodity fetishism affects the way in which we experience, and therefore theorize about, the social world. It conceals the true nature of capitalism. This concealment can be understood as something like an optical illusion. Marx uses the instructive analogy of the appearance of air:

The recent scientific discovery, that the products of labour, so far as they are values, are but material expressions of the human labour … by no means, dissipates the mist through which the social character of labour appears to us to be an objective character of the products themselves …. [T]his fact appears to the producers … to be just as real and final, as the fact, that, after the discovery by science of the component gases of air, the atmosphere itself remained unaltered. (Marx 1867 [1932: 85–86])
While the appearance of air – or of commodities – is not altered by our coming into scientific knowledge of their true constitution, the fact remains that we can and have acquired this knowledge in spite of that appearance. Thus, for Marx all that is needed to ‘see through’ commodity fetishism is to employ reason to enquire into what lies beneath the apparent nature of capitalism.

2.2 Reification
Lukács’ most influential work, History and Class Consciousness (1923), extends and revises Marx’s analysis of commodity fetishism. Unlike Marx, Lukács does not see capitalism as merely presenting a misleading appearance (as an optical illusion might) but exerting an effect on our very way of thinking and perceiving altogether (as a hallucination or delusion might). Accordingly, Lukács held commodity fetishism to have a systemic and general influence on consciousness. He used the term ‘Verdinglichung’ or ‘reification’ to name the broader effects he saw as stemming from commodity fetishism. Reification covers not only thought about or adjacent to the economy, but thought in general:

In the process we witness, illuminatingly, how here, too, the contemplative nature of man under capitalism makes its appearance … all issues are subjected to an increasingly formal and standardised treatment and in which there is an ever-increasing remoteness from the qualitative and material essence of the ‘things’ to which bureaucratic activity pertains. (Lukács 1923 [1971a: 97–99])
The richness and unpredictability of the world is glossed over by a form of thought which presumes complete agreement between abstract concepts and particular things. And this narrowing of thought is replicated in a narrowing of experience. Reification as a form of thought is grounded in and enforced by a broader tendency towards standardization which capitalism carries with it, from commodification (the increasing tendency to convert goods into products produced for exchange in line with the demands of the market), to bureaucratization (the administration of the social world according to inflexible rules), and so on. The claim, then, is that under capitalism we come to conceive of and experience the world in terms of general categories.

This is a problem because these general categories are, it is claimed, overly restrictive. The world has properties (causal, moral, and functional) which are not captured by these categories, and hence are excluded from reified thought and reified experience. Facts which might undermine the present social order (the exploitation integral to capitalist production, for example) are excluded from thought and experience. This also has moral implications. General moral and political categories (like ‘citizen’, ‘human’, ‘worker’, and so on) will tend towards excluding persons who do not conform to the restrictive standards these categories come with.

The idea of reification – if we hope to overcome it, or indeed to explain our ability to perceive it – requires us to posit some counter-reifying force. For Lukács, it is the proletariat (working class) that are (potentially) this force. The proletariat need only realize the exploitative nature of capitalist production and recognize that they have the power to change this process. The proletariat are necessarily always on the brink of this realization, in Lukács’ view, as their working conditions are intrinsically counter-reifying. They themselves are the contingent human labour which is concealed by the apparently natural and necessary world of capitalism and commodities.

2.3 Critical Theory’s Account of Reification
While Lukács’ account of reification exerted great influence on the Frankfurt School (see Feenberg 2017, Kavoulakos 2017), the proletarian strand of his thought did not. Returning to Horkheimer’s seminal “Traditional and Critical Theory” we find the following remark:

But it must be added that even the situation of the proletariat is, in this society, no guarantee of correct knowledge. … Even to the proletariat the world superficially seems quite different than it really is (Horkheimer 1937 [2002: 213–214]).
This leaves critical theory in a difficult position. If reification is total, and counter-reifying forces are totally absent, then Horkheimer’s claim to be thinking in ‘service of’ those oppressed by capitalism is incoherent. Critical theory requires a counter-reifying force to explain how it can perceive and see beyond the epistemic errors produced by reification.

For Marx, commodity fetishism was a problem with the comprehension of labour, which could be solved either through reading more accurate literature about labour (hence the writing of Capital), or better acquainting oneself with the reality of the production process (hence Marx’s ability to write Capital). Lukács appears to identify revolutionary proletarian consciousness as the means to undo the effects of reification, although the mechanics of its formation are not entirely clear. For critical theory, by contrast, commodity fetishism and reification are not problems which can be overcome by modifying either the content of experience (by reading Capital, say) or the social position of the experiencer. One can learn as much as possible about the conditions of the working class, and be as close to the production process as one likes, but reification will still obtain.

For critical theory, reification is a problem with the very form of experience itself; and the solution to reification will come through processes which can engage and manipulate that form of experience. Thus, for critical theory reification is an aesthetic problem in its original and broadest sense: as concerned with the realm of sensible experience.

Critical theory’s task will be to find avenues which can disrupt the influence of reification and make the genuine state of things – and the genuine possibilities for change – open to thought and experience. Several avenues for producing this effect were explored, but chief among them was art, and aesthetic experience.

3. The Problem of Commodification
Lukács, whose early work had been so influential on Critical theory, later became much closer to Soviet orthodoxy, even to the extent that he repudiated his earlier work on reification. He also broke with his earlier, equally influential, work on aesthetics (like 1910’s Soul and Form). Here too Lukács moved into closer alignment with Soviet thought. The Soviet art movement known as Socialist realism explicitly thematized concrete social and political factors in art, in order to instil a correct understanding of them in the art appreciator. Lukács was not uncritical of the actual implementation of this movement, especially under Stalin (Lukács 1971b: 116–130), and nor does he deny the quality and power of the ‘critical realists’ who likewise include concrete historical detail, but without explicit socialist ideology (Lukács 1971b, chapters 2 and 3). But above all it is realism in opposition to the ‘decadent formalism’ (Lukács 1971b: 133–135) of modernism that Lukács urged as best serving the moral and political aims of Marxism.

Adorno, Marcuse, and Benjamin, by contrast, found their favourite examples of the liberating power of art in multiple schools and time periods, including modernism, nearly all of which refused any overt political content. They also openly questioned the efficacy and quality of much political art (see, e.g., Marcuse 1978: 19; 33–39, Schoolman 1976: 58–60, Klassen & Blumenfeld 2018, Adorno 1961 [1977a], 1977b). This rejection was seen by many, including the later Lukács, as evidence of elitism, political disengagement, and immoral indifference to the world at large (see Zuidervaart 1994: 28–44 for an overview).

However, critical theory’s rejection of ‘politically engaged’ art is not driven by disagreement with the politics of such artworks. Rather, it stems from an apparently apolitical disagreement about the necessary and appropriate relationship between content and form. critical theory generally emphasizes the primacy of form in explaining both art’s achievement and its counter-reifying effects (see Hartle 2018 for an overview of the development of this position). The demands of such formal success are (it is claimed) incompatible with direct political engagement. To clarify this, we need to look more closely at what form and content mean in this context.

3.1 Form and Content
For critical theory, aesthetic value is reducible neither to form nor content. A complex relationship between the two is at the core of critical theory’s aesthetics. It is claimed that form and content are inextricable and mutually informing. But here again, we should not confuse this with more familiar positions. The claim is not that form and content are identical (as in A.C. Bradley’s infamous claim in 1909’s Oxford Lectures on Poetry, or Cleanth Brooks’ (1947: 192–215) ‘Heresy of Paraphrase’). Rather, the claim most often advanced is that content is converted into, or ‘sedimented into’, form while still not becoming identical with it.

This idea requires further explanation. When Marcuse and Adorno claim that content is converted into form within art, they are best understood not as advancing the claim that the surface themes and content of the artwork are rendered formal, but as claiming that the meaning of the artwork is expressed primarily by the formal properties of the artwork.

For example, while Kafka’s The Trial has as its surface content a story about a man who is subjected to an interminable and senseless trial for an unknown crime, its true meaning (according to Adorno) is an expression of certain features of capitalism (see further O’Connor 2013). This expression of its deeper meaning is achieved through the texture and organization of the novel itself, rather than in its apparent themes. Nowhere does The Trial baldly state that capitalism has produced an absurd and inhuman form of life. Nor, importantly, is it necessary that Kafka believed such a thing. Critical theory’s claim is that the truth of life under capitalism is in some way communicated to the form of artworks directly. (Some problems with this view are presented in section 3.3.)

To see why content must be converted into form in this way for art to be successful, and why this seemingly abstruse issue prevents critical theorists from endorsing expressly political art, we need to look once more at reification.

The theory of reification claims that thought has taken on a characteristically abstract and instrumental character. It is this form of thought which ultimately underlies the persistence and dangers of capitalism. Anything which can be assimilated by this form of thought is rendered exchangeable and serves as evidence of the appropriateness of that form of thought itself. This means that moral or political claims in art can themselves become part of their commodity value, and fail to disrupt the form of thought which caused the moral or political injuries that motivated the art itself. The dissenting novel, music, or film is something which for all its sincerity is still offered alongside all the others, and whose mode of delivery and consumption replicates, and vindicates, the very form of exchange – and hence consciousness – it seeks to undermine.

3.2 Avoiding Commodification
If art is able to show a form of life and thought outside of capitalism, then it must resist absorption by the characteristic structure of capitalism. Via Marx, critical theorists see the characteristic structure of capitalism as commodity exchange. Commodity exchange has the characteristic function of rendering objects perfectly exchangeable by assigning them monetary value. Via Lukács, critical theorists see commodity exchange also having a characteristic, reified kind of experience and thought – that which sees objects as all conforming to general laws, concepts, and types.

From this, we see that successful art must resist being easily turned into a generic commodity, and must also resist being formed by and wholly comprehensible through pre-set types. In art’s case, these pre-set types would include compositional conventions, cliches, platitudes and the like.

These are structural reasons why art must resist easy adoption by the market (and, in turn, audiences). It must also resist easy adoption to avoid contradicting its own meaning. If art is aimed against exchangeability and cliché, then it must also be aimed against having its content easily converted into products and clichés.

This is why form is primary in understanding art’s ability to undo the effects of reification. It is only through the problematization of the relationship between form and content, and art and its consumption, that the meaning of an artwork can be protected from commodification and nullification. Art is obliged to ceaselessly formally innovate, and to ceaselessly reject pre-set established compositional norms.

In this way, art’s meaning – its critical relationship to capitalism – is communicated in its form, and not in its explicit subject matter. Artworks avoid baldly stating claims about the inhumanity of contemporary society – these could become slogans; be resold and defused – and instead show it through their challenging formal organization.

How, exactly, this critical content becomes converted into formal properties is not clear. Nor is it clear how broad social problems come to be encapsulated in specific artworks authored by specific artists. This is a problem which critical theory never fully mastered. In Adorno’s work, there is no suggestion that this formalization of content is a product of the artist’s conscious or subconscious intention. Rather, the process is ‘blind’, and the social content enters the artwork, and creative process, not as thematic content but as formal problems with which the artist is faced (see Hulatt 2016a).

Marcuse (1978: 14–15), for his part, likewise emphasizes the migration of social content into form, and notes that it is a central perplexity in Marxist aesthetics. In a move which mirrors Adorno’s attempt to see social problems as translated into artistic problems, Marcuse (1978: 41) explains this migration partly in terms of the social preformation of the meaning of artistic materials. The very stuff out of which art is made (‘word, colour, tone’) carries socially encoded meanings, expectations and conventions. This ‘limitation of aesthetic autonomy’, he writes, ‘is the condition under which art can become a social factor’.

Unlike Adorno, however, Marcuse also sees art as a repository of ‘transhistorical’ truths and features. These relate both to certain core definitional features of art and aesthetic response (Marcuse 1978: 15–16) and to core features of the transhistorical ‘species being’ he sees as underlying human experience (Marcuse 1978: 29). Marcuse introduces these non-historicist elements in order to rule out a reductive Marxist account which would claim that art’s function, quality and meaning is chained to its society and class position. Marcuse holds that aesthetic properties cannot be so reducible, as this would render our appreciation of artworks across epochs and economic systems inexplicable. Further, it would nullify art’s important ability to speak beyond and outstrip contemporary ideals and norms.

While for Adorno art’s relationship to its social content was thoroughly hostile – both in its defiance of contemporary norms, and its demonstration of the revisability of those norms – Marcuse offers a more variegated picture. Art critically engages with and negates society, but also describes unrealized potentials for improvement (see, e.g., Marcuse 2006: 140–149). This difference between the two thinkers will be discussed further in section 4.

3.3 Popular Culture
Adorno and Horkheimer coined the phrase ‘Culture Industry’ in their jointly authored book Dialectic of Enlightenment. It was intended ironically, as an insulting oxymoron. This irony has been overtaken by history. The music and film spheres, for example, proudly refer to themselves as industries. This outcome illustrates, after a fashion, the thesis and limits of critical theory’s critique of popular culture.

To critical theory, culture is in a process of coming to an end. This coming to an end is precipitated by ever-increasing strength of norms of exchange and standardization (‘industry’) in controlling the production of art (‘culture’) (see further Hulatt 2016b). This thesis is developed in part sociologically, through examination of the mechanics of radio transmission, the nascent Hollywood system, the distribution and sale of records, and so on. Empirical research into this claim was pursued in various ways, including the Princeton Radio Research Project at which Adorno worked once he arrived in America (see further Jenemann 2007).

The thesis was also developed through aesthetic analysis of popular culture itself. Critical theory found Western popular artforms wanting, judging whole genres and media devoid of merit. Adorno’s criticisms of specific popular artworks were often mordant and unsympathetic, and have had an outsize effect on perception of critical theory’s stance towards popular culture. It can appear that critical theory simply rejects popular culture as inferior, and seeks a return to the kind of high culture many critical theorists enjoyed in fin-de-siecle Germany. Such snobbery would appear particularly odd in supposedly radical Marxist philosophy. However, the broader account of popular culture – its superiority to ‘high culture’ in some respects, and the tragedy of its unrealized technical potential – is innovative and often overlooked.

In a 1936 letter to Benjamin, Adorno remarks that popular and ‘high’ art are ‘two torn halves, which do not add up to a whole’ (Adorno 1977c: 123). This remark conveys a number of things which must be borne in mind. The existence of two separate spheres of popular culture and high art is a reflection of the wrong state of things – their opposition and polarization would not be part of a good society. Both popular culture and high-art have capacities and deficiencies which the other lacks – indeed, they each form a mirror image of the other. However, these two spheres of culture cannot be re-unified by sheer force of will – they do not add up to a unified whole, because they have been objectively driven apart by the nature of the social whole. (Marcuse 1965 takes a similar view.)

We see, then, that Adorno does not see popular culture as merely coarse product. It in fact preserves properties key to art, and serves as a rebuke to ‘high’ culture. For example, popular music is pleasant to listen to, and more or less immediately accessible. This kind of accessibility and pleasure is part of what ideal art would be – and it is part of the failure of modern art that it has been forced to lose both its accessibility and its beauty. (We will enter into the reasons why it suffers this loss momentarily.) Adorno is also not blind to the economic injustices which govern the world of high art – the leisure time and disposable income necessary to acquire a taste in classical music for example.

Conversely, classical music has the virtue that it delivers on what it promises – genuine novelty, pure aesthetic satisfaction and formal achievement. By contrast, popular music promises novelty, ungoverned satisfaction of impulses, and perhaps some associated lifestyle, but is forced to repeatedly renege on these promises (see for example Adorno’s discussion of syncopation in Jazz, in Lewandowski 1996). One way of putting Adorno’s point is that popular culture is not objectionable because it stimulates the passions, but because it does not truly satisfy them (Rebentisch & Trautmann 2019: 22–23). This criticism is shared by Marcuse, who sees increasingly permissive popular culture not as objectionable because it is permissive (he is in favour of relaxed sexual mores, for instance) but because it never delivers on its promise. For Marcuse the release of emotions in cultural products – be they sexual longing, political frustration, or repressed aggression – is allowed just enough to ensure the popularity of the product, and not explored in its full transformative power.

This idea of reneging on promises also underwrites Adorno’s criticism of movements to popularize high culture. For example, Adorno deeply objected to the transmission of performances of Beethoven over the radio. This was not because Beethoven was too good for the public, but because the technology involved was not good enough for the public. The genuine experience of a live performance was being replaced by a low resolution transmission – and the public were invited to believe they had received the genuine article. (Or so Adorno claims, Adorno 1945: 209–210.)

This thread runs through critical theory’s treatment of popular culture – not a suspicion of popularity, accessibility, or pleasurability, but a suspicion of the failure to genuinely provide these things. Popular music and cinema promise excitement and novelty – but in fact work according to pre-set schema. Popular media promises to bring excitement of high culture into the home – but in fact the transmission is poor, and incapable of properly conveying what you wish to listen to. This is the mirror image of high culture’s failure. Quoting Brecht, Adorno notes that culture’s ‘mansion is built out of dogshit’ (Adorno 2006: 366). By this he means that high culture is dependent on a society whose inequality and inhumanity makes the better form of life high art outlines ever more impossible. (For more on the role of Brecht in Adorno’s thought, see Rothe 2018.)

Under capitalism, both high culture and popular culture are not good enough for the public; not vice versa.

4. Aesthetic Experience Against Reification
In section 3.2, the counter-reifying force of art was explained negatively. The unusual formal organization of artworks serves to protect their content from being nullified by the effects of commodification and reification. But the position is not simply that the artwork conceals its meaning such that it cannot be easily commodified. It is that the meaning of the artwork in some fashion breaks with and undermines the false consciousness it has been protected against. Art is thus not merely defensively organized, but aggressively organized also. It does not merely elude reification, but actively combats it.

This claim is one of the more vexed and complex elements of critical theory’s aesthetics. To anticipate, it will be the subject’s experience of the artwork which is held to have this aggressive counter-reifying effect. That experience will be elicited by the appreciator’s active participation in attending to the form/content organization of the artwork.

The counter-reifying effect of art appears to be socio-historically specific (it can be lost over time) and critical (the artwork appears to indict and overturn certain pathologies of thought and social organization). In this section we will look more closely at how various critical theorists explain this view.

4.1 Ernst Bloch
We can find one source of this idea in the work of a contemporary, Ernst Bloch, who had a marked influence on the Frankfurt School. Bloch saw art as combining a faithful account of both the current state of things, and of possibilities usually ignored. This is the ‘not-yet’ which Bloch saw as concealed by conventional experience, but forcibly disclosed by the successful artwork (as well as other cultural products – see Kellner and O’Hara 1976). The utopian aspect to the artwork – its representation of another form of life – is in some sense fantastical and imaginary. It is not a depiction of how things are, but a depiction of how they genuinely might be. But in this way, art discloses the close proximity of another way of ordering life, and the malleability of that which appeared as necessary and natural (see Moir 2018, Levitas 1990).

As with Adorno and Marcuse, Bloch apparently paradoxically combines a keen attention to the formal constitution of the artwork and its autonomy (Bloch 2000: 118) with a commitment to the claim that social conditions enter into the artwork (Bloch 2000: 129–141). Crucially, it is the combination of the autonomous processes of art and the participation of the appreciator which yields up a disruptive experience (Bloch 2000: 143). This disruption within the aesthetic experience serves to point beyond the integration of the artwork’s form and its material; and via the relationship between the artwork and the social content it integrates, it likewise points beyond the established social order (Bloch 2000: 117, 149, 155–156). This is the utopian moment integral to great art, for Bloch. In Spirit of Utopia Bloch largely develops this thought through analysis of music, but in later works like Principle of Hope (Bloch 1954–59), the same approach is recapitulated and applied not only to the various artistic media, but other cultural areas also (see Bloch 1989).

This core vision – of the artwork as eliciting in aesthetic experience a changed outlook on a reified society – is inherited by both Adorno and Marcuse. But both revise Bloch’s approach in differing ways.

4.2 Adorno
For Adorno art does not directly show the world as it really is, but makes plain the contradictions and distortions internal to reified consciousness itself. Rather than disclosing new potentials for action and change, art instead discloses the full estrangement between reified consciousness and the world. In other words genuine aesthetic experience is a process of disillusionment. Our reified consciousness perceives no obvious difference between the world as we take it to be, and the world as it is. But art forcibly discloses this difference. The art appreciator comes to learn the depth and extent of their ignorance, and the urgency of breaking with the dominant form of thought. The utopianism of Bloch reappears in a more muted form – art gives us awareness that the demands and tendencies of capitalist society are not absolute but could in principle change. Adorno’s caveat is that we do not know how or in what direction this change should be effected. This is due to the broader ethical dislocation produced by modern society. (For differing takes on Adorno’s negative ethics see Finlayson 2002, Freyenhagen 2011, 2013, Bernstein 2001; a transcribed discussion between Adorno and Bloch on the nature of utopian thinking provides useful further detail, in Bloch 1989: 1–18).

The way in which art demonstrates the disconnection between reified thought and the world is highly complex. We can say that for Adorno art engages the conceptual structure of thought and experience, and guides it such that this structure is lead to collapse in the course of its being applied. This is a power it shares with philosophy, with the key difference that philosophy achieves this explicitly – through manipulation of and argument about concepts in the course of their application in thought – and art accomplishes it through the manipulation of aesthetic material. To answer how the ‘hermetically sealed’ artwork is able to do this, we would need to spell out Adorno’s claim that extra-aesthetic content is sedimented into formal aesthetic problematics in individual artworks. This is beyond the scope of the present entry (but see Hulatt 2016a, Nicholsen 1997a, Paddison 2011, Sherratt 2009: 169–209).

4.3 Marcuse
Marcuse sees artworks as having a liberating potential, and as disclosing to us possibilities for change. Art discloses these possibilities in both a destructive and creative way. It is destructive in that it serves as an ‘indictment’ of certain governing forms of thought and experience. But it is also creative; art serves to re-present the genuine state of things, and thus uncovers the objective potentials for ‘liberation’ which are concealed there (Marcuse 1978: 6). Marcuse’s work can thus be understood as an amalgam of some of the formal sophistication of Adorno’s aesthetics with the utopian emphasis of Bloch’s account.

For Bloch and Adorno, engagement with the artwork leads in radically differing directions. For Adorno, aesthetic experience does not point towards a better future, nor show any signs of such a future in the present. It rather shows the inhumanity of the present. This is the ‘indictment’ Marcuse likewise finds in art. For Bloch, by contrast, aesthetic experience presents an account of the genuine state of things, which includes already existing anticipations of a better form of life. These ‘anticipations’ provide the intelligible, if incomplete, link between the present and the utopia that could emerge from it. These Marcuse often refers to as moments of ‘liberation’.

It may seem that the two points from Bloch and Adorno are in tension, and thus that Marcuse is ill-advised to try to combine them. However, Marcuse introduces a third theoretic element which resolves the apparent tension – the work of Sigmund Freud (1856–1939). The rational, critical element of Marcuse’s account conforms significantly to Adorno’s approach – but the utopianism of Bloch is recast in a psychoanalytic register.

Marcuse understands the utopian function of art to be discharged by its ability to facilitate ‘desublimation’ – the expression of repressed urges. This should not be confused with catharsis. Catharsis – in which emotions are activated and then ‘purged’ – leaves the subject pacified, free of the troublesome energies that were activated. This pacifying effect of catharsis in art is criticized by Marcuse, and desublimation is held to avoid it.

For Marcuse, desublimation is an expression of repressed emotions, urges and potentialities of which the appreciator was unaware. On learning of their existence, the appreciator is made aware of ‘repressed potentialities of man and nature’ and is granted an ‘emancipation of sensibility’ which points towards the form of life, and of sensibility, which revolution would bring about (Marcuse 1978: 8–9; see further 43–44). This is continuous with Marcuse’s broader political program, for which a better world would not only entail re-organization of the structure of society, but also of the psychological structure that society imposes on individuals (see Kellner 1999, Marcuse 1955).

For Marcuse, then, the aesthetic experience is both critical and rationally ordered, and an arena for the liberation of drives, emotions and potentials. These respectively articulate the contradictions and intolerability of the present, and unharness the libidinal resources which could realize the potential for a better future.

5. Natural beauty
In the 20th century philosophers – anglophone and non-anglophone – largely shifted away from discussing the aesthetics of nature. The Frankfurt School were a marked exception to this tendency. In their emphasis on the moral and aesthetic significance of nature’s beauty, critical theorists anticipate many features of contemporary work in environmental aesthetics. Like environmental aestheticians, critical theorists understand our appreciation of nature’s beauty to be morally significant. They also likewise see aesthetic appreciation of nature to be bound up with the recognition of and resistance to humanity’s destruction and exploitation of nature.

Critical theorists differ sharply from contemporary environmental aestheticians, however, in their account of what carries this moral significance. It is not nature ‘in itself’, without human interference, that is of prime importance. Any idea of nature being ahistorically beautiful ‘in itself’ is rejected. It is nature in relation to our current human capacities, needs, and projects which carries both beauty and a moral impetus to relate to nature in a less damaging fashion.

This section will touch briefly on some of the distinctive features of their work on this theme.

5.1 Nature and History
Adorno, Horkheimer, and Marcuse consistently problematized the distinction between nature and history. They did not claim that nature – as picked out by the physical sciences – was historically relative. They rather claimed that the concept (and experience) of nature – as a realm of facts and values outside of human control – was historically determined in its content and means of application.

When Adorno and Horkheimer claim that what is natural is historical they are interested in normative conceptions of nature which are used either to make the contingent appear necessary (‘capitalism simply follows human nature’) or to advocate for inhumane treatment of persons. Likewise, they propose that nature as experienced is influenced by these historical factors. (Adorno’s complex account of nature is developed in its various strands in Cook 2011, Vogel 1996: 51–101, Flodin 2018, 2022.)

Natural beauty in both its scope (what counts as natural) and quality (those aesthetic properties which can attach in virtue of an object’s being natural) is seen as a historical phenomenon (Hammer 2015: 45–72). This further means that natural beauty stands under the threat of cliché, ideological misuse, and co-optation, just as artworks do. Simply, the aesthetic properties of nature shift across time – and what is thus appropriate in judging natural beauty likewise develops over time (Daniels 2020).

Any well-developed aesthetics of nature includes not only a criterion of appropriate judgement, but also a criterion of the appropriate objects of judgement. Trivially, aesthetic judgements of nature cannot be well-formed when aimed at an object which is the product of, and perceived under the description of, complete artificiality. Conventionally, the meaningful debate to be had is over what degree, if any, objects produced by artifice are appropriate objects of the appreciation of natural beauty.

Critical theory adds a unique complication to this debate. It denies that the merely natural is an appropriate object for aesthetic experience (Johnson 2011). We will explore this in the next section.

5.2 Mere Nature
Critical theory sets a high store on the aesthetic experience of nature, but not on the experience of mere nature. This is made particularly stark in Aesthetic Theory, where Adorno compares raw nature with industrial materials:

[N]ature that has not been pacified by human cultivation, nature over which no human hand has passed – alpine moraines and taluses – resembles … industrial mountains of debris. … [Human artifice] is said to have ravished nature, yet under transformed relations of production it would just as easily be able to assist nature and on this sad earth help it to attain what perhaps it wants. … [I]n every particular aesthetic experience of nature the social whole is lodged. Society … determines what nature means[.] (Adorno 1970 [2004: 89])
This rejection of an aesthetics of nature in the raw is combined with a keen interest in natural beauty. This appears paradoxical, but can be elucidated through comparison with contemporary environmental aesthetics.

For environmental aesthetics, mere nature is indeed an appropriate object of aesthetic attention (see, e.g., Budd 1996: 211–213, Saito 1998). The properties of mere nature, as opposed to artworks, are held to undermine some traditional dichotomies. For example, Ronald Hepburn (1966) influentially observed that nature needs to be ‘framelessly’ appreciated as an enclosing space. The usual strict separation between an art-object and its environs is not present, as nature is both the immediate object of interest (a copse of trees, for example) and the enclosure continuous with the object (the vista in which the copse is situated). Separately, it has been claimed that appropriate aesthetic appreciation of nature benefits from, or requires, theoretical understanding of nature itself and its structure (for examples of such scientific cognitivism, see Carlson 1979, Eaton 1998, Parsons 2006). Such scientifically grounded appreciation of nature is held to be objective appreciation of nature as what it is.

Critical theory’s rejection of mere nature can be seen as stemming from a similar desire to appreciate nature ‘as what it is’, and a similar conviction that doing so will dissolve the oppositions and dichotomies familiar from art. But critical theory urges us to further dissolve the dichotomy between nature and human artifice itself.

Adorno, Horkheimer, and Marcuse see nature and humanity as not separate but continuous (for an overview of this, and its relation to Lukács and Marx, see Feenberg 1981: 240–255). And so proper contemplation of nature ‘as-it-is’ is of nature-for-us, as parts of that nature. Mere nature is nature which stands opposed to us, and which we are approaching as if it were radically different to us, rather than a fellow piece of nature. This is why Adorno sees untamed nature as resembling unrefined industrial material – in both cases (the merely natural; the industrial byproduct) we have objects which appear to stand opposed to human need, and present as hostile to us.

This is what stands behind remarks by both Adorno and Marcuse to the effect that both humanity and nature are not fully realized. Nature by itself, for Adorno and Marcuse, is unreconciled and hostile towards humans, who are themselves part of nature. For nature’s own benefit, a different kind of relationship between humans and nature is required. And natural beauty is held to point towards it (Krebber 2020: 184–186).

It is common to find in environmental aesthetics an admission that the area of enquiry is bound up either with moral intuitions about nature’s value, or in some way importantly tied to a broader moral project of environmental conservation and transformation (e.g. Saito 1984, Saito 2018, Carlson 2018, Alcaraz León 2022, Carlson & Lintott 2008). Here again, Adorno and Marcuse can be understood as sharing this feature and transforming it. Their environmental aesthetics is not embedded in care for nature-without-humans, but for nature-for-humans, and correlatively, humans-for-nature.

5.3 Reconciliation, Ruins and Landscapes
Adorno and Marcuse embed aesthetic response to nature into a broader project of reconceiving and reorienting the relationship between humanity and nature. Following on from a myriad of influences (some anthropological – chiefly Henri Hubert and Marcel Mauss (1902–1903), James George Frazer (1890), and Roger Caillois (e.g., 1937) – and some philosophical) critical theory understands the relationship between humanity and nature to have been predominantly ‘dominating’, just as humanity’s relationship to itself has been dominating and oppressive. Similarly, just as aesthetic experience offers a vision of a different order of social being, so too does aesthetic experience of nature offer a vision of a reconciled relationship between human agency and nature.

Mere nature, then, is ineligible to produce such an insight, as mere nature is outside of and opposed to a relationship with humanity. This leads Adorno to find images of reconciliation in, paradoxically, images of bygone eras of the exploitation of nature. Here Adorno intersects with the rich aesthetic traditions of both the picturesque and the ruin, but brings a very different sensibility to these phenomena. In Aesthetic Theory he writes,

Historical works are often considered beautiful that have some relation to their geographical setting, as for instance hillside towns that are related to their setting by the use of its stone …. So long as progress … does violence to the surface of the earth, it will be impossible – in spite of all proof to the contrary – completely to counter the perception that what antedates the trend is in its backwardness better and more humane. (Adorno 1970 [2004: 84])
What grounds Adorno’s positive aesthetic evaluation here is the image of a relationship between nature and humanity which, from our timeframe, appears more pacified. Given the environmentally disastrous logic of modern society, the images of a reconciled, utopian relationship to nature can be found only retrospectively, in the image of the less-bad relationships to nature which have now passed away.

6. Conclusion – Critical Theory and Aesthetics
We can pick out four features of critical theory’s distinctive treatment of aesthetics. The first of these is a product of critical theory’s general methodology at that time. First generation critical theory largely holds that any object of experience or theory is a part in and reflective of a social totality of facts and processes (see further Jay 1992). The form and depth of this ‘reflection’ varies; but the general view is that any element of this totality could be determined by, and inform us about, any other. This warrants speculative kinds of philosophical interpretation, in which facts apparently limited to one sphere (of logic, or psychology, say) are in fact determined by and reflective of apparently disconnected spheres (of economic exchange, or social institutions, say). Art is no exception to this approach. Both works of high art and ephemeral pieces of pop culture can have philosophical, social, and political content as parts of their objective constitution.

Secondly, this emphasis on the philosophical richness of art is combined with a primary interest in aesthetic form. In turn, this amalgam is wedded to a conviction that aesthetic experience – despite being formally rigorous and content-laden – can offer a form of insight unavailable through conventional conceptual reasoning. This insight – precisely by being a break with the kind of experience moulded to the ‘wrong state of things’ – is held to have emancipatory promise.

Thirdly, the tensions and instabilities between these jointly held aesthetic methodologies are not accidental, but intentional. Critical theory refuses to attempt to deliver static definitions, but also refuses to do without such definitions. (It is neither ‘rationalized’, nor irrational.) Art really does have general features; but successful artworks always both engage and destabilize general concepts. There is an emphasis on the necessity of using concepts to understand art, and on the ability of artworks to ultimately elude conceptual capture. This complex approach – which is incapable of delivering a closed definition of art or aesthetic value – is intended to prioritize fidelity to aesthetic experience over dogmatic conceptual structures.

Fourthly, and following from the preceding three features, there is the seemingly paradoxical conviction that artworks are political and practical via their very refusal of explicit politics and practicality. It is artworks of surpassing quality, attended to for their own sake, that have a liberating potential. Critical theory tends to assume (and sometimes explicitly argue) that artists who primarily pursue a non-artistic goal in their art (political or otherwise) cannot realize this level of quality, and art-appreciators who seek a pay-off in art (political or otherwise) cannot properly attend to the value of artworks.

These four features all reach their most extreme expression in the work of Theodor Adorno. But they are also pursued at differing levels of extremity and emphasis in other critical theorists and thinkers associated with them, most obviously Marcuse, Benjamin and Bloch.

For critical theorists, there is a balance to be struck between analysing art – and hence articulating it in conceptual terms – and making clear the importance of the artwork’s ability to outstrip concepts altogether. The preceding has been an account of the ways in which critical theory has explored and handled this demanding balancing act, and the broader theoretic considerations which, together with the artworks investigated, entered into that balance.

. History of Empirical Research on Art and Aesthetics
Modern scientific approaches to art and aesthetics find their origins in Germany in the nineteenth century, in some of earliest works in experimental psychology. Most notably, with his Vorschule der Aesthtik (1876), Gustav Fechner pioneered what came to be known as “bottom-up aesthetics”, which tried to discover general laws of taste by examining preferences for simple geometric shapes such as rectangles of varying proportions, colors, and arrangements of lines (for a summary of “bottom-up” aesthetics, see Nadal & Ureña 2022).

In mid-twentieth century, art historian and philosopher Thomas Munro—who founded the American Society of Aesthetics in 1942 and served as the editor of the society’s publication The Journal of Aesthetics and Art Criticism between 1945 and 1964—continually expressed optimism about the prospect of integrating philosophical and scientific approaches to aesthetics (1928, 1948, 1951, 1956, 1963). In “The Psychology of Art: Past, Present and Future” (1963), Munro observes that philosophers have actually been asking, for a long time, questions about art and aesthetics that are at least partly empirical, such as how do artists come to create works? how does the experience of art affect the audience’s character? are there rules by which the arts can please and instruct? do some works universally please across epochs and cultures? how can different species of aesthetic pleasure be taxonomized?. In “Methods in the Psychology of Art” (1948), he notes that authors in The Journal of Aesthetics and Art Criticism often make empirical claims as part of their arguments, and so should use empirical methods more.

Not all philosophers have been as optimistic as Munro. George Dickie (1962) argued that psychology is not relevant to aesthetics. Some of Dickie’s worries echoed earlier ones: for example, he argued that the psychology of art was impoverished by simplified stimuli, such as the use of geometric shapes rather than real artworks (compare Arnheim 1952). Other worries were due to his specific conceptions of philosophy of art and aesthetics, and philosophy in general. First, Dickie believed that aesthetics is “concerned only with the language and concepts which are used to describe and evaluate works of art” (1962: 289), and so questions outside of this conception—such as how do artists come to create works?—are simply irrelevant. Second, Dickie believed that philosophy is discontinuous with science, such that “the problems of ethics are not solved by a scientific study nor are the problems of the philosophy of science” and aesthetics is no exception (1962: 301–302).

While Dickie’s criticisms, and hardline view, held considerable sway over philosophical aesthetics and the philosophy of art in the second half of the twentieth century, this has not continued. From the late 1980s onwards, philosophical aestheticians and philosophers of art have increasingly appealed to the findings of cognitive sciences (for a summary, see entry on aesthetics and cognitive science), and to a lesser extent to the findings of empirical aesthetics, and particularly evolutionary aesthetics (see, for example, Dutton 2009). Indeed, from around 2010 onwards, philosophers joined the psychologists of art and empirical aestheticians in conducting empirical studies.

In some ways, it is difficult to pinpoint exactly where modern empirical aesthetics and the psychology of art ends, and the experimental philosophy of art and aesthetics, begins. But a rough characterization can be made along the lines that Dickie suggested. Empirical aesthetics and the psychology of art is primarily concerned with characterizing the psychological responses to art and aesthetically significant objects. It answers questions such as: what is the nature of the responses (such as chills, pleasure, changes in self-conception)? what features of aesthetic objects and artworks tend to elicit these responses (such as curvature, certain colors, etc.)? are there systematic individual differences in relation to this? Whereas experimental philosophy of art and aesthetics—as a branch of philosophy—is primarily concerned with empirically studying conceptual distinctions. It answers questions such as: do people think that a moral demerit can also be an aesthetic demerit? do people think that something can be art if it does not have any aesthetically valuable properties? what is the nature of the folk’s concept of art and beauty—are they purely descriptive or evaluative concepts?

Nonetheless, it is important to stress that, at best, this way of carving up the distinction picks out a central tendency of the two fields. In reality, the work done by researchers in philosophy, including experimental philosophical aesthetics, and empirical aesthetics overlaps in many ways. To give a few examples. Philosophers have had a longstanding concern in trying to establish whether there is a distinctive kind of aesthetic state of mind, and empirical aestheticians have recently become interested in this question. A couple of influential ideas about this from philosophy are that the pleasure taken in beauty is of a disinterested kind, where this roughly means that it is not the result of desire satisfaction, or does not essentially produce desires (see Kant 1790); and that approaching objects aesthetically involves adopting a distanced attitude where we disengage from the object practically, and do not relate it to our standing desires or interests (Bullough 1912). More recently, empirical aestheticians have attempted to tackle this issue with the tools provided by neuroscience and psychology. For example, Marcus Nadal and Martin Skov (2018) have argued against the idea that there is a distinctive sui generis state of mind, on the grounds that, for example, the same neural hardware that is involved in responding to pleasant tasting food and sex have been shown to be involved in the appreciation of aesthetic objects. By contrast, Amy Belfi and colleagues (2019), for example, have shown that aesthetic appreciation involves activation of the Default Mode Network, which they suggest may show that self-reflection, rather than self-detachment, may form part of what makes aesthetic responses unique. Philosophers have also been interested in explaining beauty in terms of a harmony between the beautiful objects and our psychological faculties in some ways (as present in, for example, Hume 1757a and Kant 1790), and psychologists have sought to explain aesthetic appeal in terms of processing characteristics, such as the fluency with which an object is experienced (Reber et al. 2004). Both experimental philosophers as well as aesthetic psychologists have tried to elucidate the features of moral actions and traits that lead to attributions of beauty, as well as the kind of psychological state appreciation of this kind of beauty gives rise to (see, for example, Doran 2023; see §6).

Unfortunately, notwithstanding the many overlapping concerns, the fields of philosophical aesthetics (including experimental philosophical aesthetics) and empirical aesthetics have remained largely siloed. On the side of philosophical aestheticians, this has continued to lead to missed opportunities for testing the empirical aspects of their theories and for experimental philosophical aestheticians to methodologically innovate. And on the side of empirical aestheticians, this has led to a failure to benefit from the theoretical and argumentative sophistication that tends to be characteristic of the best work in philosophy. However, there are signs that this is changing. Empirical aestheticians are increasingly attempting to test claims drawn from philosophy (for example, Brielmann & Pelli 2017; Winner 2019), and experimental philosophers of art and aestheticians are increasingly working with psychologists, if not empirical aestheticians yet (for example, Humbert-Droz et al. 2020). Nowhere is this more clear than in the case of work on awe and the sublime, where philosophers have worked productively with psychologists, and where philosophical claims have informed the design of empirical studies and the resultant theory building in turn (for example, Clewis 2021; Keltner & Haidt 2003; Shiota et al. 2007; Arcangeli et al. 2020; Shapshay 2021).

2. Definition of Art
“What is art?” stands as one of the central questions in philosophical aesthetics. In fact, we can ask different questions about the concept of art. First, we may ask about its extension: which works count as art? Second, we may ask about its intensional structure: are there conditions necessary and sufficient for a work to count as art? Third, we may ask about its function: is calling a work ‘art’ a praise, or merely a classification? Since many philosophers of art agree that the definition of art should be compatible with art practices and the way ordinary people think about art, unsurprisingly, it was also one of the first questions in aesthetics to be empirically investigated.

Which works count as art? In his first empirical study on intuitions about the extension of the concept of art, Richard Kamber (2011) presented participants with a large number of descriptions and images of objects and asked whether they would classify these objects as art. The main focus of this study was putting to test prominent definitions of art: aesthetic definitions claim that art is created with an intention to be aesthetically appreciated; institutional definitions claim that art is created by an artist and presented to an artworld; and historical definitions claim that art is created with an intention to belong to the same set of objects as previously created works of art. Kamber’s approach was to examine a variety of “hard cases” discussed in the aesthetics literature, such as objects of low aesthetic value and objects that were made prior to social art-making practices. He concluded that none of the art theories succeed in fully tracking people’s intuitions about the various hard cases, but the aesthetic definition of art, which holds an artwork to be an object created with an intention to provide people with aesthetic experiences, was somewhat more successful than others. In a follow-up study, Kamber and Taylor Enoch (2019) also asked participants to justify their decisions of what is art by selecting some of fourteen possible reasons, which included those that emphasized intentional creation, the creator’s consciousness, beauty or evoking imaginative experiences. In this study, justifications involving intentionality were the most often chosen. Nevertheless, this study again indicated that none of the main definitions of art fully aligned with what the study participants, predominantly art professionals or art lovers, found intuitive.

However, these studies have received some criticism. While Annelies Monseré (2015) is sympathetic to Kamber’s criticism of philosophers’ reliance on intuitions in defining art, she is equally skeptical of reliance on ordinary people’s intuitions. Instead, she advocates for a more indirect role for intuitions, on which they are not used to directly justify any specific definition of art, but as elucidations of how the concept gets invoked in practice. Ellen Winner (2019: 21) notes that Kamber “designed his study very informally, testing a grab bag of theories, using only one or two examples to test each one”, and that it might benefit from a more sensitive measure than a dichotomous choice of ‘yes’ or ‘no’, as was used.

Although neither folk nor expert intuitions strictly require a work to have high aesthetic value for it to be classified as art, more beautiful (or more liked—a more common concept in the psychological literature, which nowadays tends to steer clear of discussions of beauty) works do tend to be classified as art more often. Matthew Pelowski and colleagues (2017) investigated the relationship between ratings of liking and attributions of art status. Participants were shown a set of 140 digital images of abstract paintings, hyperrealistic paintings, poorly-executed paintings and ready-made sculptures, and were asked to spontaneously classify them as ‘art’ or ‘not art’. They were also asked to rate the extent to which they liked those images. Pelowski’s findings revealed a positive correlation where higher ratings of liking were associated with a greater likelihood of being categorized as art, which provides some support for the aesthetic definitions of art.

Can art be defined by necessary and sufficient conditions at all? Elzė Mikalonytė and Markus Kneer (forthcoming) investigate whether the folk concept of art is an essentialist or a non-essentialist one; in other words, whether it can be defined by a set of individually necessary and jointly sufficient conditions. In contrast to Kamber’s studies mentioned earlier, they asked people who were not art professionals. In two vignette studies, Mikalonytė and Kneer manipulated three properties of artworks—namely, being intentionally created, having aesthetic value, and being institutionally recognized—aiming to see whether any of those properties, corresponding to the main essentialist art definitions, are seen by the folk as necessary conditions for an object to be classified an artwork. The results, similar to Kamber’s, also suggest that the folk concept of art is not an essentialist concept, but rather a cluster concept. Interestingly, none of the three properties were considered necessary—there were cases where art status was ascribed even to accidentally created objects. This finding is surprising considering the role that intentional creation and the creator’s intentions are thought to play in this context in the literature on philosophical aesthetics (Mag Uidhir 2013), as well as some studies in the psychology of art. For example, Jean-Luc Jucker et al. (2014) discovered that when people are asked to classify artefacts into art and non-art, their decisions are guided by inferences about the creator’s intentions. George Newman and Paul Bloom’s (2012) results showed that participants’ beliefs about whether an object was intended to be an artwork or not had an important effect on how they see a physically identical copy of the same object. More generally, it is widely believed that people classify objects into artefact kinds by making inferences about the creator’s intentions (Bloom 1996). Mikalonytė and Kneer’s study, however, is not the only one showing that intentional creation is not seen by the folk as necessary—they have also discovered that although people consider AI-generated paintings to be art to a similar extent as human-created paintings, they are not very willing to consider AI-creators artists. In the context of artistic creation, mental state (including intention) ascription to AI agents is relatively low, and this might partially explain why AI robots are not accepted as artists (Mikalonytė & Kneer 2022). However, another study by Mikalonytė and Kneer suggests that the phenomenon of art without an artistic intention might not be confined to the realm of AI-generated art: even human creators are seen as capable of creating artworks without intending to do so (Mikalonytė & Kneer forthcoming).

Is calling a work ‘art’ to praise it, or to merely classify it? Shen-yi Liao, Aaron Meskin, and Joshua Knobe (2020) take a different tactic to understand the concept of art. Their aim is not to uncover its extension, or to defend any specific concept of art, but to clarify its nature. Descriptivists about the concept of art contend that to call something ‘art’ merely conveys a classificatory status, whereas evaluativists contend that to do so is to convey a positive evaluation. Liao, Knobe, and Meskin use linguistic patterns to argue that the concept of art is neither. Instead, it is a “dual character concept”, which involves characteristic values that are realized by concrete features (Knobe, Prasada, & Newman 2013). To diagnose the nature of the concept of art and other art concepts, they examine participant responses to sentences of the following schema:

That is not good, but it is true [concept].

Extant research shows that dual character concepts, but not descriptive concepts, tend to sound fine when combined with the “true” modifier (Knobe, Prasada, & Newman 2013). So, for merely descriptive concepts, the sentence makes little sense. For example, it sounds weird to say “that is a true sonnet”. Moreover, for positive evaluative concepts, the sentence also makes little sense because of the explicit negative evaluation. For example, it sounds weird to say “that masterpiece is not good”. Since participants think that the sentence “that is not good, but it is true art” sounds fine, Liao, Meskin, and Knobe argue that the concept of art is neither descriptive nor evaluative, but dual character.

3. Ontology of Art
Ontology of art (see entry on history of the ontology of art) aims to discover what kind of things works of art are, which ontological category or categories they belong to, whether it is possible and what it means to create or destroy them, and what it means for two different objects to be ‘the same’ work. Works of art can be divided into two categories: repeatable and non-repeatable. The former category consists of musical works and other kinds of works that exist in multiple instantiations. The latter category consists of singular works of art where there is only one original instance of that work and all others are merely copies of the original, for example, paintings or sculptures. This distinction also has implications for the way people evaluate work of art.

For repeatable artworks, the most pressing ontological question concerns the conditions under which two performances are of the same work. Christopher Bartel (2018) investigated intuitions on the repeatability of pop songs. He presented study participants with three scenarios describing three pairs of musical performances, with each of these pairs reflecting one of the following differences: a difference in provenance (two identically sounding performances are played by two different bands), in affect (one performance sounding humble, and one sounding dramatic), and in connotation (the two performances are played by different bands, with different lyrics and expressive of different emotions). Bartel found that a difference in provenance does not make a difference to whether the song is identical across different performances, but differences in affect and in connotation do.

Elzė Mikalonytė and Vilius Dranseika (2020) focused on works of classical music. They created scenarios that reflected the main points of disagreement among theories of the individuation of musical works, such as sonicism (which claims that identity of musical works depends on their acoustic properties only), instrumentalism (which also adds the instrument used to perform the musical work to the list of identity-conferring properties), and contextualism (which also emphasizes the importance of musico-historical context). In contrast to many other studies, Mikalonytė and Dranseika target intuitions about the identity of two performances at the same point in time. They presented the participants with seven scenarios, including, for example, two identically sounding performances of two identical scores which were independently created by two composers, or two performances that differ only with respect to their emotional expressivity. They concluded that folk intuitions correspond most with pure sonicism, the theory which claims that work identity depends solely on its (non-timbral) acoustic properties, although the identity of the composer is also an important factor. While Bartel concludes that pop music songs are not easily repeatable—in many cases, participants were inclined to deny that two performances were of the same song—Mikalonytė and Dranseika’s study points in the opposite direction: people consider works of classical music to be quite easily repeatable.

Nemesio Puy (2022) has criticized this approach for relying solely on textual vignettes, lacking real musical stimuli (for more on this discussion, see section 8). Puy’s experiments show that, compared to Bartel (2018) and Mikalonytė and Dranseika (2020), when study participants have the chance to hear musical works, they are even more likely to answer the individuation (or repeatability) question in the sonicist way. This tendency is especially apparent if the question is asked immediately after hearing two musical samples, without any contextual information being provided.

Two more empirical studies in this area of inquiry investigate people’s intuitions regarding the persistence of musical works—in other words, their identity over time. Mikalonytė and Dranseika (2022) explored the hypothesis that musical works’ identity crucially depends on their purposes: different versions of a musical work remain versions of the same work if and only if they retain the same overall point they were created for. Their results provide some support for this hypothesis, but purpose was not considered to be a necessary condition. Again, this study shows that people have mostly sonicist intuitions—they believe that the identity of musical works mostly depends on their acoustic properties, and this is considered to be a much more important criterion in judgments of identity compared to the overall purpose of the work as intended by the composer.

Elzė Mikalonytė and Clément Canonne (forthcoming) found that judgments of the identity of artworks—both musical works and paintings—are partially normative. Their results provide some support for the Phineas Gage effect—according to which, changes in valued qualities, and especially moral properties, change identity judgments—suggesting that if a musical work undergoes some changes and becomes more aesthetically valuable, people are more likely to say that it is still the same musical work compared to the condition when the musical work becomes less aesthetically valuable. However, the effect observed was easily overridden by changes in material identity or moral value and for this reason it does not seem sufficient to claim that musical works are essentialized in terms of their aesthetic value.

All of the empirical studies in the ontology of musical works so far have focused on their identity conditions. Many other topics remain unexplored by experimental philosophers, such as the way musical works come into existence and cease to exist. An overview of such topics and a systematic survey of philosophers’ appeals to ordinary intuitions regarding musical works is presented in Mikalonytė (2022), where she also discusses how the ontology of musical works could benefit from further empirical research.

Unlike repeatable artworks that can have many genuine and potentially equally valuable instances, other works of art, such as paintings or sculptures, can only have one physical object. The relationship between different instances of these artworks is of copy and original, where only one physical object can count as a given artwork. This has important implications both for identity judgments and aesthetic evaluation.

Given that many non-repeatable artworks share similarities with ordinary, non-artistic artefacts, it is helpful to compare studies that explore the role of material continuity in judgments of artefact and artwork persistence. Sergey Blok, George Newman, and Lance Rips (2005) investigated people’s intuitions about the persistence of various types of objects, including persons, animals, plants, and artefacts. Participants were presented with a vignette about each of these objects either (a) being disassembled into individual particles, transported and reassembled again, or (b) being replaced by an identical material copy, the original of which is destroyed. People were inclined to see artefacts as the same after being ‘copied’. In a related study, David Rose and colleagues (2020) have investigated intuitions about the Ship of Theseus puzzle across different cultures. Their results suggest that people are ambivalent about whether it is the continuity of form or the continuity of material that is decisive in matters of identity. Results of both studies suggest that material identity might not be the main criterion for judgments of persistence of artefactual objects. However, extant empirical research suggests that judgments of the persistence of artworks are different from those of other artefacts. When presented with a scenario about someone creating a copy of either an artwork or of a tool and destroying the original object, people are not willing to see the copy as ‘the same’ object, even if the only difference between the tool and the artwork is labeling them as such (Newman, Bartels, & Smith 2014).

Some philosophers, such as Arthur Danto (1973), claim that a copy of a non-repeatable artwork is always aesthetically less valuable. Empirical research also suggests that people tend to value a copy of an artwork less than the original, even if the two are perceptually indistinguishable (Rabb, Brownell, & Winner 2018). George Newman has conducted a series of studies to explain this effect. One possible reason is that the created object is evaluated as the result of a unique creative act; another is that there is a perceived physical contact between the object and the original creator (Newman & Bloom 2012). When a duplicate object is made by someone other than the original creator, people are less inclined to see it as the same object (Newman, Bartels, & Smith 2014). Since people believe that an object’s or person’s essence can be transferred by means of physical contact, Newman and Smith (2019) hypothesized—and confirmed—that differences in evaluation between a copy and an original painting are mediated by the artwork’s perceived anthropomorphism; that is, feelings that the artwork seems alive and expresses emotions. In some cases, physical contact is not necessary for beliefs in contagion: intentional contact may be enough (Stavrova et al. 2016). Shen-yi Liao, Aaron Meskin, and Jade Fletcher (2020) examined the contagion effect in the museum context. They asked the participants (a) whether the objects in the gallery embody “the very being” of their creator, and (b) whether they are unique, and they found that contagion has an effect on perceived aesthetic value both in the museum and laboratory context, while uniqueness matters only in the latter.

Finally, there is one more way aesthetic information has an effect on ontological judgments, even if this kind of research does not speak directly to the ontology of art: aesthetic preferences may influence judgments of personal identity. Previously, it had been thought that we consider humans and their ‘true selves’ to be fundamentally morally good, and that changes to someone’s moral character influence judgments of a person’s identity. Joerg Fingerhut and colleagues discovered that changes in our aesthetic taste are also seen as profoundly transformative changes: when someone’s aesthetic preferences change, they cease to be the same person (Fingerhut et al. 2021).

4. Aesthetic Judgments
A particularly fruitful area of experimental philosophical research has centered around the question of how objective our aesthetic judgments are, and related issues such as the possibility of aesthetic testimony. This has principally been done by either examining meta-aesthetic intuitions, or by examining the amount of agreement in aesthetic matters, and the source of this agreement.

With respect to the issue of objectivity, many philosophical aestheticians have thought that aesthetic judgments intend to express truths about the way the world is, and that some people have better access to these truths than others. David Hume (1757a) suggests that some people are better able to detect and weigh the aesthetic merits of a work than others—they have delicate taste—and that works that are reliably appreciated over time and across cultures are those which are truly good. Immanuel Kant (1790) suggests that while our judgments of beauty are based in pleasure, they command universal agreement—that is, we expect others to make the same judgments as us. In this respect, aesthetic judgments have been thought to be unlike statements of personal taste, such as ‘broccoli is delicious’, about which there can only be blameless disagreement; and like empirical judgments, such as ‘there is a piece of broccoli on my plate’, about which there can be genuine disagreement. Indeed, some have thought that the way the folk act presupposes such a realist conception of aesthetic judgments, with Noël Carroll (1999), Nick Zangwill (2005), and Peter Kivy (2015) noting that we argue with each other about aesthetic matters.

Taking this as a starting point, a number of psychologists and experimental philosophers have presented findings that suggest realism cannot be given special status as the commonsensical view, and philosophical accounts of aesthetic judgments do not need to accommodate realist intuitions. This line of research started with Geoffrey Goodwin and John Darley (2008), who asked people to determine whether comparative aesthetic judgments—such as ‘Shakespeare was a better writer than Dan Brown’—were true, false, or a matter of opinion. Most participants described aesthetic statements as opinions (despite the strength of agreement with each statement) and they did this more frequently than in the case of comparable moral, factual statements, or statements reflecting social conventions.

Then, in a series of studies led by Florian Cova, the folk’s meta-aesthetical views were further tested by presenting participants with an aesthetic disagreement—such as where someone finds a sunset beautiful and the other does not—between two interlocutors (or between the participant and an interlocutor), and asking participants whether one person is correct, both are correct, or neither is correct. Across different kinds of objects (including natural objects and art widely recognized to be beautiful, as well as objects that study participants personally find beautiful), type of aesthetic judgments (including judgments of beauty and ugliness), and across a wide range of different countries, it has been found that most select the option “Neither is correct” (Cova & Pain 2012; Cova, Olivola, et al. 2019; for further studies utilizing the disagreement method, see Andow 2022).

Returning to the comparative method, Nathaniel Rabb, Alex Han, and colleagues (2022) have presented further evidence against the idea that the folk are aesthetic realists by explicitly asking participants whether aesthetic judgments are matters of opinion or matters of fact. They showed that people believe that aesthetic judgments are subjective even after learning that one of the two works has been historically acclaimed, or even when they liked one artwork much more than another (though, for criticisms of this study, see Moss & Bush 2021).

Supporters of the presumption in favor of realism have, however, fought back. Zangwill (2019) argues that Cova and his colleagues’ studies are not about whether people think aesthetic judgments can be true or false, but rather about whether a given person is right or wrong, and so leave the presumption in favor of realism unscathed. The distinction Zangwill is aiming at is as follows: Someone who guesses correctly that it is raining outside would be saying something true when they say that “it is raining outside”, but they cannot be described as right. Being right is a matter of being justified in saying something. In addressing Zangwill’s critique, in the same design where participants are asked to consider an interlocutor disagreeing with them in making various kinds of judgments, including aesthetic judgments, Cova (2019) asked participants whether one, both or neither person said something true or false. The results here were quite different from those of the studies conducted to date: with the modal response being that only one person says something true (40%), followed closely by the response that says that both say something true (39%). Despite these differences, Cova suggests that these do not support the idea that the folk tend to be realists about aesthetic judgment on the grounds that the pattern of responses did not match the pattern for paradigmatic factual judgements (that is, a disagreement about whether something is steel, where 71% of participants selected the response that only one person said something true).

A further objection has been raised to this work on folk meta-aesthetics by Filippo Contesi and colleagues (2024). They point out that all the studies discussed above reveal that the folk’s explicit meta-aesthetic views are subjectivist, and that this is consistent with what supporters of aesthetic realism say. These supporters—such as Carroll (1999), Zangwill (2005), and Kivy (2015)—claim that the folk are implicitly realist in arguing about matters of taste, even if they hold explicit subjectivist attitudes, as expressed by hackneyed proverbs such as “there’s no accounting for taste”. As such, Contesi and colleagues suggest that Cova’s results are inconclusive, and that disproving folk aesthetic realism as it has been conceived of by realists to support the plausibility of their position would require a different methodological approach.

Turning away from critiques of aesthetic realism to positive accounts of folk meta-aesthetics, experimental philosophers have also suggested that folk meta-aesthetical views might nonetheless allow for some degree of objectivity, and have found that the concept of good taste might behave differently from that of aesthetic truth.

Cova (2019) suggests that the folk might be expressivists about aesthetic judgments, and that they may think that there can nonetheless be correctness conditions for aesthetic judgments, insofar as people can, for example, be mistaken about the cause of the feelings they express. In one study to begin to test this position, Cova presented participants with a case where someone judges the Eiffel tower to be beautiful as a result of being high on drugs, or as a result of seeing the Eiffel tower unimpaired. The results reveal that participants were less likely to say that a judgment of beauty was true and more likely to say that the judgment was false when the experience was the result of drugs. Similarly, across five studies that manipulated the type of disagreement (cross-cultural or intercultural, or internal disagreement of one individual over time) and asked participants about the possibility of error in aesthetic judgments, James Andow (2022) found that while people do not hold realist beliefs, they do believe that they have correctness conditions (though see Murray 2020 for results suggesting that people do not think that disagreement implies that they are seen as incorrect).

Moreover, although most studies on aesthetic judgments point in the direction of subjectivism, research on aesthetic taste suggests that people believe aesthetic taste can be good or bad. Constant Bonard et al. (2022) asked participants whether it makes sense to distinguish between good and bad taste, and then asked to define what it is. The majority of participants agreed with the distinction, and although a significant part defined good taste in terms of the ability to detect aesthetic properties, expressing the view compatible with aesthetic realism, for other participants, good taste was compatible with aesthetic subjectivism, since ‘good taste’ was defined simply as something corresponding to their own personal preferences. Another phenomenon that has been thought to be relevant to the issue of whether good taste exists, is that of ‘guilty pleasures’—enjoying aesthetic objects one feels one should not enjoy. As Kris Goffin and Florian Cova (2019) observe, the existence of guilty pleasure at first sight might be considered evidence for the existence of good taste among the folk. However, they present evidence suggesting that the guilt people experience should be understood as guilt for violating social norms, not aesthetic ones, and therefore should not be seen as evidence for folk aesthetic realism.

In addition to the meta-aesthetical method outlined above, psychologists and experimental philosophers have also examined realism about taste by considering the mechanisms that result in people’s aesthetic judgments.

Some philosophers have suggested that the idea that there be objective aesthetic value might be demonstrated simply by pointing to the fact that some artworks and not others are universally judged as aesthetically valuable. For example, Hume (1757a) suggests that some works are, truly, better than others, and that those works will pass the test of time: they will be judged to be good across cultures and epochs, and they will do this in virtue of truly having aesthetically good-making features.

However, James Cutting (2003) has presented evidence that has seemed to put pressure on this Humean view. Having found that merely exposing people to impressionist works made them like them more, Cutting suggests that we may like canonical works because they have been continually broadcast to the world. Armed with Cutting’s findings, the aesthetic skeptic might argue that passing the test of time isn’t an indication of aesthetic quality, but rather an indication that people have merely experienced the works more frequently.

In defense of the Humean view, Meskin et al. (2013) suggest that mere exposure might not indiscriminately improve liking of works, irrespective of their aesthetic quality; but rather, help us to more accurately appreciate their true aesthetic merits and demerits. As a corollary, they also suggest that works may enter the canon because they are truly better. Putting their Humean defense to the test, they merely exposed participants to works that the authors and many critics consider good and bad (namely, works by John Millais and Thomas Kinkade, respectively). The results revealed that participants liked the Kinkade paintings less the more they were exposed to them, and the results suggested a trend for participants to like the late Millais paintings more the more they were exposed to them (though this was not significant). Meskin and colleagues interpret this evidence as consistent with the existence of aesthetic value, as well as the reliability of the test of time: with repeated exposure, we are better able to appraise a work’s good- and bad-making features, and so those works that endure do so, at least in part, in virtue of having good-making features.

Bence Nanay (2017) has criticized the idea that mere exposure is relevant to aesthetic realism. First, studies on mere exposure target spontaneous reactions, while aesthetic judgments are traditionally thought to be reflective and unfolding in time. Secondly, the mere exposure effect seems to work only with good artworks and not with bad ones—exposure to good artworks makes positive aesthetic judgments more likely, but not the other way around. Most importantly, according to Nanay, experiments show that exposure to one artwork changes our preference for that particular artwork, but not for any other artwork. In order for these experiments to count as evidence against aesthetic realism, Nanay contends, we would need to demonstrate that exposure to one particular artwork can influence our preferences for other artworks of the same kind (for example, of the same artistic style).

Finally, another tightly related question is about the nature of aesthetic testimony: if our aesthetic judgments are similar to empirical judgments, we can reliably learn about aesthetic properties from what other people say—if during a phone call someone says that a piece of broccoli they are having for lunch ‘is beautiful’, should we trust their testimony to the same extent that we would trust their claim that ‘there is a piece of broccoli on my plate’?

Andow (2019) asked his study participants whether they think that forming aesthetic beliefs based on testimony given by a friend or an expert is less permissible and legitimate compared to forming such beliefs based on first-hand experience, and also compared to forming non-aesthetic beliefs, such as beliefs about size or price. Although his results confirm that there is an asymmetry between the extent to which people are inclined to trust aesthetic testimony, compared to testimony about non-aesthetic properties, interestingly, this effect was not moderated by the participants’ attitudes toward the status of aesthetic judgments. Moreover, another similarly designed study shows that aesthetic and moral beliefs based on testimony, in contrast to descriptive beliefs, are not seen as constituting knowledge (Andow 2020).

5. Aesthetic Adjectives
Aesthetic adjectives, such as ‘beautiful’ and ‘elegant’, are central to aesthetic communication: they are the most common tools with which we attribute aesthetic properties to works and communicate aesthetic judgments with others. Some philosophers contend that aesthetic adjectives constitute a segment of natural language that is interesting in its own right, for different reasons. Frank Sibley (1959, 2001) argues that aesthetic adjectives are distinctive in that they require taste to apply. By this, Sibley means that whether an aesthetic adjective applies to a work is never determined by any set of non-aesthetic properties. Tim Sundell (2017) argues that although aesthetic adjectives are not semantically distinctive, they are metalinguistically distinctive because of their role in coordinating and negotiating standards. By this, Sundell means that when you say ‘this artwork is beautiful’ and I say ‘no it is not’, we are not only attributing properties to the work itself, but communicating our different standards of beauty through our different applications of the term ‘beautiful’.

There is a nearby segment of natural language that has attracted much attention from philosophers and linguists: predicates of personal taste such as ‘tasty’ and ‘fun’. Indeed, some experimental philosophers have made valuable contributions to this debate (such as Kneer, Vicente, & Zeman 2017; Dinges & Zakkou 2020; Kneer 2021). However, scholars in this debate typically set aesthetic adjectives to the side in their investigations. For example, Peter Lasersohn (2005: 645) explicitly does so in order to avoid fundamental issues in aesthetics. In contrast to the lively scholarly activity on predicates of personal taste, there are only a few works that explicitly and primarily investigate aesthetic adjectives. As such, it remains an open question whether aesthetic adjectives are distinct from predicates of personal taste, or whether there exists a unified treatment of the two.

Louise McNally and Isidora Stojanovic (2017) argue that while predicates of personal taste are necessarily mind-dependent insofar as they entail an experiencer, aesthetic adjectives are semantically distinctive because they express evaluations without entailing an experiencer. McNally and Stojanovic’s diagnostic appeals to the fact that the verb ‘find’ tends to complement adjectives with an experiencer. For example, sentences like ‘I find him attractive’ tend to sound fine but sentences like ‘I find him tall’ tend to sound weird. Using the British National Corpus, they found that aesthetic adjectives do not tend to complement ‘find’, which they take to be evidence that “their evaluative component is not based directly on personal experience” (2017: 29).

Shen-yi Liao and Aaron Meskin (2017) argue that aesthetic adjectives are semantically distinctive because they exhibit a strange sort of context-sensitivity. Standardly, gradable adjectives are classified as absolute or relative. Absolute adjectives—such as ‘straight’ or ‘spotted’—have their standards of application built in, and do not rely on the context to fix this threshold. By contrast, relative adjectives—such as ‘warm’ or ‘long’—do rely on a context for its threshold of application. Through a series of experiments involving a diagnostic used to classify gradable adjectives, Liao and Meskin found that aesthetic adjectives behaved like neither absolute nor relative adjectives. Participants were presented with pairs of objects and asked to pick out ‘the [adjective] one’. The key to this diagnostic is that ‘the’ implies both existence (there is at least one) and uniqueness (there is at most one). As such, most participants are unable to pick out the spotted disc when presented with two discs that are spotted to different degrees because ‘spotted’, as an absolute adjective, has a context-insensitive threshold of application which is met in both cases. By contrast, most participants are able to pick out the long rod when presented with two rods that are long to different degrees because ‘long’, as a relative adjective, has a threshold of application that is sensitive to the context. In particular, participants are able to construct an implicit comparison class using the context of application: they pick out the longer rod as ‘the long one’. However, Liao and Meskin found that about half of the participants use ‘beautiful’ like ‘spotted’ and about half of the participants use ‘beautiful’ like ‘long’. Moreover, the same pattern holds also for negative aesthetic adjectives like ‘ugly’ and thick aesthetic adjectives like ‘elegant’. These results are difficult to explain for the standard typology of gradable adjectives.

Stojanovic (2019) argues that Liao and Meskin’s results do not provide grounds for drawing any interesting conclusions regarding semantic adjectives because the studies do not reveal a stable pattern. The 50/50 pattern in response to the request to pick out the beautiful / ugly / elegant object is just what would be expected if participants were answering by chance. Liao, McNally, and Meskin (2016) conducted further experiments and corpus observations to show the instability of aesthetic adjectives’ behaviors. On some diagnostics they pattern with absolute adjectives, but on other diagnostics they pattern with relative adjectives. In response to these results, they propose a different hypothesis: aesthetic adjectives are like relative adjectives insofar as both involve implicit comparison classes, but unlike relative adjectives insofar as their implicit comparison classes are not determined by the immediate context of application.

Where the studies described above have attempted to treat aesthetic adjectives as a homogeneous and sui generis class, more recent studies have pointed to important sources of heterogeneity amongst them. ‘Beautiful’ and ‘pretty’ are similar adjectives in that they can both express certain descriptive contents—namely, that an appearance is intrinsically pleasing, or that it is, for example, delicate, small, and soft. But they differ insofar as prettiness is thought to be more closely tied to appearances and less important than beauty. In trying to account for this patterning, Doran (forthcoming a) suggests that BEAUTY but not PRETTINESS is a dual-character concept, and that in addition to the descriptive senses they share, BEAUTY has a normative sense connected to our most cherished values, including, most prominently, moral goodness. In support of this claim, in one of the studies reported, he shows that ‘beauty’ but not ‘prettiness’ is judged to be able to felicitiously combine with the ‘true’ modifier, which is thought to be one source of evidence that the concept expressed by a given lexical item is dual-character (Knobe et al. 2013). “That is true beauty” sounds perfectly natural to native speakers of English, but “That is true prettiness” sounds decidedly odd.

6. Morality and Aesthetics
Morality and aesthetics stand as two prominent normative domains. How do the concerns in these two domains interact with one another? Drawing from a substantive philosophical literature on these interactions (see Harold 2023 for overviews), topics at the intersection have also been empirically investigated in recent years. Here, we roughly divide works into two aspects: concerning morality’s influence on aesthetics, and concerning aesthetics’ influence on morality.

In the first direction, concerning morality’s influence on aesthetics, philosophers have wondered about the influence of moral attitudes on aesthetic attitudes. In traditional philosophical aesthetics, this is sometimes known as the “value interaction” or “ethical criticism of art” debate (Clavel Vázquez 2018; Giovanelli 2007; Liao & Meskin 2018; McGregor 2014). There are three main positions: autonomists say that moral attitudes do not influence aesthetic attitudes; moralists say that negative moral judgments always negatively influence aesthetic judgments; and contextualists say that moral attitudes’ influence on aesthetic attitudes depends on the context.

This direction of value interaction might affect taste perception. Patrik Sörqvist and colleagues (2013) found that, between two qualitatively identical cups of coffee, participants whose attitudes are congruent with sustainability rated the one labeled as “eco-friendly” as tastier. However, Aaron Meskin and Shen-yi Liao (2018) were unable to conceptually replicate this result. Similarly, Beth Armstrong and colleagues (2019) found that the valence of ethical information affected consumers’ expected experience of food. Taken together, these results suggest that a folk psychology of moralism or contextualism is currently more plausible than a folk psychology of autonomism.

This direction of value interaction might also affect judgments of beauty. One longstanding debate in this context surrounds whether moral goodness can be beautiful in itself—with philosophers such as Plato (c. 370 BCE) and Shaftesbury (1711) claiming that moral beauty exists, and others such as Edmund Burke (1757) and Immanuel Kant (1790) denying this (though see Doran forthcoming d). Until recently, one of the principal ways that philosophers have tried to settle this matter is by examining the use of ordinary language from the armchair. Berys Gaut (2007), for example, argues in favor of the existence of moral beauty principally by noting that we call people beautiful when they are good. Gaut argues that this kind of talk cannot be intended non-literally, as was suggested by Burke (1757), as neither of the two defeators of literal use—obvious falsity (as in ‘my boss is a pig’) or trivial truthfulness (as in ‘I’m not over the moon’)—seem to apply to locutions that appear to express moral beauty. But as Ryan Doran (2021) notes, Gaut’s method of testing for non-literal use is too demanding, as it wrongly assumes that people are always truth-maximisers. To move past this apparent impasse from the armchair, and help to reveal the number of species of moral beauty that exist, Doran suggests that we turn to experimental studies. He shows that people tend to judge morally good people to be more beautiful, and that this cannot be deflated in terms of non-literal intent or an error (such as misattribution) on the grounds that making the source of the goodness salient, and giving people the opportunity to express their approval of the goodness prior to making the judgment of beauty, does not eliminate the effect of moral goodness on judgments of beauty. Doran also finds evidence that moral goodness can affect the beauty of physical appearances by affecting the determinants of thick aesthetic properties such as balance and delicacy, and that people’s moral character can be beautiful in itself, suggesting that beauty is not perception dependent.

Building on this work, experimental philosophy studies have also been used to help resolve apparent inconsistencies in the existing literature on which moral traits are beautiful, as well as reveal hitherto unacknowledged reasons why morally good traits and actions are beautiful, among other things.

Supporters of moral beauty can be divided into particularists about the beauty of traits—who tend to hold that only the ‘warmer’ virtues such as compassion are beautiful (for example, Kant 1764)—and universalists about moral beauty—who tend to argue that all virtues are beautiful, and indeed that certain colder non-moral traits such as intelligence can be beautiful too (Schiller 1793 [2003], 1793 [2005]; Gaut 2007; and Paris 2018).

Doran (2023) proposes that these positions only appear to be inconsistent with one another, as they range over different kinds of beauty: with universalists targeting the beauty that is found in good form, and particularists targeting the kind of beauty that lies in things that have a disposition to lead to an emotion that is variously described as ‘love,’ ‘elevation’ and ‘ecstasy’—which is characterized by feelings akin to being moved, inspired, and of unity with the object of this state. To test this view, he presented participants with a story about two individuals who are equally well-formed—in the sense that their mental states are all working harmoniously to lead them to do the right thing—but differ in the kind of virtue they exhibit, with one individual being just, and the other being compassionate. Consistent with the idea that there is a beauty in some traits which resides in the disposition to give rise to this special emotion in addition to well-formedness, participants found the fully just and fully compassionate individuals to be equally virtuous and good, but the latter to be more beautiful to the extent that this individual tended to give rise to this special emotion to a greater extent.

Examining the link between internal harmony and beauty more explicitly, Doran (forthcoming b) has tested the idea—which is most prominently found in Friedrich Schiller’s On Grace & Dignity (1793 [2005]) and Kallias (1793 [2002])—that actions are beautiful if and only if they are expressive of freedom by being the result of a high degree of internal harmony, as in cases where our desires, beliefs, and will all seamlessly work together to produce the good action. While Doran finds some evidence which is consistent with actions being beautiful to the extent that they are expressive of freedom by being the result of a high degree of internal harmony, his results also suggest that the moral actions of conflicted individuals can be as beautiful, or even more beautiful, as those of internally harmonious moral agents, and so Schiller’s claim need to be weakened and supplemented with additional claims. In one experiment, for example, participants were presented with two individuals who both do the right and good action in making necessary redundancies and giving financial support to those affected, where the only difference is that where one individual makes the redundancies without any internal conflict, the other does so with a great deal of conflict due to a reluctance to afflict the necessary suffering. Consistent with his earlier findings, the results show that the latter individual’s action is considered to be more beautiful, and that this is due to the latter individual’s tendency to move us, and make us feel at one with them. As such, Doran suggests that it is not only the internal harmony of the agent who performs an action that determines its beauty, but also the degree to which the action tends to make us feel as though we are harmoniously related to the agent that performs the action. Further elucidating some of the reasons why morally good actions can be beautiful, Doran (forthcoming c) finds that people tend to think that morally good actions are beautiful when the action is seen as expressing who the person truly is (their essence), and as stemming from a location deep inside of them, and in turn tends to lead to feelings of being moved and inspired.

Imagination may play an especially important role in mediating moral attitudes’ influence on aesthetic attitudes. Imaginative resistance refers to the phenomenon in which “an otherwise competent imaginer finds it difficult to engage in some sort of prompted imaginative activity” (Gendler & Liao 2016: 405; see also Miyazono & Liao 2016). Imaginative resistance is puzzling because imagination is standardly unconstrained. Typically, a competent imaginer finds no difficulty in imagining factual deviations, such as a fictional world in which humans and dragons co-exist. However, it has been hypothesized that imaginative activities that involve moral deviations are especially prone to evoke imaginative resistance (Gendler 2000, 2006). For example, it has been suggested that a fictional world in which female infanticide is morally right is likely to evoke imaginative resistance (Walton 1994). Philosophers disagree about many aspects of imaginative resistance, such as: whether the resistance is special to imagining moral deviations, whether the resistance reflects an intrinsic limitation of imagination, and indeed, whether the phenomenon is real in the first place. Experimental philosophers and psychologists have sought to bring systematic empirical evidence to help resolve these disagreements.

As an early example of this kind of work, Liao, Strohminger, and Sripada (2014) conducted two studies on imaginative resistance and its driving factors. In the first study, they asked participants to engage with a story in the style of Greek myths, in which it is morally right to trick a person into entering a romantic relationship. They found evidence for imaginative resistance being a real phenomenon: the extent to which this fictional world is counter to participants’ moral attitude is correlated with the extent of their self-reported imaginative difficulty. However, they also found evidence against the resistance reflecting an intrinsic limitation of imagination: the extent to which participants are familiar with the genre conventions of Greek myths is also correlated with the extent of their self-reported imaginative difficulty. In the second study, they presented a fictional world in which it is morally right to sacrifice an infant, but varied the genre of the story such that some participants engaged with a story in the style of police procedurals but others engaged with a story in the style of Aztec myths. Sure enough, participants do have a harder time accepting that infant sacrifice really is morally right in the police procedural world, but an easier time accepting the same for the Aztec myth world. This contrast found in this study (replicated by Mark Phelan and colleagues in Cova, Strickland, et al. 2021) lends further support to the reality and the non-intrinsicality of imaginative resistance.

Subsequent investigations by other philosophers and psychologists have found additional support for the reality of imaginative resistance and further uncovered its contours. Jessica E. Black and Jennifer L. Barnes (2017) have designed and validated a scale for measuring imaginative resistance. With this scale, they have also found that participants do experience imaginative resistance in response to moral deviance, albeit with contextual and individual variations (Barnes & Black 2016; Black & Barnes 2020). However, Hanna Kim, Markus Kneer, and Mike Stuart (2019) found that the resistance is not special to imagining moral deviations. Instead, imaginative resistance reflects the “weirdness” of the claim that participants are asked to imagine, which is itself an amalgam of three factors: unusualness, counterfactuality, and surprisingness. Morally deviant claims, as a class, are not necessarily more weird than factually deviant claims, as a class. Moreover, given that surprisingness is a component, weirdness depends on expectations which might be modified by genre expectations and other contextual factors. Dylan Campbell, William Kidder, Jason D’Cruz, and Brendan Gaesser (2021) found that the resistance does not reflect an intrinsic limitation of imagination. Instead, imaginative resistance reflects individual differences in emotional reactivity: participants who experience less negative affect in response to harms also experience less difficulty in imagining moral deviance.

In the second direction, concerning aesthetics’ influence on morality, philosophers have also wondered about the influence of aesthetic attitudes on moral attitudes. This direction comes up too, albeit much more rarely, in the “value interaction” debate (Harold 2006; Stecker 2005). In psychology, however, aesthetic attitudes’ influence on moral attitudes has been systematically studied in an extensive literature on the beauty-is-good stereotype (Dion et al. 1972; compare the metaanalyses in Eagly et al. 1991 and Langlois et al. 2000). Roughly, the idea is that positive aesthetic judgments always positively influence moral judgments of persons. This stereotype holds in a surprisingly wide variety of domains, such as pedagogy and politics.

Philosophers have been equivocal in their answer to the question of whether aesthetic appreciation has a salubrious effect on us morally. Cynics about beauty have suggested that appreciating beauty might have a corrupting influence. J. Robert Loftis (2003), for example, suggests that beauty might lead us to focus on the superficial, “skin deep”, features of the world. But some philosophers have been more sanguine about the prospect of moral cultivation via beauty. Plato, in the Symposium, suggests that the appreciation of physical beauties leads to the appreciation of non-perceptual kinds of goodness; and Kant (1785) suggests that a love of natural beauty in particular is a “mark of the good soul”, and indicates that a person is susceptible to the “moral feeling”. Since this issue is an empirical one to an important extent, it is perhaps no surprise that experimental philosophers and empirical aestheticians have entered the fray. Providing correlational support for the optimistic view, Diessner et al. (2013) found that the tendency to be sensitive to beauty (that is, to notice it, and be moved by it), and particularly sensitivity to natural beauty, was associated with the moral attitudes towards close and distant others (in line with Kant’s suggestion). Providing evidence of a causal relationship, appreciation of natural beauty has been found to lead to more morally admirable behavior (Zhang et al. 2014; see Silvers & Haidt 2008 and Landis et al. 2009 for evidence concerning the morally salubrious effects of appreciating moral beauty). In addition to this work on beauty, the moral effects of appreciating the sublime have been explored in the context of empirical work on the nature of awe (see Piff et al. 2015).

Philosophers have generally held two main positions about the role that something’s beauty can play in grounding moral standing. On the one hand, optimists about beauty have argued that beauty confers intrinsic moral standing—that is, beautiful things are worthy of protection independently of their relationship to humans and other animals (for example, G.E. Moore 1903; Routley, 1973). Pessimists about beauty, by contrast, think that beauty at best provides a non-intrinsic kind of moral standing, insofar as it is but one source of pleasure for humans (for example, Passmore 1974). Experimental philosophers and empirical aestheticians have recently tried to cast light on some of the mechanisms that might be involved in beauty’s effects on judgments of moral standing, with a view to interrogating its normative significance in some cases. Doran (2022), for example, argues that both the optimists and pessimists are incorrect. Across two studies with beautiful plants, he shows that to the extent that people tend to experience the beauty of plants—and in particular to the extent that they tend to feel moved and inspired by the beauty—they tend to judge that the plant can feel pain and has intrinsic moral standing. As such, he argues that the intuitions that optimists appeal to should be debunked, and that beauty tends to give rise to a state that is more valuable than mere pleasure, contra the pessimists. Investigating the issue through the lens of Moral Foundations Theory, Klebl and colleagues (2022) found that purity intuitions tend to underlie people’s willingness to protect beautiful things.

7. Emotion and Art
There is a rich set of puzzles in philosophical aesthetics concerning emotional responses to fictions, and here, both psychologists and experimental philosophers have made contributions, in some cases moving the debate beyond the standard philosophical concerns. Here we discuss a few of those that have received the most attention from philosophers: How can fiction elicit emotional responses when we know that the characters do not exist? If certain works of art, particularly music, can evoke negative emotions, like sadness, which we typically aim to avoid in everyday life, why do we pursue such experiences in art contexts? Moreover, how can music be expressive of emotions, if there is nobody in the music itself experiencing them?

The ‘paradox’ of fiction is motivated by the following observation: if we were to learn that events in life that make us feel sad have not in fact come to pass, our sadness would disappear. But the same is not true in art. I may know that Tolstoy’s Anna Karenina does not exist, and yet may feel sad reading about her fate in the novel (Radford & Weston 1975). With this in mind, the paradox of fiction has standardly been formulated as a trilemma of three individually plausible but mutually inconsistent claims (for example, Currie 1990):

We have emotional responses directed towards fictional entities and situations in literature and art;
In order to have emotional responses we need to believe in the existence of the entities and situations that they are directed at;
We do not believe in the existence of the fictional entities in literature and art.
Most philosophers have now jettisoned this paradoxical formulation, by rejecting proposition (2). But, even if there is no paradox per se, philosophers have noted that interesting questions remain here: Do our emotional responses to fictions differ from their real-life cognates? And, if so, what might explain this? Do our emotional responses to fiction involve different mental representations, for example? And are any differences that exist sufficient to constitute a different kind of emotional response?

In connection with this, some experimental philosophers have thought that the emotional responses that are had in response to fictional entities and events might differ in their intensity, as a result of differences in self-referential processes. Sperduti et al. (2016), for example, asked participants to watch clips of scenarios apt to elicit positive or negative emotions, or neutral video clips, presented as either mockumentaries (fiction), or documentaries or amateur films (non-fiction). Participants self-reported less intense emotions only in response to the negative clips when they were presented as fictions, and even here, there were no differences in the physiological responses (and specifically, in electrodermal activity). The authors interpret this as suggesting that the emotional responses to fiction are genuine emotions, on the grounds that there are no physiological differences, but that appraisals of fictionality might nonetheless cause people to psychologically distance themselves from the content (for discussion, see Pelletier 2019). Humbert-Droz et al. (2020), by contrast, found that longer clips of sad scenes lead to lower skin conductance when labeled as non-fictional versus fictional, as well as greater self-reports of sadness—suggesting that believing that the clip was real led to greater sadness. Given the mixed findings in this context, the issues of whether the emotions that we feel in response to fiction differ from those we feel in non-fictional contexts, and if so why, remain open questions.

The paradox of negative emotion (Hume 1757b), has intrigued philosophers since the time of Aristotle: why do we seek exposure to art arousing negative emotions if negative emotions is something that we tend to avoid in our everyday life? One important example is listening to sad music, which is often thought to evoke sad mood in listeners (though for dissent, see Kivy 1990). There is vast psychological literature on emotional responses to music that are relevant to this philosophical discussion (see Mitterschiffthaler et al. 2007; Juslin & Västfjäll 2008; Vuoskoski & Eerola 2012; Koelsch, 2014; Peltola & Eerola 2016; Juslin 2019), and it has received some attention from experimental philosophers as well.

Mario Attie-Picker and colleagues (2024) tested the hypothesis that people choose to listen to sad music because of the emotions music is expressive of: listening to sad music allows people to feel more connected. In the first study, participants were presented with vignettes describing musical pieces with differing levels of musical proficiency and emotional expressiveness. They were then asked to what extent they agreed that the described piece of music was good and embodied the essence of what music is ‘all about’. The results revealed that emotional expressiveness, more so than technical proficiency, influenced judgments on what are the characteristic values of music. In the second study, participants were asked to complete sentences about (a) the characteristic values of music, (b) feeling connected in conversations, and (c) pleasantness of music. They found an overlap between the emotions people listed as embodying what music is “all about” and the emotions that make people feel connected in conversations. Attie-Picker and colleagues thus try to explain the paradox by shifting the focus away from the traditional emphasis on the listener’s felt emotions and instead centering it on emotions one perceives in music.

The paradox of emotional expressiveness is related not to the emotions we feel when we listen to music, but rather emotions we hear in the music itself. In our everyday conversations, we often characterize music as joyful, sad or angry. We use those terms when discussing a piece of music—an entity that does not have mental states and is incapable of experiencing emotions.

One way to empirically study the relationship between music and emotional expressiveness is through cross-cultural research. Psychological literature suggests that cross-cultural recognition of emotions in music is quite limited. Some studies have shown that the list of cross-culturally recognizable emotions in music is limited to three basic emotions of happiness, sadness and fear (Fritz et al. 2009). Other studies suggest that even major and minor chords may not, after all, be universally associated with happiness and sadness (Lahdelma et al. 2021; Smit et al. 2022). However, at least aversion to dissonant musical chords appears to be cross-cultural (Lahdelma et al., 2021).

The question of cross-cultural recognizably has also been tackled in Constant Bonard’s experimental philosophy paper (2019). Bonard argues that the affective meaning of a musical piece depends on musical grammar, as there is an overlap of cognitive mechanisms constituting the capacity for language and capacity for music. According to him, listeners familiar with certain musical idioms and grammatical organizations are better able to perceive the affective meaning of a piece. Bonard presented his participants in Geneva and India with excerpts from Western classical music, South Indian music, as well as a set of atonal melodies that do not belong to either of these cultures. They were asked to identify musical excerpts that do not correspond to musical grammatical rules. For both Indian and Western participants, the Western and atonal (but not Indian) stimuli were easier to encode for those familiar with the musical idiom. Participants were also asked to listen to musical extracts and continually rate how much the music expressed a given emotion. The study confirmed that participants were better at recognizing the affective dimension of music that originated from their region. Taken together, these studies present tentative evidence that the recognition of emotions in music may depend on familiarity with local musical grammar rules (for more readings on musical semantics, also see Schlenker 2017, 2019, 2022).

The topic of art and emotion induction may also be relevant to discussions on art and morality. Angelika Seidel and Jesse Prinz (2013b) found that music can be used to induce positive or negative emotions, which in turn modifies moral evaluations. Roughly, happy music increases judgments of goodness, and angry music increases judgments of wrongness. Seidel and Prinz (2013a) further discovered that different negative musically-induced emotions, anger and disgust, can impact the severity of different kinds of moral judgments. A more complex result comes from Ansani and colleagues (2024), which shows that musical expertise is likely to lead to more individualizing moral foundations as opposed to binding ones.

8. Methodological Debates
Throughout this entry, we have generally focused on recent empirical research done by philosophers on topics in philosophy of art and aesthetics. However, this scope is admittedly arbitrary. As noted at the start, the research program that we have surveyed is continuous with empirical aesthetics in psychology, and comes from a long historical tradition that encompasses both philosophy and psychology. The principal reasons to draw boundaries are pragmatic ones.

Like other branches of experimental philosophy, experimental philosophy of art and aesthetics involves gathering data using empirical methods and bringing analyses of the data to bear on philosophical theorizing. As a matter of general fact, research in experimental philosophy is relatively replicable (Cova, Strickland, et al. 2021), and relatively free of scientific misconduct such as p-hacking (Stuart, Colaço, & Machery 2019). While experimental philosophy of art and aesthetics is bolstered by this general track record, it also inherits a number of methodological challenges from experimental philosophy and related areas of psychology regarding instruments, samples, and stimuli.

By far, the most common instrument used in experimental philosophy of art and aesthetics is—like other branches of experimental philosophy and related areas of psychology—the questionnaire. Participants’ responses are measured by their answers to questions posed by the researchers. Nick Zangwill (2019) expresses a general skepticism toward studies that use questionnaires, and criticizes experimental philosophy of art and aesthetics for its wide use of this specific measurement instrument. Drawing inspiration from Wittgenstein, Zangwill is pessimistic about the questionnaire’s attempt to use language to reveal agents’ thoughts generally. In addition, he is pessimistic about the questionnaire’s capacity to reveal agents’ normative judgments, such as judgments of beauty, as opposed to non-normative judgments, such as judgments of agreeableness. Zangwill’s critique could be taken as an invitation for experimental philosophers to explore methodological tools beyond the questionnaire. In fact, some philosophers have already experimented with eye movement tracking (Wright et al. 2019), virtual reality (Francis et al. 2016), electroencephalography (Bricker 2020), and corpus analysis (Liao, McNally, & Meskin 2016; McNally & Stojanovic 2017; Sytsma et al. 2019; Chartrand 2022; Doran, forthcoming a). Some of these or other proposed methods (see Fischer & Curtis 2019; Fischer & Sytsma 2023) might also enrich experimental aestheticians’ toolboxes.

The most common sample used in experimental philosophy of art and aesthetics is—once again, like other branches of experimental philosophy and related areas of psychology—WEIRD: participants from Western, Educated, Industrialized, Rich, and Democratic countries (Henrich, Heine, & Norenzayan 2010). Whether responses from these WEIRD participants are representative of people in general remains an open question. Within experimental philosophy (and related areas in psychology), there is an ongoing debate about the legitimacy of making theoretical generalizations based on empirical results from WEIRD samples (for criticisms, see Stich & Machery 2023 and Peters & Lemeire 2024; for defenses, see Knobe 2019, 2021). Clearly, this ongoing debate impacts the evidentiary value of existing research in experimental philosophy of art and aesthetics as well.

That said, it is important to highlight a couple of cross-cultural works in experimental philosophy of art and aesthetics. In one work, Florian Cova, Christopher Olivola, and colleagues (2019) extend Cova’s earlier research on the intersubjective validity of aesthetic judgments to a sample that includes participants from 19 countries on four continents. Across six geographical areas (Europe, Middle East, Central and North America, South America, East Asia, and South and Southeast Asia), they found both variations and convergences in patterns of responses. While participants from East Asia tend to endorse ‘subjectivism’ about aesthetic judgments (when two people disagree, both can be correct), participants from other geographical areas tend to endorse ‘nihilism’ (when two people disagree, neither is correct or incorrect). At the same time, people everywhere tend to not endorse ‘realism’ (when two people disagree, at most one can be correct). In another work, Constant Bonard (2019) conducted studies in Switzerland and India to vindicate the hypothesis that musical idioms have grammatical structures. The grammar of Western classical music was found to be more recognizable to Switzerland participants than Indian participants, but no reverse asymmetry was found for South Indian classical music. Another study investigated aesthetic judgments of mathematical beauty between Chinese and British mathematicians and found that they do not seem to be strongly influenced by cultural differences (Sa et al. 2024). As things stand, these three cross-cultural works remain the exception, and not the norm, in experimental philosophy of art and aesthetics. The fact that the vast majority of work has been conducted with Western European and American samples is not dissimilar to the situation in empirical aesthetics (see Che, Sun, Gallardo, & Nadal 2018) or music cognition (see Jacoby et al 2020). As such, this is a problem that needs to be addressed in all fields that empirically study art and aesthetics.

In addition, samples used in experimental philosophy of art and aesthetics tend to be ordinary people with no special expertise in philosophy or the relevant arts. One criticism of experimental philosophy’s relevance for philosophical theorizing, commonly called the expertise objection, endorses privileging experts’ responses over ordinary people’s. While the existing debate primarily concerns the expertise of philosophers—insofar as the objectors privilege philosophers’ intuitions from thought experiments—in the domain of philosophy of art and aesthetics, expertise in the respective artforms might be relevant as well. Many psychology studies have shown differences between ordinary people and art experts in aesthetic judgments and preferences (Hekkert & Van Wieringen, 1996; Leder, Ring, & Dressler 2013), as well as emotional responses to art (Silvia 2013; Leder, Gerger, et al. 2014), and these differences are relevant to at least some of the topics experimental philosophers are interested in. As such, we want to highlight a few works in this domain that use experts as samples.

Three studies in experimental philosophy of aesthetics have compared expert and non-expert samples. In one empirical study based on moral foundations theory, Alessandro Ansani and colleagues (2024) found that musical experts tend to have a higher preference for individualizing moral foundations, Harm and Care. Elzė Mikalonytė and Vilius Dranseika (2020) compared intuitions on the individuation of musical works between musicians and non-musicians and found that although they tend to be similar, musicians’ intuitions are usually more pronounced. However, Mikalonytė and Dranseika (2022) found no notable differences between professional singers and orchestra musicians working in the opera and participants with no music education. Most of Richard Kamber’s (2011) study participants were art professionals or ‘art buffs’, so the study itself does not allow us to compare experts’ and non-experts’ responses. Kamber explains this methodological decision by stating that if there is a consensus between professional artists on what counts as art, philosophers are inclined to agree with professional artists.

In other branches of experimental philosophy, many studies rely on intuitions that arise from thought experiments. This is less so in experimental philosophy of art and aesthetics. Indeed, Cova and Réhault (2019b: 3) speculate that it is because intuitions play a much less prominent role in philosophical aesthetics that the field did not draw the initial attention of experimental philosophers. With that said, it is important to stress that there is variation with respect to this within this branch of experimental philosophy too. Emanuele Arielli (2018) distinguishes studies that solicit intuitions and other cognitive responses and studies that solicit aesthetic reactions and other perceptual and phenomenological responses. While critical of the former type of studies, he finds the latter type of studies more promising insofar as they are more continuous with empirical aesthetics in psychology.

Others have remarked on this difference between experimental philosophy of art and aesthetics and other branches of experimental philosophy. Clotilde Torregrossa (2020, 2024) argues that insofar as experimental philosophy of art and aesthetics is more reliant on reactions to aesthetic phenomena, standard objections against experimental philosophy that turn on the reliance on intuitions from thought experiments are less applicable. Jonathan Weinberg (2019) argues that the availability of artworks means that experimenters need not rely solely on descriptive vignettes. The presentation of actual artworks can fill in gaps that are usually left by the short textual vignettes that are typical of philosophical thought experiments. We should note, however, that in actuality such studies remain relatively rare (some examples are Kamber 2011; Meskin et al. 2013; Liao & Meskin 2017; Bonard 2019; Puy 2022; Mikalonytė & Canonne forthcoming).

There is ongoing debate about whether studies about music should depend on the use of acoustic stimuli. Building on Weinberg’s argument, Nemesio Puy (2022) contends that ontological judgments about artworks involve an aesthetic dimension and must therefore be grounded in the experience of real works of art. This contention receives indirect support from a study that shows people are generally unwilling to base their beliefs about aesthetic dimension of an artwork on testimony alone, without first-personal perceptual access (Andow 2019). Moreover, this contention receives direct support from two studies that show that the decision to include or exclude acoustic stimuli has an effect on the results of studies investigating ontological judgments, even if the descriptive part of the stimuli is kept as consistent as possible (Puy 2022; Mikalonytė & Canonne forthcoming). Notwithstanding this, Elzė Mikalonytė (forthcoming) points out several reasons why purely textual vignettes are so widely used and might not always be easily replaceable. Such vignettes might help the participants to focus on the most relevant aspects and filter out irrelevant factors. In fact, additional perceptual information may actually distract participants, as may occur, for example, in making judgments in the ontology of art, which arguably should be made on the basis of conceptual rather than perceptual information (such as information about the artist’s intentions). Especially in the case of music, presenting the participants with short descriptions without corresponding works of music might help to avoid relying on sustained attention over extended periods of time.

. Morality
When philosophers engage in moral theorizing, what is it that they are doing? Very broadly, they are attempting to provide a systematic account of morality. Thus, the object of moral theorizing is morality, and, further, morality as a normative system.

At the most minimal, morality is a set of norms and principles that govern our actions with respect to each other and which are taken to have a special kind of weight or authority (Strawson 1961). More fundamentally, we can also think of morality as consisting of moral reasons, either grounded in some more basic value, or, the other way around, grounding value (Raz 1999).

It is common, also, to hold that moral norms are universal in the sense that they apply to and bind everyone in similar circumstances. The principles expressing these norms are also thought to be general, rather than specific, in that they are formulable “without the use of what would be intuitively recognized as proper names, or rigged definite descriptions” (Rawls 1979, 131). They are also commonly held to be impartial, in holding everyone to count equally.

1.1 Common-sense Morality
… Common-sense is… an exercise of the judgment unaided by any Art or system of rules : such an exercise as we must necessarily employ in numberless cases of daily occurrence ; in which, having no established principles to guide us … we must needs act on the best extemporaneous conjectures we can form. He who is eminently skillful in doing this, is said to possess a superior degree of Common-Sense. (Richard Whatley, Elements of Logic, 1851, xi–xii)
“Common-Sense Morality”, as the term is used here, refers to our pre-theoretic set of moral judgments or intuitions or principles.[1] When we engage in theory construction (see below) it is these common-sense intuitions that provide a touchstone to theory evaluation. Henry Sidgwick believed that the principles of Common-Sense Morality were important in helping us understand the “first” principle or principles of morality.[2] Indeed, some theory construction explicitly appeals to puzzles in common-sense morality that need resolution – and hence, need to be addressed theoretically.

Features of commons sense morality are determined by our normal reactions to cases which in turn suggest certain normative principles or insights. For example, one feature of common-sense morality that is often remarked upon is the self/other asymmetry in morality, which manifests itself in a variety of ways in our intuitive reactions. For example, many intuitively differentiate morality from prudence in holding that morality concerns our interactions with others, whereas prudence is concerned with the well-being of the individual, from that individual’s point of view.

Also, according to our common-sense intuitions we are allowed to pursue our own important projects even if such pursuit is not “optimific” from the impartial point of view (Slote 1985). It is also considered permissible, and even admirable, for an agent to sacrifice her own good for the sake of another even though that is not optimific. However, it is impermissible, and outrageous, for an agent to similarly sacrifice the well-being of another under the same circumstances. Samuel Scheffler argued for a view in which consequentialism is altered to include agent-centered prerogatives, that is, prerogatives to not act so as to maximize the good (Scheffler 1982).

Our reactions to certain cases also seem to indicate a common-sense commitment to the moral significance of the distinction between intention and foresight, doing versus allowing, as well as the view that distance between agent and patient is morally relevant (Kamm 2007).

Philosophers writing in empirical moral psychology have been working to identify other features of common-sense morality, such as how prior moral evaluations influence how we attribute moral responsibility for actions (Alicke et. al. 2011; Knobe 2003).

What many ethicists agree upon is that common-sense is a bit of a mess. It is fairly easy to set up inconsistencies and tensions between common-sense commitments. The famous Trolley Problem thought experiments illustrate how situations which are structurally similar can elicit very different intuitions about what the morally right course of action would be (Foot 1975). We intuitively believe that it is worse to kill someone than to simply let the person die. And, indeed, we believe it is wrong to kill one person to save five others in the following scenario:

David is a great transplant surgeon. Five of his patients need new parts—one needs a heart, the others need, respectively, liver, stomach, spleen, and spinal cord—but all are of the same, relatively rare, blood-type. By chance, David learns of a healthy specimen with that very blood-type. David can take the healthy specimen's parts, killing him, and install them in his patients, saving them. Or he can refrain from taking the healthy specimen's parts, letting his patients die. (Thomson 1976, 206)
And yet, in the following scenario we intuitively view it entirely permissible, and possibly even obligatory, to kill one to save five:

Edward is the driver of a trolley, whose brakes have just failed. On the track ahead of him are five people; the banks are so steep that they will not be able to get off the track in time. The track has a spur leading off to the right, and Edward can turn the trolley onto it. Unfortunately there is one person on the right-hand track. Edward can turn the trolley, killing the one; or he can refrain from turning the trolley, killing the five. (Thomson 1976, 206).
Theorizing is supposed to help resolve those tensions in a principled way. Theory construction attempts to provide guidance in how to resolve such tensions and how to understand them.

1.2 Contrasts Between Morality and Other Normative Domains
1.2.1 Morality and Ethics
Ethics is generally understood to be the study of “living well as a human being”. This is the topic of works such as Aristotle’s Nicomachean Ethics, in which the aim of human beings is to exemplify human excellence of character. The sense in which we understand it here is that ethics is broader than morality, and includes considerations of personal development of oneself and loved ones. This personal development is important to a life well lived, intuitively, since our very identities are centered on projects that we find important. Bernard Williams and others refer to these projects as “ground projects”. These are the sources of many of our reasons for acting. For Williams, if an agent seeks to adopt moral considerations, or be guided by them, then important ethical considerations are neglected, such as personal integrity and authenticity (Williams 1977; Wolf 1982). However, Williams has a very narrow view of what he famously termed “the morality system” (Williams 1985).

Williams lists a variety of objectionable features of the morality system, including the inescapability of moral obligations, the overridingness of moral obligation, impartiality, and the fact that in the morality system there is a push towards generalization.

There has been considerable discussion of each of these features of the morality system, and since Williams, a great deal of work on the part of standard moral theorists on how each theory addresses the considerations he raised. Williams’ critique of the morality system was part of a general criticism of moral theory in the 1980s on the grounds of its uselessness, harmfulness, and even its impossibility (Clarke 1987). This anti-theory trend was prompted by the same dissatisfaction with consequentialism and deontology that led to the resurgence of Virtue Ethics.

A major criticism of this view is that it has a very narrow view of what counts as a moral theory. Thus, some of these approaches simply rejected some features of William’s characterization of the morality system, such as impartiality. Others, however, Williams’ included, attacked the very project of moral theory. This is the ‘anti-theory’ attack on moral theorizing. For example, Annette Baier argued that morality cannot be captured in a system of rules, and this was a very popular theme amongst early virtue ethicists. On this view, moral theory which systematizes and states the moral principles that ought to guide actions is simply impossible: “Norms in the form of virtues may be essentially imprecise in some crucial ways, may be mutually referential, but not hierarchically orderable, may be essentially self-referential” (Baier 220).

Robert Louden even argued that the best construal of virtue ethics is not as an ethical theory, but as anti-theory that should not be evaluated as attempting to theorize morality at all. (Louden 1990). According to Louden, moral theories are formulated to a variety of reasons, including to provide solutions to problems, formulas for action, universal principles, etc. Louden notes that this characterization is very narrow and many would object to it, but he views anti-theory not so much as a position against any kind of moral theorizing, but simply the kind that he viewed as predominant prior to the advent of Virtue Ethics. This is a much less severe version of anti-theory as it, for example, doesn’t seem to regard weightiness or importance of moral reasons as a problem.

Some of the problems that Williams and other anti-theorists have posed for morality, based on the above characteristics, are:

Morality is too demanding and pervasive: that is, the view that moral reasons are weighty indicates that we should be giving them priority over other sorts of reasons. Further, they leach into all aspects of our lives, leaving very little morally neutral.

Morality is alienating. There are a variety of ways in which morality can be alienating. As Adrian Piper notes, morality might alienate the agent from herself or might alienate the agent from others – impartiality and universality might lead to this, for example (Piper 1987; Stocker 1976). Another way we can understand alienation is that the agent is alienated from the true justifications of her own actions – this is one way to hold that theories which opt for indirection can lead to alienation (see section 4 below).

Morality, because it is impartial, makes no room for special obligations. That is, if the right action is the one that is impartial between persons, then it does not favor the near and dear. On this picture it is difficult to account for the moral requirements that parents have towards their own children, and friends have towards each other. These requirements are, by their nature, not impartial.

Morality is committed to providing guides for action that can be captured in a set of rules or general principles. That is, morality is codifiable and the rules of morality are general.

Morality requires too much. The basic worry is that the morality system is voracious and is creeping into all aspects of our lives, to the detriment of other important values. The worry expressed by 4 takes a variety of forms. For example, some take issue with a presupposition of 4, arguing that there are no moral principles at all if we think of these principles as guiding action. Some argue that there are no moral principles that are complete, because morality is not something that is codifiable. And, even if morality was codifiable, the ‘principles’ would be extremely specific, and not qualify as principles at all.

Since Williams’ work, philosophers have tried to respond to the alienation worry by, for example, providing accounts of the ways in which a person’s reasons can guide without forming an explicit part of practical deliberation. Peter Railton, for example, argues in favor of a form of objective consequentialism, Sophisticated Consequentialism, in which the rightness of an action is a function of its actual consequences (Railton 1984). On Railton’s view, one can be a good consequentialist without being alienated from loved ones. Though not attempting to defend moral theory per se, other writers have also provided accounts of how agents can act on the basis of reasons – and thus perform morally worthy actions, even though these reasons are not explicitly articulated in their practical deliberations (Arpaly 2002; Markovits 2014). Deontologists have argued that autonomous action needn’t involve explicit invocation of, for example, the Categorical Imperative (Herman 1985). Generally, what characterizes these moves is the idea that the justifying reasons are present in some form in the agent’s psychology – they are recoverable from the agent’s psychology – but need not be explicitly articulated or invoked by the agent in acting rightly.

One way to elaborate on this strategy is to argue that the morally good agent is one who responds to the right sorts of reasons, even though the agent can’t articulate the nature of the response (Arpaly 2002). This strategy makes no appeal to codifiable principles, and is compatible with a wide variety of approaches to developing a moral theory. It relies heavily on the concept, of course, of “reason” and “moral reason,” which many writers on moral issues take to be fundamental or basic in any case.

There has also been debate concerning the proper scope of morality, and how moral theories can address problems relating to impartiality. Kant and the classical utilitarians believed that moral reasons are impartial, what others have termed agent-neutral. Indeed, this is one point of criticism that virtue ethics has made of these two theories. One might argue that moral reasons are impartial, but that there are other reasons that successfully compete with them – reasons relating to the near and dear, for example, or one’s own ground projects. Or, one could hold that morality includes special reasons, arising from special obligations, that also morally justify our actions.

The first strategy has been pursued by Bernard Williams and other “anti-theorists”. Again, Williams argues that morality is a special system that we would be better off without (Williams 1985). In the morality system we see a special sense of “obligation” – moral obligation – which possesses certain features. For example, moral obligation is inescapable according to the morality system. A theory such as Kant’s, for example, holds that we must act in accordance with the Categorical Imperative. It is not optional. This is because morality is represented as having authority over us in ways that even demand sacrifice of our personal projects, of the very things that make our lives go well for us. This seems especially clear for Utilitarianism, which holds that we must maximize the good, and falling short of maximization is wrong. A Kantian will try to avoid this problem by appealing to obligations that are less demanding, the imperfect ones. But, as Williams points out, these are still obligations, and as such can only be overridden by other obligations. Thus, the theories also tend to present morality as pervasive in that morality creeps into every aspect of our lives, making no room for neutral decisions. For example, even decisions about what shoes to wear to work becomes a moral one:

Once the journey into more general obligations has started, we may begin to get into trouble – not just philosophical trouble, but the conscience trouble – with finding room for morally indifferent actions. I have already mentioned the possible moral conclusion that one may take some particular course of action. That means that there is nothing else I am obliged to do. But if we have accepted general and indeterminate obligations to further various moral objectives…they will be waiting to provide work for idle hands… (Williams 1985, 181)
He goes on to write that in order to get out of this problem, “…I shall need one of those fraudulent items, a duty to myself” (Williams 1985, 182). Kantian Ethics does supply this. Many find this counterintuitive, since the self/other asymmetry seems to capture the prudence/morality distinction, but Kantians such as Tom Hill, jr. have made strong cases for at least some moral duties to the self. In any case, for writers such as Williams, so much the worse for morality.

Other writers, also concerned about the problems that Williams has raised argue, instead, that morality does make room for our partial concerns and projects, such as the norms governing our relationships, and our meaningful projects. Virtue ethicists, for example, are often comfortable pointing out that morality is not thoroughly impartial because there are virtues of partiality. Being a good mother involves having a preference for the well-being of one’s own children. The mother who really is impartial would be a very bad mother, lacking in the appropriate virtues.

Another option is to hold that there are partial norms, but those partial norms are themselves justified on impartial grounds. This can be spelled out in a variety of different ways. Consider Marcia Baron’s defense of impartiality, where she notes that critics of impartiality are mistaken because they confuse levels of justification: “Critics suppose that impartialists insisting on impartiality at the level of rules or principles are committed to insisting on impartiality at the level of deciding what to do in one’s day-to-day activities” (Baron 1991). This is a mistake because impartialists can justify partial norms by appealing to impartial rules or principles. She is correct about this. Even Jeremy Bentham believed, for example, that the principle of utility ought not be applied in every case, though he mainly appealed to efficiency costs of using the principle all the time. But one can appeal to other considerations. Frank Jackson uses an analogy with predators to argue that partial norms are strategies for maximizing the good, they offer the best chance of actually doing so given our limitations (Jackson 1991). Similarly, a Kantian such as Tom Hill, jr., as Baron notes, can argue that impartiality is part of an ideal, and ought not govern our day-to-day lives (Hill 1987). Does this alienate people from others? The typical mother shows the right amount of preference for her child, let’s say, but doesn’t herself think that this is justified on the basis of promoting the good, for example. A friend visits another in the hospital and also does not view the partiality as justified by any further principles. But this is no more alienating than someone being able to make good arguments and criticize bad ones without a knowledge of inference rules. Maybe it is better to have an awareness of the underlying justification, but for some theories even that is debatable. For an objective theorist (see below) it may be that knowing the underlying justification can interfere with doing the right thing, in which case it is better not to know. For some theorists, however, such as neo-Aristotelian virtue ethicists, a person is not truly virtuous without such knowledge and understanding, though Rosalind Hursthouse (1999) does not make this a requirement of right action.

Recently consequentialists have been approaching this issue through the theory of value itself, arguing that there are agent-relative forms of value. This approach is able to explain the intuitions that support partial moral norms while retaining the general structure of consequentialism (Sen 2000). Douglas Portmore, for example, argues for a form of consequentialism that he terms “commonsense consequentialism” as it is able to accommodate many of our everyday moral intuitions (Portmore 2011). He does so by arguing that (1) the deontic status of an act, whether it is right or wrong, is determined by what reasons the agent has for performing it – if an agent has a decisive reason to perform the act in question, then it is morally required. Combined with (2) a teleological view of practical reasons in which our reasons for performing an action are a function of what we have reason to prefer or desire we are led to a form of act-consequentialism but one which is open to accepting that we have reason to prefer or desire the well-being of the near and dear over others.

Though much of this is controversial, there is general agreement that moral reasons are weighty, are not egoistic – that is, to be contrasted with prudential reasons, and are concerned with issues of value [duty, fittingness].

1.2.2. Morality and Aesthetics
Moral modes of evaluation are distinct from the aesthetic in terms of their content, but also in terms of their authority. So, for example, works of art are evaluated as “beautiful” or “ugly”, and those evaluations are not generally considered as universal or as objective as moral evaluations. These distinctions between moral evaluation and aesthetic evaluation have been challenged, and are the subject of some interesting debates in metaethics on the nature of both moral and aesthetic norms and the truth-conditions of moral and aesthetic claims. But, considered intuitively, aesthetics seems at least less objective than morality.

A number of writers have noted that we need to be cognizant of the distinction between moral norms and the norms specific to other normative areas in order to avoid fallacies of evaluation, and much discussion has centered on a problem in aesthetics termed the “Moralistic Fallacy” (D’Arms and Jacobson 2000).

One challenge that the anti-theorists have raised for morality was to note that in a person’s life there will be certain norm clashes – including clashes between types of norms such as the moral and the aesthetic. It is giving too much prominence to the moral that judges a person’s life as going well relative to the fulfillment or respect of those norms. Can’t a human life go well, even when that life sacrifices morality for aesthetics?

This sort of debate has a long history in moral theory. For example, it arose as a form of criticism of G. E. Moore’s Ideal Utilitarianism, which treated beauty as an intrinsic good, and rendering trade-offs between behaving well towards others and creating beauty at least in principle justified morally (Moore 1903). But the anti-theorists do not pursue this method of accommodating the aesthetic, instead arguing that it is a separate normative realm which has its own weight and significance in human flourishing.

2. Theory and Theoretical Virtues
There is agreement that theories play some kind of systematizing role, and that one function is to examine important concepts relevant to morality and moral practice and the connections, if any, between them. For example, one very common view in the middle of the 20th century, attributed to John Rawls, was to view moral theory as primarily interested in understanding the ‘right’ and the ‘good’ and connections between the two (Rawls). Priority claims are often a central feature in the systematizing role of moral theory. Related to this is the issue of explanatory, or theoretical, depth. That is, the deeper the explanation goes, the better.

Theories also strive for simplicity, coherence, and accuracy. The fewer epicycles the theory has to postulate the better, the parts of the theory should fit well together. For example, the theory should not contain inconsistent principles, or have inconsistent implications. The theory should cover the phenomena in question. In the case of moral theories, the phenomena in question are thought to be our considered moral intuitions or judgements. Another coherence condition involves the theory cohering with a person’s set of considered judgments, as well.

One last feature that needs stressing, particularly for moral theories, is applicability. One criticism of some normative ethical theories is that they are not applicable. For example, Virtue Ethics has been criticized for not providing an account of what our moral obligations are – appealing to what the virtuous person would do in the circumstances would seem to set a very high bar or doesn’t answer the relevant question about how we should structure laws guiding people on what their social obligations are. Similarly, objective consequentialists, who understand “right action” in terms of actual consequences have been criticized for rendering what counts as a right action in a given circumstance unknowable, and thus useless as a guide to action. Both approaches provide responses to this worry, but this supports the claim that a desideratum of a moral theory is that it be applicable.

2.1 The Tasks of Moral Theory
One task (though this is somewhat controversial) of a moral theory is to give an account of right actions. Often, this will involve an explication of what counts as good – some theories then get spelled out in terms of how they approach the good, by maximizing it, producing enough of it, honoring it, etc. In addition, some theories explicate the right in terms of acting in accordance with one’s duties, or acting as a virtuous person would act. In these cases the notions of ‘duty’ and ‘virtue’ become important to the overall analysis, and one function of moral theory is to explore the systematic connections between duty or virtue and the right and the good.

Moral theories also have both substantive and formal aims. Moral theories try to provide criteria for judging actions. It might be that the criterion is simple, such as right actions maximize the good, or it may be complex, such as the right action is the one that gives adequate weight to each competing duty. Sometimes, in recognition that there is not always “the” right action, the theory simply provides an account of wrongness, or permissibility and impermissibility, which allows that a range of actions might count as “right”.

In addition to simply providing criteria for right or virtuous action, or for being a virtuous person, a given moral theory, for example, will attempt to explain why something, like an action or character trait, has a particular moral quality, such as rightness or virtuousness. Some theories view rightness as grounded in or explained by value. Some view rightness as a matter of reasons that are prior to value. In each case, to provide an explanation of the property of ‘rightness’ or ‘virtuousness’ will be to provide an account of what the grounding value is, or an account of reasons for action.

In addition, moral theories may also provide decision-procedures to employ in determining how to act rightly or virtuously, conditions on being good or virtuous, or conditions on morally appropriate practical deliberation. Thus, the theory provides substance to evaluation and reasons. However, moral theories, in virtue of providing an explanatory framework, help us see connections between criteria and decision-procedures, as well as provide other forms of systemization. Thus, moral theories will be themselves evaluated according to their theoretical virtues: simplicity, explanatory power, elegance, etc. To evaluate moral theories as theories, each needs to be evaluated in terms of how well it succeeds in achieving these theoretical goals.

There are many more specialized elements to moral theories as well. For example, a moral theory often concerns itself with features of moral psychology relevant to action and character, such as motives, intentions, emotions, and reasons responsiveness. A moral theory that incorporates consideration of consequences into the determination of moral quality, will also be concerned with issues surrounding the proper aggregation of those consequences, and the scope of the consequences to be considered.

2.2 Theory Construction
There’s been a long history of comparing moral theories to other sorts of theories, such as scientific ones. For example, in meta-ethics one issue has to do with the nature of moral “evidence” on analogy with scientific evidence. On what Ronald Dworkin terms the “natural model” the truths of morality are discovered, just as the truths of science are (Dworkin 1977, 160). It is our considered intuitions that provide the clues to discover these moral truths, just as what is observable to us provides the evidence to discover scientific truths. He compared this model with the “constructive model” in which the intuitions themselves are features of the theory being constructed and are not analogous to observations of the external world.

Yet, even if we decide that morality lacks the same type of phenomena to be accounted for as science, morality clearly figures into our normative judgments and reactions. One might view these – our intuitions about moral cases, for example – to provide the basic data that needs to be accounted for by a theory on either model.

One way to “account for” our considered intuitions would be to debunk them. There is a long tradition of this in moral philosophy as well. When scholars provided genealogies of morality that explained our considered intuitions in terms of social or evolutionary forces that are not sensitive the truth, for example, they were debunking morality by undercutting the authority of our intuitions to provide insight into it (Nietzsche 1887 [1998], Joyce 2001, Street 2006). In this entry, however, we consider the ways in which moral theorists have constructed their accounts by taking the intuitions seriously as something to be systematized, explained, and as something that can be applied to generate the correct moral decisions or outcomes.

Along these lines, one method used in theory construction would involve the use of reflective equilibrium and inference to the best explanation. For example, one might notice an apparent inconsistency in moral judgements regarding two structurally similar cases and then try to figure out what principle or set of principles would achieve consistency between them. In this case, the theorist is trying to figure out what best explains both of those intuitions. But one also might, after thinking about principles one already accepts, or finds plausible, reject one of those intuitions on the basis of it not cohering with the rest of one’s considered views. But full theory construction will go beyond this because of the fully theoretical virtues discussed earlier. We want a systematic account that coheres well not only with itself, but with other things that we believe on the basis of good evidence.

3. Criteria
Consider the following:

Malory has promised to take Chris grocery shopping. Unfortunately, as Malory is leaving the apartment, Sam calls with an urgent request: please come over to my house right now, my pipes have broken and I need help! Torn, Malory decides to help Sam, and thus breaks a promise to Chris.
Has Malory done the right thing? The virtuous thing? Malory has broken a promise, which is pro tanto wrong, but Sam is in an emergency and needs help right away. Even if it is clear that what Malory did was right in the circumstances, it is an interesting question as to why it is right. What can we appeal to in making these sorts of judgments? This brings to light the issue of how one morally justifies one’s actions. This is the task of understanding what the justifying reasons are for our actions. What makes an action the thing to do in the circumstances? This is the criterion of rightness (or wrongness). We will focus on the criterion of rightness, though the criterion issue comes up with other modes of moral evaluation, such as judging an action to be virtuous, or judging it to be good in some respect, even if not right. Indeed, some writers have argued that ‘morally right’ should be jettisoned from modern secular ethics, as it presupposes a conceptual framework left over from religiously based accounts which assume there is a God (Anscombe 1958). We will leave these worries aside for now, however, and focus on standard accounts of criteria.

The following are some toy examples that exhibit differing structural features for moral theories and set out different criteria:

Consequentialism.
The right action is the action that produces good amongst the options open to the agent at the time of action (Singer). The most well-known version of this theory is Classical Utilitarianism, which holds that the right action promotes pleasure (Mill).

Kantian Deontology.
The morally worthy action is in accordance with the Categorical Imperative, which requires an agent refrain from acting in a way that fails to respect the rational nature of other persons (Kant).

Rossian Deontology.
The right action is the action that best accords with the fulfillment and/or non-violation of one’s prima facie duties (Ross).

Contractualism.
An action is morally wrong if it is an act that would be forbidden by principles that rational persons could not reasonably reject (Scanlon).

Virtue Ethics.
The right action is the action that a virtuous person would characteristically perform in the circumstances (Hursthouse 1999).

These principles set out the criterion or standard for evaluation of actions. They do not necessarily tell us how to perform right actions, and are not, in themselves, decision-procedures, though they can easily be turned into decision procedures, such as: you ought to try to perform the action that maximizes the good amongst the options available to you at the time of action. This might not be, and in ordinary circumstance probably isn’t, a very good decision-procedure, and would itself need to be evaluated according to the criterion set out by the theory.

These theories can be divided, roughly, into the deontological, consequentialist, and virtue ethical categories. There has been a lively debate about how, exactly, to delineate these categories. Some have held that deontological theories were just those theories that were not consequentialist. A popular conception of consequentialist theories is that they are reductionist in a particular way – that is, in virtue of reducing deontic features of actions (e.g. rightness, obligatoriness) to facts about an agent’s options and the consequences of those options (Smith 2009). If that is the case, then it seems that deontological approaches are just the ones that are not reductive in this manner. However, this fails to capture the distinctive features of many forms of virtue ethics, which are neither consequentialist nor necessarily concerned with what we ought to do, our duties as opposed to what sorts of persons we should be.

One way to distinguish consequentialist from deontological theories is in terms of how each approaches value. Philip Pettit has suggested that while consequentialist theories required promotion of value, deontological theories recommend that value be honored or respected. On each of these views, value is an important component of the theory, and theories will be partially delineated according to their theory of value. A utilitarian such as Jeremey Bentham believes that hedonism is the correct theory of value, whereas someone such as G. E. Moore, a utilitarian but a pluralist regarding value, believes that hedonism is much too narrow an account. A Kantian, on the other hand, views value as grounded in rational nature, in a will conforming to the Categorical Imperative.

Because of the systematizing function of moral theory discussed earlier, the simplest account is to be preferred and thus there is a move away from endorsing value pluralism. Of course, as intuitive pressure is put on each of the simpler alternatives, a pluralistic account of criteria for rightness and wrongness has the advantage of according best with moral intuitions.

Reasons-first philosophers will delineate the theories somewhat differently. For example, one might understand goodness as a matter of what we have reason to desire, in which case what we have reason to desire is prior to goodness rather than the other way around. Value is still an important component of the theories, it is simply that the value is grounded in reasons.

Another distinction between normative theories is that between subjective and objective versions of a type of theory. This distinction cuts across other categories. For example, there are subjective forms of all the major moral theories, and objective versions of many. An objective standard of right holds that the agent must actually meet the standard – and meeting the standard is something ‘objective’, not dependent on the agent’s psychological states – in order to count as right or virtuous. Subjective standards come in two broad forms:

Psychology sensitive: are the justifying reasons part of the agent’s deliberative processes? Or, more weakly, are they “recoverable” from the agent’s psychology [perhaps, for example, the agent has a commitment to the values that provide the reasons].
Evidence sensitive: the right action isn’t the one that actually meets the standard, but instead, is the action that the agent could foresee would meet that standard. [there are many different ways to spell this out, depending on the degree of evidence that is relevant: in terms of what the agent actually foresees, what is foreseeable by the agent given what the agent knows, is foreseeable by someone in possession of a reasonable amount of evidence, etc.]
Of course, these two can overlap. For theorists who are evaluational internalists, evidence-sensitivity doesn’t seem like a plausible way of spelling out the standard, except, perhaps, indirectly. The distinction frequently comes up in Consequentialism, where the Objective standard is taken to be something like: the right action is the action that actually promotes the good and the Subjective standard is something like: the right action is the action that promotes the good by the agent’s own lights (psychology sensitive) or the right action is the action that promotes the foreseeable good, given evidence available at the time of action (evidence sensitive standard). It is certainly possible for other moral standards to be objective. For example, the right action is the action that the virtuous person would perform, even though the agent does not realize it is what the virtuous agent would do in the circumstances, and even if the person with the best available evidence couldn’t realize it is what the virtuous person would do in the circumstances.

We certainly utter locutions that support both subjective and objective uses of what we ‘ought’ to do, or what is ‘right’. Frank Jackson notes this when he writes:

…we have no alternative but to recognize a whole range of oughts – what she ought to do by the light of her beliefs at the time of action, …what she ought to do by the lights of one or another onlooker who has different information on the subject, and, what is more, what she ought to do by God’s lights…that is, by the lights of one who knows what will and would happen for each and every course of action. (Jackson 1991, 471).
For Jackson, the primary ought, the primary sense of ‘rightness’ for an action, is the one that is “most immediately relevant to action” since, otherwise, we have a problem of understanding how the action is the agent’s. Thus, the subjective ‘ought’ is primary in the sense that this is the one that ethical theory should be concerned with (Jackson 1991). Each type of theorist makes use of our ordinary language intuitions to make their case. But one desideratum of a theory is that it not simply reflect those intuitions, but also provides the tools to critically analyze them. Given that our language allows for both sorts of ‘ought,’ the interesting issue becomes which, if either, has primacy in terms of actually providing the standard by which other things are evaluated? Moral theory needn’t only be concerned with what the right action is from the agent’s point of view.

There are three possibilities:

neither has primacy
the subjective has primacy
the objective has primacy
First off we need to understand what we mean by “primacy”. Again, for Frank Jackson, the primary sense of ‘right’ or ‘ought’ is subjective, since what we care about is the ‘right’ that refers to an inward story, the story of our agency, so to speak. On this view, the objective and subjective senses may have no relationship to each other at all, and which counts as primary simply depends upon our interests. However, the issue that concerns us here is whether or not one sense can be accounted for in terms of the other. Option 1 holds that there is no explanatory connection. That is not as theoretically satisfying. Option 2 holds either there really is no meaningful objective sense, just the subjective sense, or the objective sense is understood in terms of the subjective.

Let’s look at the objective locution again “He did the right thing, but he didn’t know it at the time (or he had no way of knowing it at the time)”. Perhaps all this means is “He did what someone with all the facts and correct set of values would have judged right by their own lights” – this would be extensionally the same as “He performed the action with the best actual consequences”. This is certainly a possible account of what objective right means which makes use of a subjective standard. But it violates the spirit of the subjective standard, since it ties rightness neither to the psychology of the agent, or the evidence that is actually available to the agent. For that reason, it seems more natural to opt for 3. An advantage of this option is that gives us a nice, unified account regarding the connection between the objective and the subjective. Subjective standards, then, are standards of praise and blame, which are themselves evaluable according to the objective standard. Over time, people are in a position to tell whether or not a standard actually works in a given type of context. Or, perhaps it turns out that there are several standards of blame that differ in terms of severity. For example, if someone acts negligently a sensible case can be made that the person is blameworthy but not as blameworthy as if they had acted intentionally.

As to the worry that the objective standard doesn’t provide action guidance, the objective theorist can hold that action guidance is provided by the subjective standards of praise/blameworthiness. Further, the standard itself can provide what we need for action guidance through normative review (Driver 2012). Normative review is a retrospective look at what does in fact meet the standard, and under what circumstances.

Now, consider a virtue ethical example. The right action is the action that is the actual action that a virtuous person would perform characteristically, in the circumstances, rather than the action that the agent believes is the one the virtuous person would perform. Then we evaluate an agent’s “v-rules” in terms of how close they meet the virtuous ideal.

4. Decision Procedures and Practical Deliberation
Another function of moral theory is to provide a decision procedure for people to follow so as to best insure they perform right actions. Indeed, some writers, such as R. M. Hare hold action guidance to be the function of the moral principles of the theory (Hare 1965). This raises the question of what considerations are relevant to the content of such principles – for example, should the principles be formulated taking into account the epistemic limitations of most human beings? The requirement that moral principles be action guiding is what Holly Smith terms the “Useability Demand”: “…an acceptable moral principle must be useable for guiding moral decisions…” (Smith 2020, 11). Smith enumerates different forms satisfaction of this demand can take, and notes that how one spells out a principle in order to meet the demand will depend upon how the moral theorist views moral success. For example, whether or not success is achieved in virtue of simply making the right decision or if, in addition to making the right decision, the agent must also have successful follow-through on that decision.

There has been enormous debate on the issue of what is involved in following a rule or principle, and some skepticism that this is in fact what we are doing when we take ourselves to be following a rule. (Kripke 1982) Some virtue theorists believe that it is moral perception that actually does the guiding, and that a virtuous person is able to perceive what is morally relevant and act accordingly (McDowell 1979).

As discussed earlier in the section on criteria, however, this is also controversial in that some theorists believe that decision procedures themselves are not of fundamental significance. Again, objective consequentialist who believes that the fundamental task of theory is to establish a criterion for right argues that decision procedures will themselves be established and evaluated on the basis of how well they get us to actually achieving the right. Thus, the decision-procedures are derivative. Others, such as subjective consequentialists, will argue that the decision-procedures specify the criterion in the sense that following the decision-procedure itself is sufficient for meeting the criterion. For example, an objective consequentialist will hold that the right action maximizes the good, whereas the subjective consequentialist might hold that the right action is to try to maximize the good, whether or not one actually achieves it (Mason 2003 and 2019). Following the decision-procedure itself, then, is the criterion.

The distinction between criterion and decision-procedure has been acknowledged and discussed at least since Sidgwick, though it was also mentioned by earlier ethicists. This distinction allows ethical theories to avoid wildly implausible implications. For example, if the standard that the theory recommends is ‘promote the good’ it would be a mistake to think that ‘promote the good’ needs to be part of the agent’s deliberation. The consequentialist might say that, instead, it is an empirical issue as to what the theory is going to recommend as a decision-procedure, and that recommendation could vary from context to context. There will surely be circumstances in which it would be best to think in terms of meeting the standard itself, but again that is an empirical issue. Likewise, it is open to a Virtue Ethicist to hold that the right action is the one the virtuous agent would perform in the circumstances, but also hold that the agent’s deliberative processes need not make reference to the standard. Pretty much all theories will want to make some space between the standard and the decision-procedure in order to avoid a requirement that agent’s must think in terms of the correct standard, in order to act rightly, or even act with moral worth. There is a distinction to be made between doing the right thing, and doing the right thing for the right reasons. Doing the right thing for the right reasons makes the action a morally worthy one, as it exhibits a good quality of the will. It is possible for a theory to hold that the ‘good will’ is one that understands the underlying justification of an action, but that seems overly demanding. If consequentialism is the correct theory, then demanding that people must explicitly act intentionally to maximize the good would result in fewer morally worthy actions than seems plausible. The ‘for the right reasons’ must be understood as allowing for no explicit invocation of the true justifying standard.

This has led to the development of theories that advocate indirection. First, we need to distinguish two ways that indirection figures into moral philosophy.

Indirection in evaluation of right action.
Indirection in that the theory does not necessarily advocate the necessity of aiming for the right action.
To use Utilitarianism as an example again, Rule Utilitarianism is an example of the first sort of indirection (Hooker 2000), Sophisticated Consequentialism is an example of the second sort of indirection (Railton 1984). One might hold that some versions of Aristotelian Virtue ethics, such as Rosalind Hursthouse’s version, also are of the first type, since right action is understood in terms of virtue. One could imagine an indirect consequentialist view with a similar structure: the right action is the action that the virtuous person would perform, where virtue is understood as a trait conducive to the good, instead of by appeal to an Aristotelian notion of human flourishing.

The second sort relies on the standard/decision-procedure distinction. Railton argues that personal relationships are good for people, and explicitly trying to maximize the good is not a part of our relationship norms, so it is likely good that we develop dispositions to focus on and pay special attention to our loved ones. The account is open to the possibility that people who don’t believe in consequentialism have another way of deciding how to act that is correlated with promotion of the good. If the criteria a theory sets out need not be fulfilled by the agent guiding herself with the reasons set out by the criteria, then it is termed self-effacing. When a theory is self-effacing, it has the problem of alienating a person from the justification of her own actions. A middle ground, which is closer to Railton’s view, holds that the correct justification is a kind of “touchstone” to the morally good person – consulted periodically for self-regulation, but not taken explicitly into consideration in our ordinary, day-to-day lives. In this way, the theory would not be utterly self-effacing and the agent would still understand the moral basis for her own actions.

1. Background
Writers on aesthetics in the empiricist tradition such as Shaftsbury, Hume, and Reid thought of their contributions as broadly empirical (see Shelley 2006 [2020]), and among the very first experimental investigations in psychology were studies of aesthetic preferences and responses, for example Fechner’s attempt to discover whether the “golden section” is especially preferred to other ratios (1871). The late nineteenth and early twentieth centuries were rich in work of this kind, and there were more speculative studies of “embodied” responses to architectural structures, a debate that brought the term “empathy” to the English language (Vischer 1873; Lipps 1903). But for most of the twentieth century aesthetics was seen, by its proponents and its detractors, as an “armchair” project. Wittgenstein, whose conception of philosophy strongly influenced the field’s direction in the century’s latter half, said

There doesn’t seem to be any connection between what psychologists do and any judgement about a work of art. (1967, 19; on Wittgenstein’s own experimental work on the perception of rhythm see Guter 2020)

To the extent that aesthetics in this period was influenced by thinking about the mind it was more often prompted by ideas from psychoanalysis (Wollheim 1993). The last thirty years have seen a shift back towards empirical inquiry, assisted by a resurgence of interest in the imagination, now often treated as a capacity with an evolutionary and developmental history (Fuentes 2020; Harris 2000) and as subject to selective damage (Currie 1996).

It has never been plausible to think of aesthetics as wholly a priori. Even those who think its main business is the analysis of widely shared folk concepts (Dickie 1962) will agree that a philosopher of music should have a good deal of knowledge of the history, art making practices and traditions of criticism of some musical tradition. In this respect aesthetics is closer to the philosophy of physics or economics than it is to metaphysics. There is also a frequently acknowledged connection between aesthetics and the study of perception. When Baumgarten (1750–58) first introduced the term “aesthetics” he called it a “science of perception” and according to Nanay (2014: 101)

many, maybe even most traditional problems in aesthetics are in fact about philosophy of perception and can, as a result, be fruitfully addressed with the help of the conceptual apparatus of the philosophy of perception.

This is a stronger claim than many would make (see Margolis 1960; Sibley 1965; Schellekens 2006; and Robson 2018) but theses in aesthetics do sometimes depend on claims, including scientifically tractable claims, about the nature of perception. A notable example is the debate about whether perception is cognitively penetrable; if it is, the way a picture looks or a piece of music sounds may depend on what the observer knows about contextual factors (for the rejection of cognitive penetrability see Danto 2001; for criticism see Rose & Nanay 2022).

Philosophical doubts about the relevance of empirical work to aesthetics are of various kinds. Some authors merely suggest that philosophers are apt to make hasty generalisations from slender experimental evidence, give insufficient attention to their details and limitations, and fail to acknowledge the existence of conflicting expert opinion (Konečni 2013; Stock 2014). Philosophers, aestheticians among them, can certainly overestimate their expertise on occasion and it may well be that they are even more prone to do so when it comes to empirical matters. None of this points to anything in principle problematic about their enthusiasm for the empirical. Other doubters emphasise what they see as more systemic dangers. Roger Scruton (2014) suggests a tendency to try to assimilate aesthetic phenomena to available scientific frameworks that turn out to be ill fitting; he cites the attempt to use Chomskian linguistics to found a generative theory of music which, he claims, fails to accommodate either music’s structure or purpose. Gordon Graham (2014), contrasting Hume’s aesthetic theory unfavourably with that of Reid, says that the project of an “aesthetic science” must ignore or deny the role of reason and judgement. Again, one may be alive to both kinds of dangers while retaining an openness to empirical work. In particular, Graham’s challenge might be responded to by noting that studies of the role actually played by reasons in aesthetic judgement point to a sometimes striking disconnect between what people think of as their reasons and the factors that determine their preferences (Lopes 2014 and Irvin 2014; see also below, Section 3); this can hardly be irrelevant to someone investigating the rationality of aesthetic judgement. Further, work which aestheticians have thought of as conceptual analysis in a broad sense has often presumed answers to what are in fact empirical questions. Noel Carroll says that

The supposition that aesthetic properties are objective also explains better how we talk about them than does the projection theory. …people involved in disputes about aesthetic properties … speak as if one side of the disagreement is right and the other wrong. (Carroll 1999: 117)

Studies in the new field of “experimental philosophy” (for some overviews of this field, as applied to aesthetics, see Cova & Réhault 2018 and Torregrossa 2020) challenge this view. One study (Cova, Olivola, et al. 2019) gathered more than two thousand responses across 19 countries to an imagined disagreement about whether an object of aesthetic interest is beautiful. It found that in all populations the least popular view was that “someone is right and someone is wrong”. The study itself has come in for criticism (Zangwill 2018) but, regardless of its success, it highlights the general point that we should not assume without evidence that the philosopher’s understanding of folk concepts (or folk practice) matches that of the folk themselves: something that should be of concern to those, such as Dickie, who conceive of the work of aestheticians as largely involving the elucidation of folk concepts.

The question discussed above and to which Carroll and the members of the Cova team give such different answers is a higher order question, concerning whether people think of aesthetic judgements as having normative force. We can ask similar questions about lower level judgements. Consider the claim that we shouldn’t form aesthetic judgements on the basis of testimony, a claim said to be supported by our supposed practice of refraining from forming such judgements (Kant 1790 [2000: 9]; Nguyen 2020: 1127). Contrary to this, Robson (2014) argues that this descriptive claim about our judgement forming practice is mistaken—something which would, if true, undermine one significant motivation for the normative claim.

From an entirely different perspective, there are those who think that philosophers have very little role to play in answering key aesthetic questions and that we should seek to replace, rather than merely supplement, traditional philosophical methods with more scientific approaches. Exemplifying this tendency, Semir Zeki says

no theory of aesthetics that is not substantially based on the activity of the brain is ever likely to be complete, let alone profound,

going on to say that painting is an inquiry into the laws of the brain and that what pleases us is what pleases our brains (1999: 1–2). While attitudes such as this may not constitute a complete dismissal of philosophical aesthetics they show little awareness of its results, or understanding of the limitations of neurological studies. Facts about brain processes do not tell us—at least without some substantive and controversial bridging premises—what objects are art works, or what works have value (see Hyman 2010 for wide ranging criticism of this approach). The relations between empirical work and normative aesthetics will recur throughout this entry.

2. Bottom-up and Top-Down Approaches to Aesthetics
A good deal of empirical work on the aesthetic, while not deriving from hostility to philosophical ideas, adopts a “bottom up” approach, focusing on the immediate reactions of untutored subjects to simple stimuli. We have seen that, by contrast, philosophical aesthetics is much concerned with normative issues of judgement and value, and with the claimed dependence of these things on training, knowledge and what is sometimes called “taste”, But these empirical studies are not philosophically irrelevant; we should not assume that the aesthetic responses of experts are discontinuous with those of naïve subjects (though for a discussion of the burden of proof here see Williamson 2011), nor that expert judgements are sensitive only to the factors that experts themselves endorse. It is important to ask whether there are baseline aesthetic responses, perhaps invariant across cultures, from which the great variety of aesthetic traditions emerge, and which may impose limits on those traditions and their products (for a defence of this “bottom up” approach to experiments in aesthetic see McManus 2011). Wölfflin, a founder of modern art history, had an interest in questions framed in this way: he claimed, for example, that there is a general tendency to prefer a rightward placement of a significant object within an image (1928). Subsequent research suggests a more complex picture, with preference dependent on physical and cultural factors: handedness, and whether one reads/writes left-to-right or right-to-left (see Palmer, Gardner, & Wickens 2008 for references and discussion; Chahboun et al. 2017 found some limited support for the idea that placement preference depends on the subject’s direction of reading/writing). Other studies have shown robust preferences for certain colours (Palmer & Schloss 2010), shapes, positioning of objects within a frame (Palmer, Gardner, & Wickens 2008) and size (Linsen, Leyssen, Sammartino, & Palmer 2011). These preferences often turn out to be ecologically driven; for example, people tend to like the colours of liked objects, with strong and culturally invariant liking of saturated blue and invariant hostility to colours associated with faeces and vomit. There are cultural variations; for example Japanese subjects appear to be unusual in their lack of enthusiasm for dark red (Palmer & Schloss 2010). Some colour preferences are very culturally specific, as with the preferences of college students for colours associated with their schools (Schloss, Poggesi, & Palmer 2011).

Studies such as these may be thought valuable in their capacity to explain persistent features of aesthetic artefact-making but are unlikely to provide more than a general background against which aesthetic preferences and judgements, debates and disagreements concerning particular artefacts are played out. Saturated blue will be the right colour in a certain context, while muddy brown will be right in another. We cannot say that the presence of saturated blue in a picture is in general a reason, even a pro tanto reason, for admiring it or preferring it to a picture without that colour. When it comes to aesthetic reasons and judgements rather than unreflective preferences—when, that is, we consider top-down effects—cognitive science so far offers little to help us. This may to some extent be due to the nature of human cognition itself. Aesthetic reasons, in so far as we have them at all, are highly resistant to formulation in general terms (Sibley 1959), and once we abandon the view that aesthetic qualities supervene on an object’s appearance narrowly described (colour, shape and size in the case of visual art), there is no obvious way to limit the factors that legitimately impact on such judgements. This suggests a domain where, roughly speaking, everything is relevant to everything: the isotropy that is crucial to Fodor’s (1983) scepticism about a cognitive science of thinking. One recent study does attempt a unification of top-down and bottom-up approaches. Bullot and Reber (2013) develop a “psycho-historical” process that begins with a perceptual confrontation with the work, and culminates in artistic understanding, mediated by the viewer’s adoption of the design stance which involves reconstructing the genealogy of the work. This approach encounters two difficulties. First is their assumption that progress between stages is able to be done largely by the extraction of information from the perceptual signal. In fact the work’s appearance is often ambiguous or misleading as to its history—as when a work is carefully constructed to look haphazard in its construction (Levinson 2013; Ross 2013). The model needs to recognise the importance of art-historical knowledge gained from such sources as lectures and text books, knowledge which will often precede and inform the first stage. The second objection concerns the role of presumed essentialist assumptions in people’s approach to art works. Bullot and Reber argue that the historical nature of appreciation is explained by our commitment to psychological essentialism which makes us look beyond a things appearance to investigate its essence (2013: 132). But an inclination to look beyond appearances need not depend on essentialism; it might be that the interest or value of a thing sometimes resides in its relational and contingent properties (Korsgaard 1983). Valuing a work on account of its history of making no more implies an essential connection between the work and its history than valuing an object because it was a gift from a loved one implies an essential connection between the object and the giver.

3. Preference, Judgement and Reasons
As well as studies designed to elicit preferences, there are empirically motivated theories that purport to explain them. Among the earliest is Wundt’s (1874) suggestion that we judge a form pleasing to the extent that the eye finds it easy to follow the contour. Recent work generalises this thought: there is said to be a tendency for stimuli to be experienced positively when our perceptual or cognitive grasp on them comes easily (Reber, Schwarz, & Winkielman 2004). This idea of processing fluency is not limited to the aesthetic domain; we generally find a proposition more believable, apparently, if it is expressed in rhyme or in an easy to read font and this is said to be because these things increase fluency (Reber & Schwarz 1999; McGlone & Tofighbakhsh 2000). So aesthetic and fluent phenomena are not coextensive, and no reduction of the one to the other is possible. The most that can be said (and this is not insignificant) is that certain kinds of fluency underpin experiences of aesthetic pleasure and displeasure, symmetry offering a plausible example; for comments on the significance of such research into underlying mechanisms see Section 4 below.

One concern about an approach to the aesthetic by way of fluency is that fluency apparently encourages a less careful examination of the stimulus (Song & Schwarz 2008), a suggestion at odds with the aesthetic ideal of a finely discriminating attitude to art. Another is that fluency-based likings for stimuli seem to have very limited application in aesthetics beyond the domain of simple shape preferences. Pictures which are visually complex are often widely admired and not merely by the art-world elite (see for example Frith’s busy genre paintings). It is said that the pleasure derived from fluency is relative to expectation: pleasure taken in a Bach fugue depends on it being more fluent than one expects (Reber 2012: 228, citing Hansen, Dechêne, & Wänke 2008). But why must lovers of Bach have unrealistic expectations about the fluency of his music? Finally (a theme we will return to) citing fluency would not be a reason for admiring the picture or fugue in question; this approach seems to focus on pleasure and leave notions of reason and judgement out of account. (For criticism of the fluency program in aesthetics see Cochrane 2021a, Section 2.3.)

Another aspect of fluency is said to be the mere exposure effect: our tendency to find a statement more believable if we have heard it often, repeated hearings leading to progressively more fluent processing (Begg, Anas, & Farinacci 1992). Cutting (2003) investigated an aesthetic variant of the idea: liking for a picture is increased by repeated exposure to it. This, it is suggested, may explain the stability of the artistic canon: works canonical at t are more available to be seen, and the mere exposure effect raises the likelihood of their being canonical at t + 1. Cutting does not claim that exposure is the only factor contributing to aesthetic judgement, so accepting this view does not make one an error theorist about aesthetic value. It does, however suggest that the critical robustness of canonical judgements has been overestimated, since one’s exposure to a picture is not a reason for thinking it good. However, Meskin, Phelan, and colleagues (2013) compared the effect of mere exposure to (what critics have widely judged to be) good and bad art, finding that there was increased liking for good art but decreased liking for bad art, the implication being that increased exposure makes observers more aware of the objective qualities of the work (see also Delplanque et al. 2015). In that case significant exposure to a work, while not a reason why it is good or bad, is a reason why one’s judgement of it is reliable and so may be said to belong to the space of aesthetic reasons. Cutting (2017), while offering some methodological reservations, expresses broad agreement with the Meskin et al. study.

Recently the link between aesthetic effects and pervasive features of perception and cognition has been highlighted again by the theory of predictive processing: the idea that the fundamental activity of the brain is to make predictions and test and revise them against incoming information from the senses, the goal being the reduction in predictive errors or what is also called reduction in uncertainty (for a survey of philosophical applications of this idea see Hohwy . It is suggested that positive affect is the product of such reductions, while negative affect results when uncertainty is increased. Works of art are said to provide the brain with exercises, sometimes challenging ones, in error reduction (van de Cruys & Wagemans 2011). But why should we seek more exercises in prediction error reduction than the many that the world throws at us continually? What is to say that the eventual reduction in error outweighs the initial increase posed by a work of art?

We see, then, that there are a number of suggestions for how empirical work may shed light on our aesthetic preferences and their aetiology, while doubts remain about the capacity of this work to illuminate the normative structure of aesthetic judgement, which is said to be reason-focused and involve Kantian features such as disinterestedness (though see Meskin, Phelan, et al. 2013: 140–1). A second and familiar objection is the Wittgensteinian thought that merely explaining the origins of a phenomenon doesn’t give us the kind of explanation which philosophers are concerned with (Vrahimis 2020). However, the idea of reasons in aesthetics itself deserves critical interrogation, given the evidence that the reasons people give seem often to be confabulations and that focusing on reasons while making a judgement can make you a worse judge rather than a better one (see Irvin 2014 and Lopes 2014, mentioned above, Section 1). Nor is the problem here confined to the aesthetic domain: Hugo Mercier and Dan Sperber (2017) argue that, quite generally, reasoning works well as an instrument of public debate but poorly as a guide to personal judgement and decision.

4. Art, Empathy and Neuroaesthetics
German aesthetics of the late nineteenth century was particularly focused on explaining human attraction to form in picturing, sculpture and building. Not always easy to interpret in detail, the broader themes of this work include the ways that our bodily and ecological situation determine the aesthetic preferences we have for such things as symmetry (Wölfflin 1886), how architectural form is appreciated in terms of its capacity to facilitate movement through space (Schmarsow 1894, and how a sense of beauty arises from our projecting the feelings provoked by an external object into that object (Lipps 1903, see Currie 2011 for discussion). After a long period of neglect, these and related topics have reappeared under the banner “embodied cognition.” Freedberg and Gallese (2007) have drawn attention to the ways our bodies tend to reproduce the posture of a statue, to produce implicit or simulated movements that mirror those that seems to have produced a brush stroke or chisel mark, to resonate sympathetically with the pain of a represented figure. These, they say, are intrinsic aspects of aesthetic experience neglected by theorists who emphasise an intellectualist approach to art. They also claim that evidence for these effects based on introspective reports can now be supplemented by an empirically verified theory of brain processing that appeals to mirror and canonical neurons. Critics have alleged that this approach neglects the aesthetic impact of top-down factors (Kesner & Horáček 2017) and that motor responses are absent in many encounters with art (Casati & Pignocchi 2007: 410). Arguably though, Freedberg and Gallese seek only to identify an aesthetic factor of some significance and need not claim either exclusiveness or universality for it. Formalists in art might object even to this restricted claim, arguing that what Freedberg and Gallese have identified are obstructions to the sort of “purely optical” attention to pictures that Greenberg (1960) advocated (on which see Steinberg 1965; Currie 2007). But here again aesthetic prescription cannot afford to ignore empirical work: standards of aesthetic attention that no one has ever lived up to are not to be taken seriously.

Attempting to illuminate aesthetic phenomena by appeal to mirror neurons and other neurological processes is now so common that we have a substantial genre of neuroaesthetic writings (see Chatterjee 2013 for the field’s growth over 30 years). What remains controversial, though, is the significance (if any) this research into brain structures and processes has for aesthetics. We have already seen that—despite the bold claims sometimes made by neuro-aestheticians—some critics are inclined to dismiss this kind of work as irrelevant to evaluating the kinds of claim which aestheticians are concerned with. Gallese and Freedberg, responding to critics, say that

no esthetic judgment is possible without […] mirroring mechanisms in the forms of simulated embodiment and empathetic engagement that follow upon visual observation. (2007: 411)

But to understand the aesthetic significance of facts about such mechanisms we need to distinguish between two claims:

Evidential claim: facts about mechanisms may provide evidence to support or undermine a claim about what is involved in aesthetic experience or judgement.
Constitutive claim: facts about mechanisms may be constitutive parts of the story we tell about what is involved in aesthetic experience or judgement.
The evidential claim is more plausible than the constitutive claim. Consider first the evidential relevance of cognitive and neurological mechanisms for aesthetics. We have just now referred to a dispute about how much of our experience of visual art depends on a work’s tendency to encourage bodily empathy with what is depicted. While people sometimes report experiences of these kinds it may be that they vary in accessibility, with some unnoticed without high levels of attention, though still making a contribution to our overall liking for/valuing of the work. Studies of brain activity across a range of artistic stimuli might suggest that these empathic responses are very common indeed—or that they are relatively unusual—because (let us suppose) empathy has an identifiable neurological signature. Such studies would be useful as evidence for the claim that something needs to be factored in to an account of aesthetic experience. But the relevant something would be the empathic response, not the brain process (See Carroll, Moore, & Seeley 2012: 54 where studies of this kind are described as “data” for aesthetic theories).

The difference between aesthetic effects and the mechanisms that realise them is an important one. Take the shimmering quality we find in impressionist art. It is said that this depends on the fact that the visual system involves two separate processing streams and that certain colour combinations are processed exclusively by one of them (Livingstone 2002). Should this fact, if it is one, be regarded as an aesthetic fact? Arguably no. What matters aesthetically is the experienced shimmering quality; a world in which just that shimmering quality was produced by some other mechanism of perceptual processing would not be aesthetically different from our world, at least in this respect. The same applies to scientific studies of the stimuli themselves; chemical analysis of an ancient pigment might show that it was a hard medium to work in, and that would be relevant to understanding the works of art produced by its means. But what matters is the difficulty of the medium; discovering its chemical formula simply provides evidence for that.

5. Authenticity
A Kant-inspired view of long standing is that aesthetics is concerned not with how things are but with how they appear (e.g., Urmson 1957), a view sometimes called aesthetic empiricism (Currie 1989) On this view aesthetics takes no cognisance of authenticity, which depends on history and not on appearance; a fake Vermeer does not become authentic by being visually indistinguishable from a genuine one. Is an object’s history really aesthetically irrelevant? Those with an interest in art, craft and the design of artefacts generally are much concerned with the object’s history, and there are arguments to support the view that the application of at least some aesthetic properties depends on assumptions about that history. Consider two such attributions from Sibley’s (1959) extensive list of aesthetic properties: being delicate and being dynamic. A line that is seen as delicate when thought to be drawn by hand may not seem so when revealed as the product of a machine, and Walton (1970) argued that, of two works which are visually indistinguishable, one may be fairly described as dynamic and the other not, depending on differences in their category membership (for defences of aesthetic empiricism concerning visual art see Zanwill and, on music, Dodd; for the aesthetic relevance of genuineness or authenticity see Korsmeyer 2019). The suggestion of at least the first of these examples (being delicate) is that the aesthetic properties of a thing, and the aesthetic values that possession of these properties entails, depend on the kind of achievement that the fashioning of the work represents. What may be a considerable achievement for one person at one time may be a greater, lesser or at least a different achievement for someone else, working with different materials or in a distinct artistic culture (Dutton 1979; Currie 1989; Huddleston 2012; Levinson 2016); James Grant (2020) suggests that we do better to think in terms of a work’s capacity to exhibit the skills of its maker rathen than in terms of its constituting an achievement of the maker (see also Currie 2018).

Both formulations imply a close connection between valuing the work and valuing the maker, helping thereby to explain how an interest in the authenticity of a work can belong to the space of aesthetic reasons. It also suggests a connection with our desire to preserve and possess otherwise unremarkable objects because of their relation to some person or event that interests us (JFK’s sweater). Some psychologists have thought to bring art works and other valued artefacts together under the heading of contagion, the idea that

a person’s immaterial qualities or ‘essence’ can be transferred to an object through physical contact; (Newman, Diesendruck, & Bloom 2011)

an idea that goes back to early anthropological work by Frazer (1890 [1994]) and others. As the examples just given indicate, the idea that a person’s essence is transferable to an object does not depend on the object’s aesthetic merits, and this severely limits the usefulness of this idea in explaining the role of authenticity in the arts (see, however, Korsmeyer 2019 where it is argued that genuineness is itself an aesthetic property). Bloom and colleagues stress the role of physical contact in the contagion process: “An original Picasso may be valuable because Picasso actually touched it” (Newman & Bloom 2012: 3). But Picasso may have got no closer than a brush length from Guernica, a picture likely to command higher art-world respect (and higher prices) than a restaurant napkin sketch he then wiped his hands on. However, a more general and perhaps rather vague sense of “closeness” to the artist and their act of making does seem to be explanatory of some art-world practices, such as a preference for lower numbered (and hence earlier) prints even where there is no decline in quality across later ones (Smith, Newman, & Dhar 2016).

Bloom and colleagues offer another explanation for our interest in authentic items, this one closer in spirit to the suggestion above that aesthetic judgements are sensitive to achievement or the manifestation of ability:

[A]n original is different from a forgery because it is the end point of a different sort of performance . The original is a creative work while the forgery is not. (Newman & Bloom 2012: 559)

Guernica, very likely, is a more creative work than the napkin sketch and plausibly valued more for that reason. A question which arises is this: how do people make judgements of the quality of, say, a painting, if not solely on the basis of how it looks? For a few highly trained experts the answer may be: through deep immersion in the cultural and artistic context of the work. The rest of us, where we make a judgement at all, may be dependent on short cuts such as the “effort heuristic”, which treats evidence of effort as evidence of quality. In an experiment, people asked to judge the relative merits of pictures A and B tended to judge A as better than B when told A took longer to paint, while the group who were told that B took longer preferred B. This effect was found equally among experts and among laypersons, though the experts were “self-described” (Kruger et al. 2004).

6. Pictures, Imagination and Perception
Wollheim (1980) said that what is distinctive about pictorial representation is its capacity to generate a certain kind of experience: the experience of “seeing the subject in the picture”. For many this has seemed to capture something deeply important about the nature of depictive representation, though Wollheim’s specific claims about it are disputed. What exactly is seeing-in? One subsequent suggestion has been that we see something, X, in a picture when we experience a resemblance between the outline shape visible in the picture and the outline shape that would be presented from that same perspective by X (Hopkins 1998; for a related proposal see Peacocke 1987). Another proposal is that we see X in a picture when we are prompted by it to imagine, of our act of seeing the picture, that it is a seeing of X (Walton 1992). Is cognitive science able to help us adjudicate between rival theories in this area? It does look as if these ostensibly philosophical theories are committing themselves, if somewhat vaguely, on empirical issues. Surely, we cannot settle from the armchair whether the perception of pictures draws on imaginative capacities. Scientists have gone a long way towards locating areas of the brain implicated in certain kinds of functions, emotion and the forms of perception being good examples. But we are not similarly able to localise imaginative activity and there may be an in-principle barrier to our doing so; there is some reason to suppose that there are not dedicated mechanisms of imaginative activity, but that imagining involves the reuse of systems designed for other purposes. This issue has been a central contention in the debate over how we understand the minds of other people, a debate originally drawn in terms of two opposing outlooks, theory-theory and simulation theory, though other approaches have come into view (Gallagher & Hutto 2008). Theory-theory attributes to us a (perhaps tacit) theory of the mental states of others, their connection to action and so forth, from which we derive predictions and explanations of behaviour (Fodor 1992; Carruthers 1996). Simulationism, by contrast, argues that we have a capacity to use our own mental systems of inferring and deciding to model or simulate the thoughts and decisions of others (Gordon 1986). While Heal has emphasised what she takes to be the a priori principle that when we examine the thinking of another person we have to reproduce in our own minds the contents of and the logical relations between the propositions thought (1986), others have developed an empirically oriented version, postulating causal mechanisms by which practical and theoretical reasoning can be taken “off-line”, or disconnected from experiential inputs and behavioural outputs (A. I. Goldman 1989). It has been suggested that the simulation approach, understood largely in this empirically oriented way, enables us to explain a good deal about our interest in and responses to fiction (Currie & Ravenscroft 2002). Questions about causally effective mental architecture have played a role in other aesthetic debates such as the explanation of our capacity to become immersed in a narrative (Schellenberg 2013) and of the contrary tendency to “resist” imaginative involvement with stories we perceive as morally or in other ways problematic (Weinberg & Meskin 2006; Miyazono & Liao 2016).

While some of these ideas come with a detailed “boxology” that describes the cognitive implementation of the proposal, they are generally the product of work at the more philosophical and speculative end of inquiry into the architecture of mind where evidence is at best very indirect. An exception is the suggestion that there is empirical confirmation for Wollheim’s (1998) notion of twofoldness in picture perception, according to which spectators are simultaneously aware of both the surface qualities of the picture and the depicted content. In particular, it has been argued that

the twofold experience of pictures Wollheim talks about corresponds to the dichotomy between our dorsal visual processing of the surface of the picture and our ventral visual processing of the depicted scene. (Nanay 2011: 464)

Support for this idea comes from evidence that people with damage to their ventral processing often have difficulty deciphering the content of a picture without corresponding difficulty in perceiving the ordinary properties of the surface of the picture (Turnbull, Driver, & McCarthy 2004). This illustrates another way that the investigation of mechanisms may further aesthetic inquiry. An aesthetic phenomenon—in this case twofoldness—is postulated and the question arises as to whether there is a plausible story about how this is implemented. If no such story were available, doubt would be thrown on the claim.

7. Emotion
A good deal of aesthetic thinking has been taken up with arguing about the limits to its capacity to appeal to the emotions. Two issues are particularly notable, one descriptive and the other, in part, normative. What, first of all, are the facts about our (apparently) emotional responses to fictions? Do we really admire, despise or pity people we know do not exist? This sounds like an empirical question, perhaps one to be resolved by studies of the brains of people engaged by fictional work. But the prospects for this approach are poor, and not only because of practical difficulties in data gathering (see Cova & Teroni 2016). Those who deny that we pity Anna Karenina do no deny that readers may display all the physiological signs that go with that emotion. What they claim is that genuine fear has a cognitive component lacking in this case: belief in the object of one’s fear. Suppose we could show empirically that readers of Tolstoy do experience all the pity-relevant physiological states while failing to believe in the existence of Anna Karenina, having instead cognitive states such as “imagining that Anna exists”. The philosophical problem would remain: are people with that psychological profile genuinely in a state of pitying, or is their state merely one of what Walton (1978) calls “quasi-pity”, which involves the typical physiological and psychological reactions of pity but which falls short of being full-fledged pity because it does not go with belief in its objects, or with the typical corresponding dispositions to action? We might at this point wonder whether what looked like a substantive question about fear and pity has been shown to be merely one about labelling. However, there are empirical considerations that bear indirectly on the issue. Gendler and Kovackovick suggest that plausible assumptions about the evolutionary proper function of the emotions tells against the idea that fear requires belief. It is likely on Darwinian grounds that our emotions were shaped not merely by their capacity to have us avoid present dangers, but also to have us avoid merely possible ones. Imagining a dangerous creature lurking in the forest may be enough to send me by a safer route, raising my chances of surviving and passing on my genes. If imagined dangers are part of the reason fear has been selected for and retained why insist that responses to the imagined creatures and events of fiction are, as Walton (1978) claims, merely quasi-fears? For other connections between aesthetics and evolutionary thinking see below, Section 9.

A similar problem arises when we consider our responses to music. We sometimes talk of sad music making us sad, though the music does not provide us with any reason for thinking that something sad has happened (sad music sometimes reminds us of a sad event, but this is not a general feature of cases where sad music affects us). Here appeal to the imagination seems less helpful; unlike fictional narratives, emotionally affecting music does not usually provide us with the materials for imagining some sad event. Kivy has argued that sadness on the part of those who listen to sad music is, where it occurs, irrelevant to musical appreciation and will indeed not occur in episodes of listening which instantiate his preferred model of musical listening, one which requires exclusive attention to the structural, phenomenological, and expressive properties of the music (1990: ch. 8). Kivy does not deny however that musical listening can and indeed should be a deeply emotional experience; we may be overwhelmed by the compositional and performance qualities the work represents. Some attempt has been made to challenge Kivy on empirical grounds by presenting evidence that music does reliable engender the emotions or moods it expresses (Sizer 2007). But Kivy denied this possibility only for the case of canonical music listening, and says that there is no reason to think that subjects in the relevant experiments were listening in that canonical way (2007). Certainly it would not be easy to control experimental conditions so as to rule-out listening which is non-canonical in Kivy’s sense (see also Kivy 2006; Carroll 2003; Carroll & Moore 2007). However, if experimental tests do consistently provide evidence of emotion-generation concordant with the emotions expressed by the music, it will not do simply to say that we have not yet tested the right listeners. The friend of Kivy will have to find robust instances of the sort of listening he approves of, and not depend merely on the personal testimony of those who favour his theory. We take it, after all, that Kivy’s ideal mode of listening is presented as a practically achievable goal. Zangwill 2004 represents a formalism that is less willing to accommodate the emotions; for a more limited defence of the formalist position see Cochrane 2021b.

Another aesthetically important question about the emotions is this: why are we attracted to representations and other devices that generate negative emotions such as sorrow and fear? It is not a satisfactory answer to this question to make the suggestion referred to above that we do not really fear Dracula and other creatures of fiction but merely quasi-fear them, because fear and quasi-fear are supposed to be qualitatively the same, so if fear is unpleasant quasi-fear is also. The question can be asked about both narrative works and about music; recall that even Kivy grants that (non-canonical) musical listening may generate sadness. However, fiction has been the greater part of this discussion, and will be here also. On this issue philosophers have tended to take one of three positions (but see Smuts 2009 for a wider range of options):

Compensation: audiences tolerate the negative emotions of tragedy, horror and other genres because they recognise compensatory benefits (Carroll 1990; Feagin 1983)
Conversion: emotions which would otherwise be experienced as unpleasant are experienced as positive in artistic contexts (Hume 1757 [1987]).
Neutrality: it is wrong to think that the so called “negative emotions” provoked by tragedy or horror are intrinsically unpleasant and hence in need of compensation or conversion. They are not intrinsically valenced and may be experienced negatively in some situations and positively in others (Gaut 1993; Walton 1990: 255–8)
Neutrality is a general claim about the emotions and relevant here as one way to argue in favour of conversion. But a conversion theorists need not be committed to Neutrality. We focus here on the contrast between Compensation and Conversion. Compensation, unlike Conversion, appeals to a type of psychological process that is hardly controversial; we accept many kinds of unpleasant experiences because avoiding them will have even less desirable consequences. Conversion sounds more problematic: how can an emotion “flip” its hedonic value and still be the same emotion? Psychologists interested in negative emotions in art have cited research which purports to reveal hedonic flip for pain: subjects who expected an intense pain but experienced only a moderately painful stimulus described the experience as “pleasant”, while subjects expecting nonpainful warmth described the same stimulus as unpleasant (Leknes et al. 2013). This doesn’t by itself show that the same thing can happen in the case of tragedy or horror, but it suggests that the conversion claim is less outlandish than it might initially seem.

While compensation may appeal to a familiar kind of trade off, it needs to tell us what specific compensatory benefits are to be had from the negative emotions provoked by art. Philosophers have long cited epistemic benefits: the distressing events of tragedy are a source of moral and psychological learning, sometimes about ourselves (Collingwood 1938; Schier 1983). Empirical scientists have added to the list: painful emotions generate strong pro-social feelings that help to unite us with both characters and authors (Bastian, Pe, & Kuppens 2017; Egloff 2017).

While philosophers have long offered theories to resolve this “paradox of tragedy,” only one substantial study of it claims to be grounded in empirical research. Menninghaus and colleagues propose what they call “the Distancing-Embracing model” (Menninghaus, Wagner, Hanich, et al. 2017) according to which an aesthetic context, as with watching a play or film, distances us from the situation represented and allows the positive effects of tragic emotions, notably their capacity to focus attention, to sustain them. While a large and relevant body of empirical work is called on in support of the idea, its theoretical commitments are unclear. In some of their formulations the function of distancing seems to be the reduction in unpleasantness of negative emotions to the point where they are “not inevitably incompatible with art-specific expectations of hedonic reward” (2017: 6); this leaves them open to the charge made against compensation theories that the work’s being apt to generate negative emotions seems to be a major factor in attracting people to the work, rather than something negative we can be persuaded to live with. In other formulations they speak of the factors “involved in making negative emotions enjoyable” (2017: 3), which suggests the conversion theory. Yet they claim that their theory is a version of neither of these approaches (2017: 15; see S. Davies 1997). (For more on the relationship between fiction and emotion see the supplement to Liao & Gendler 2018 [2020]). See below, Section 7 for further remarks on fiction and emotion.

To conclude the discussion of fiction, we note another direct appeal to work at the empirical level. Derek Matravers (2014) argues that it has been a mistake of recent theorising about fiction to appeal to the imagination. Matravers claims that work on mental models does a good job of explaining our processing and comprehension of, and responses to, both fictional and nonfictional narratives without any need to distinguish fictional cases by their connection with the imagination.

8. The Aesthetics of Literature and Literary Language
Through the latter part of the twentieth century, theoretical and interpretive studies of literature have been somewhat hostile to both cognitive and aesthetic inquiry, often thought of as irrelevant to serious interpretive work and a distraction from favoured political agendas. Cognitive studies of literature have developed somewhat in the last twenty years but remain a minority enterprise (Hartner 2017). One question of long standing that has concerned scholars in psychology, literary studies and aesthetics is the idea that we learn from literature about the world beyond the text, an idea treated with some suspicion by those influenced by deconstructive or post-modern theorising, who argue that such ambitions depend on failing to realise that characters of fiction are not, and are not like, real people (Cixous 1974). Others argue that such cognitive benefits, if they exist, are not aesthetically relevant (Lamarque & Olsen 1994: ch. 17; for criticism see Gaut 2007: ch. 7; Currie 2020: §9.6). There is, however, a strong tendency to suppose that the artistic merit of, say, a Shakespearean drama depends in part on our sense that it is revelatory of deep insights into the human condition. Similar claims are made for other arts; contemplating van Gogh’s lithograph Sorrow Kendall Walton notes responding imaginatively to the woman in ways that “gain for me an understanding of what a particular sorrow is like” (2008: 78).

Recently efforts have been made to test the idea that literature does, on occasion, provide cognitive benefits. It has been claimed that exposure to fiction—even single episodes of reading—measurably improves empathy and theory of mind (Kidd & Castano 2013); the effects, if real, seem to be small and the claimed results have been hard to replicate (Panero et al. 2016; Kidd & Castano 2019). Other work in this growing field also does not point in a single direction. Some studies suggest that fiction has a capacity to improve aspects of social cognition (Calarco et al. 2017; Mar 2018a,b) while others find no such effect (e.g., Wimmer et al. 2021a, b; see Currie 2020; chs 9–11 for general discussion). Humanistic scholars who claim educative effects for fiction may respond that these effects are slow and cumulative, and unlikely to be visible in studies of single episodes of reading; testing such claims will be difficult. But their claims are, nonetheless, empirical.

Among other examples of the cognitive study of literature are investigations of emotional responses. While aestheticians have tended to focus on the apparently “paradoxical” aspects of our emotional responses to fictions (for example why we are emotionally affected by the non-existent, see above Section 7), researchers in the cognitive sciences has begun a broader attempt to understand the role of emotions of all kinds in generating and maintaining interest in narrative and poetry (Menninghaus, Wagner, Wassiliwizky, et al. 2017); Jenefer Robinson’s (2005) is an example of work from within aesthetics that takes up this broader ambition while being strongly informed by empirical work. More recently, Kukkonen (2020) attempts an alignment between the expectations of a reader sometimes surprised by plot development and the predictive processing approach to perception and cognition (see above Section 3). It is an open question whether the predictive processing account sheds aesthetically relevant light on the reader’s experience rather than simply providing an account of the mechanisms involved (see above Section 4).

A striking example of an area long thought of as resistant to theory from outside the domain of art and the aesthetic is that of literary language. Insight into the kind of language we think of as distinctive of poetry, drama and the novel has traditionally been sought from those whose expertise is literature itself rather than from those developing general theories of communication, a view endorsed by Peter Lamarque, who regards the idea that there is value in seeing literary works as akin to quotidian genres of utterance such as letter-writing or political speech making as “utterly misconceived” (Lamarque 2007: 34). But advocates of relevance theory, a naturalistically inclined development of Gricean pragmatics, reject the idea that there is anything fundamentally special or autonomous about literary language, claiming that

stylistic and poetic effects traditionally associated with figurative utterances arise naturally in the pursuit of relevance, and call for no special treatment not required for the interpretation of ordinary literal utterances. (D. Wilson 2018: 191)

(for work that seeks empirical confirmation for relevance theory see, e.g., Happe 1993; Sperber & Wilson 2002; van der Henst, Carles, & Sperber 2002; Noveck & Posada 2003; and van der Henst & Sperber 2004).

One ambition of relevance theory is to encourage us to see “poetic uses” of language, where an idea is very indirectly suggested by the words used, as less marginal than we would if we thought of the paradigm of communication as the case where an agent utters a sentence that means exactly what the speaker wishes to communicate. Relevance theorists say that such cases of literal meaning are rare or non-existent and do not constitute any kind of communicative ideal. They point to the widespread phenomenon of “weak implicature”: audiences draw a range of conclusions from an utterance, some of them obviously intended but others harder to identify as intended (Sperber & Wilson 1995: 197–9). On this view the totality of what is meant, even in mundane cases, is never more than vaguely specifiable and would, if we bothered to investigate, be subject to the same unresolvable disputes as we find in poetic criticism. It need not be concluded that poetic discourse is entirely of a piece with over-the-garden-wall conversation; the relevance theorist’s point is that the differences are ones of degree and not of kind.

9. Aesthetics and Evolution
Theories of human evolution over the last two million years are severely limited by the very incomplete fossil record and the fact that soft tissue disappears quickly. Given these constraints it is surprising the extent to which cognitive theorising has reached into the very distant past, though its claims are agreed on all hands to be highly provisional. A vital resource has been the study of stone artefacts going back two million years or more, artefacts which preserve very well and which are available in great quantities. We are able to reconstruct a good deal about the methods of their construction and to speculate in a reasonably informed way on the cognitive capacities they require. Based on these speculations innovations in stone tool manufacture have been linked to the development of language (Higuchi et. al. 2009).

Any topic on which aesthetic and evolutionary methods converge is likely to be approached with two questions in mind.

Is the emergence and development of art-making and related activities explainable wholly or very largely in cultural terms, or is there a substantial biological component?
Is art-making an adaptation or a non-adaptive consequence of developments which may themselves have adaptive advantages (or something else)?
It might be thought that the second question arises only if one answers the first in a way that gives an important explanatory role to biology. But culturally determined “phenotypes” can also be adaptive, maladaptive or neutral. Cooking, certainly a cultural innovation, is highly adaptive and has led to significant change in gut size and perhaps in brain size as well (Godfrey-Smith 2013; for scepticism concerning the relation to brain size see Cornélio et al. 2016). Human biological and cultural evolution are closely connected.

On the question whether there is a role for biological explanation when it comes to human aesthetic activity, there is a constituency of scholars hostile to this idea; they argue that “art” and “aesthetic” are notions of recent and essentially western invention, the application of which to other and older cultures is mistaken (Gell 1998: 97). Opponents of this view readily grant the huge cultural variability of aesthetic activity. They argue that there are enough commonalities between the ways very different cultures invest in the shaping and decoration of objects, in music, dance and in story-telling to make it plausible that biology provides both the initial impetus to and a set of constraints on these activities (Dutton 2009; Currie 2012). If we thought, as we once did, that these activities could be traced no further back than the cave depictions of the European Upper Paleolithic it would be likely that the aesthetic is too late-emerging to have a biological basis. Very recent discoveries have now suggested a somewhat earlier origin; depictions in Indonesia have been given dates prior to 45,000 years ago (Brumm et al. 2021). More dramatically, though, the temporal depth of aesthetic activity is vastly increased if we turn our attention to the stone tools mentioned above, many of which show a notable symmetry of construction. Hand axes from the Acheulean industry which arose about 1.7 million years ago are sometimes too large, small or sharply pointed to be practical implements and strongly suggest an interest in the display of skilful making that we commonly recognise in, say, the art of the European renaissance (see Currie & Zhu 2021 and Gowlett 2021, both of which in different ways emphasise the importance of seeing aesthetic practices emerging in a social context). Some have seen these elaborated instruments as products of sexual selection, as peacock’s tails are said to be: reliable indicators of the maker’s qualities as a mate (Kohn & Mithen 1999). Another view has it that they were indicators of trustworthiness, and hence of one’s value as a co-operator (Spikins 2012); for criticism of both views see Hiscock 2014.

Hand axes are unlike a peacock’s tail in being detached from the body and so may be passed from one person to another—one reason to doubt their reliability as signals of any particular agent’s suitability as a mate. Where symmetry of the body itself is at issue that objection does not apply, and bodily and especially facial symmetry has been offered as an example of something evolution has tuned our sensibilities to because it is an indicator of health in a reproductive partner. However, robust evidence has been hard to find. One recent study did “not support the idea that facial symmetry acts as a reliable cue to physiological health” (Pound et al. 2014; see also Kalick et al. 1998), another that there is “little evidence that female appearance predicted health” (Foo et al. 2017; this study did support a connection between male facial appearance and fertility). For ethical arguments, supported by empirical findings, that we should rethink our standard approach to the aesthetics of human appearances, see Irvin 2017.

It should be noted that even a very early dating of the emergence of aesthetic interests does not establish that they have a biological origin; the Acheulean period seems to have been marked by high levels of planning and organisation that suggest strong social relations (Hiscock 2014). In this sense aesthetic activity may be a culturally derived “technology”, as writing is, rather than an adaptation, as language may be (S. Davies 2012: ch. 10). Could we account, on that hypothesis, for the seeming ease with which young children take to forms of aesthetic activity such as drawing and singing? By contrast, learning to write is effortful and requires great investment in pedagogy. Recent work (Heyes 2021) seeks to reshape our assumptions about what is learned, arguing that such an early and reliably developing ability as interpersonal understanding is really a socially acquired “gadget” (Heyes & Frith 2014). To add to a complex menu of options we also note that what begins as a gadget or technology might, under certain circumstances fall under complete or partial genetic control. This so-called “Baldwin effect”, once despised in evolutionary thinking, has attracted interest more recently (Weber & Depew 2003). It proposes a mechanism by which acquired characteristics can become biologically heritable, without giving any ground to the Lamarkian idea that changes that accrue in an organism’s lifetime can be passed directly to the next generation. For example, if advantages flow from learning a skill, there may be selective pressure to ease the burden and uncertainty of learning by making the skill, or some of its components, innate (Papineau 2005). Given the multiply crossing paths between cultural and biological evolution there is little definite to be said about the origins of aesthetic interests other than that we see evidence for it in human populations more than half a million years ago.

We turn now to our second question: Is art-making an adaptation or a non-adaptive consequence of developments which may themselves have adaptive advantages? Or something else entirely? Strong and contrary views have been expressed on this question; some have argued that the development of depiction, story-telling and music have been crucial for social bonding (Dissanayake 1992, 2000, 2017) or mate-choice (Miller 2000; Dutton 2009; for criticism see, e.g., C. Wilson 2016), others say that the creation of aesthetic artefacts offers no adaptive advantage but that they gained their hold on us because they provide rewards to sensory and other mechanisms in the mind that arose for quite other purposes (Pinker 1997). From a somewhat different perspective it is suggested that we do best by seeing human aesthetic activity as simply the continuation of ubiquitous practices of signalling and manipulation in the world of plants and animals. An interesting feature of this idea is that it replaces the traditional distinction between the aesthetics of artefacts and the aesthetics of nature with a distinction between, on the one hand artefacts and natural entities such as animal calls and flower colouring, and on the other natural objects such as rocks, sunsets and the night sky which “do not coevolve with sensory evaluations of them” (Prum 2013: 821; see also De Tiège, Verpooten, & Braeckman 2021). It is not clear whether we currently know enough to decide between the available alternatives (S. Davies 2012: ch. 12; for the case of fiction, Currie 2020; §7.4). But would deciding whether art-making is adaptive advance the cause of aesthetic inquiry? That is doubtful. The question whether art is an adaptation concerns whether the reason we have art now is that it contributed to the fitness of our species. That could be true without it also being true that art now enriches our lives in any of the ways commonly claimed—giving pleasure, educating our emotions, instructing us about morality. And it is possible that art does now have all those advantages while having been an evolutionary irrelevance. It is also true that even if art was not originally adaptive, it might have become adaptive (an “exaptation”) at some stage and might be adaptive now. Different answers may apply to different art forms. It is said that the novel is a particularly appropriate means by which to exercise our capacities for empathy and mind reading (Nussbaum 1990, 1995; Mar and Oatley 2008); perhaps these advantages did not appear until the eighteenth or nineteenth centuries. While questions about art posed in evolutionary terms are certainly of scientific interest, what matters to the aesthetician is more likely to be the values that art currently and historically exemplifies and the ways those values are currently and historically responded to by us.

10. The Aesthetics of the Environment
We have referred rather briefly to the aesthetics of the natural and humanly constructed environment and we conclude with a brief selection of aesthetically and cognitively relevant work in this area. Much of this discussion continues the evolutionary theme. Note that some of this work relies for its evidence on people’s responses to pictures of environments rather than to the environments themselves, leaving it uncertain how much of the response is to the environment and how much to its representation.

The empirical study of environmental preferences has long been of interest to planners, and largely represents the bottom-up mode of inquiry referred to above (see Section 2), as the aim is to discover what people in general like rather than to assess their likings against the normative constraints of connoisseurs and philosophers. Topics explored include the differences in preference across age groups, ethnicity, education and income (e.g., Kaplan & Talbot 1988). One conclusion often drawn is that there is a widespread, perhaps universal, human preference for savanna landscapes, even among communities whose own historically stable habitat is very different, and that this preference is reflected in the design of parks and gardens (Falk & Balling 2010). It is further asserted that this is the product of our evolutionary development in an East African context (E. O. Wilson 1975 is an early source of this idea); for detailed discussion, references, and some reservations see S. Davies 2012: ch. 6.

The non-prescriptive nature of much empirical research into responses to the environment contrasts with a view influential in philosophical aesthetics according to which there are quite demanding cognitive requirements for a correct aesthetic appreciation of nature. The question mirrors one we have already discussed for the arts: what factors, other than the sensory appearance of an object, are relevant to enjoying, appreciating and judging it aesthetically? In the case of the arts we saw that one answer is to consider the work’s category—the (artistic) kind of thing it is (Walton 1970; see above Section 5). Allen Carlson (2000) extends this idea to the aesthetics of nature, arguing that a scene or object must be understood as belonging to a natural kind, and as having the causal history characteristic of that kind; that is the right way to perceive it and aesthetic judgements about it are correct only if they are tied to that category. This implies that much human aesthetic experience of nature has been radically incorrect, being uninformed by the relevant scientific facts and often infused with notions of supernatural creation. Some will find this an acceptable consequence; we are used to thinking that many long-standing human ethical judgements have been radically mistaken, and aesthetic judgements might go the same way. Others however, question whether the aesthetic delight one takes in, say, a bird in flight depends for its acceptability on understanding the natural history of the creature (Budd 2001).

A different and perhaps more accommodating approach to identifying what is distinctive in aesthetic appreciation of the environment connects an older way of thinking with recent work on cognition. The eighteenth century focused attention on notions of the sublime (notably Burke [1757], but in a line of thought from Longinus) and the picturesque (Gilpin 1782, with roots in the older concept of pittoresco). These, particularly the former, have generated controversies about their relations to beauty—are they subspecies of beauty or categories distinct from it?—and about their extensions—is it only nature that can be sublime? The question also arises as to whether terms like “the sublime” pick out a class of objects to which there is actually a common aesthetic response. This sounds like something on which we could get help from the cognitive sciences, and work by Keltner and Haight (2003), drawing on a range of empirical and reflective sources, has been particularly noticed. Their work was on the concept of awe rather than the sublime but they note obvious connections, given the thought that the sublime involves a response to things physically or conceptually vast. They suggest that awe involves the recognition of the apparent vastness of the object or scene attended to and the inability of the subject to assimilate it to their existing mental schemas. Other research suggests a strongly pan-cultural component in nonverbal expressions of this emotion (Keltner et al. 2019), and its association with a diminished sense of self (Piff et al. 2015; Tom Cochrane (2012) has argued that the sublime is characterised by a feeling of self-negation). It is suggested therefore that the experience of the sublime is an aesthetic form of awe wherein the object is attended to for its own sake (Clewis 2019; Arcangeli et al. 2020), an idea elaborated by Shapsay (2021) who suggests that the sublime sometimes takes a cognitively elaborated form involving reflection of the challenge posed to existing schemata by the scene or object one confronts.

1. Speaking of Evil
“Evil” and related terms in the Germanic branch of Indo-European have referred, at various points, to suffering and wrongdoing, but also to defecation, latrines, spoiled fruit, diseases, prostitution, and (oddly enough) forks.

The Greek term “kakos” may be related to the Proto-Indo-European term “kakka”—“defecation”. But only the first two meanings survive in English, and non-ironic uses of the term are relatively rare outside of ceremonial and literary contexts. Indeed, speaking of evil nowadays often feels like an exercise in anachronism—like speaking of wickedness, abomination, uncleanness, and iniquity.

The Oxford English Dictionary explains:

In modern colloquial English it [evil]; is little used, such currency as it has being due to literary influence. In quite familiar speech the adjective is commonly superseded by bad; the noun is somewhat more frequent, but chiefly in the widest senses, the more specific senses being expressed by other words, such as harm, injury, misfortune, disease, etc. (“evil, adj. and n.1”, under A., abbreviations expanded, OED Online, accessed September 2021)

This trend is found in other modern languages, but not in all. Ruppel (2019) notes that in German-speaking lands “das Übel” declined just like “evil” did in England, but was soon replaced by “das Böse”, which is still alive and well in Germany.

This slow erasure of “evil” and its cognates from many European languages, which began in the seventeenth century, was due to the rejection of the concept of evil, especially by elites. Doctors, moral philosophers, natural scientists, and even theologians shied away from evil—preferring more tractable notions like badness, harm, and misfortune, or quasi-quantifiable concepts like pain, suffering, trauma, and disutility. Traditional views of ontologically substantive and supernatural evil—something able to possess a body or terrorize a soul—came to be seen as quaint, unscientific, embarrassing (Ruppel 2019).

Philosophers of religion are a half-exception to the rule. They did and do continue to speak of evil, at least when discussing the “problem” thereof. (See the entry on the problem of evil.) If pressed, though, they typically admit that this is because the great framers of the problem—Augustine, Aquinas, Leibniz, Bayle—used the term (in Latin or French), and then proceed to gloss it generically as, in Michael Tooley’s words, “any undesirable states of affairs” (2002 [2019]). Philosophers of religion in the broadly Continental tradition are less likely to assimilate evil to more general or anodyne notions in this way, and more likely to discuss the nature of evil as opposed to the “problem” it raises for theism (e.g., Kearney 2001 and Matuštík 2008).

Despite this widespread squeamishness about “evil” in both scientific culture and common parlance, there are moments when the pull of the ancient lexicon is irresistible—at the very least expressively, in the mode of both condemnation and lament. Premeditated mass shootings aren’t just bad or traumatic; rather, they are something else: here people still reach for “evil” or even “radical evil”. The years-long imprisonment and rape of children by their parents is a misfortune that produces negative utility, to be sure, but the transfixing horror of it seems only to be captured by the invocation of “evil”. The same is true of most instances of genocide, sex-trafficking, torture-slaying, terrorism, serial killing, and slavery: these are one and all bad, harmful, and traumatic activities, but they are also something else—something excessive, mesmerizing, and revolting all at once (see Stone 2009 for a psychologist’s account). In the face of such acts, we—along with our spiritual leaders, newscasters, and politicians—are still willing to speak, preach, and tweet about “pure evil”.

Thus after a school shooting in February 2017, Donald Trump (@realDonaldTrump) tweeted that “we must keep ‘evil’ out of our country”. (Despite the quotation marks, it was clear that he meant evil the entity, and not “evil” the word.) After the Las Vegas mass shooting in October 2017, Trump and many others in leadership referred to the event as “an act of pure evil” (Matuson 2017). Less recently, George W. Bush referred to Iran, Iraq, and North Korea as the “Axis of Evil” in a state of the union address on 29 January 2002 (see Other Internet Resources), and Ronald Reagan repeatedly characterized the Soviet Union as “the evil empire”, famously at a speech on 8 March 1983 to the National Association of Evangelicals (see Other Internet Resources).

But when we do this—when we speak of evil, das Böse, il male nowadays—what is it that we are referring to, and where does it come from?

Pressed with such questions, many people (philosophers included) revert to the more tractable terms. Of course what we are really talking about (whispering about, thundering about, shaking our heads about) in those moments of condemnation and lament is an extreme instance of suffering or disutility. Of course “evil” is to “bad” what “wicked” is to “immoral”: a conceptual vestige of a pre-scientific, credulous past that we invoke for the sake of solemnity, empathy, or emphasis. A concept that—outside of horror films and fiction—is best analyzed in terms of nature’s frustration of the basic needs of sentient creatures, or as the effects of illness and ill-parenting. Yes, evil happenings have an excessive, egregious quality that makes them notable, even transfixing. But they are not, in the end, sui generis or metaphysically mysterious: neuroscience, medicine, psychology, and law have domesticated evil. Taken to its logical extreme, the doctrine that characterizes this camp would be that all evil is “natural” (a product of various causal processes in nature).

Others prefer to answer the questions about the origins of evil in terms of choice, agency, and will. For people in this camp, evil consists in malevolent intentions, malice with forethought, and self-conscious cruelty that leads to extreme suffering and tribulation. They may allow that there are contributing factors and preconditions, but ultimately hold the agents themselves responsible for evil. Note, however, that the appeal to human free will can also be seen as an effort to domesticate evil—to make it explicable in terms of familiar concepts, to set it on a continuum with other, familiar acts and events. Taken to its logical extreme, the doctrine held by people in this camp is that all evil has “moral” origins—it is a product of choice or agency of some sort.

This debate about the roots of evil plays out not only in philosophy seminar rooms and psychology labs, but also on cable news stations and op-ed pages. People in the second camp tend to the political right, and sometimes even make a show of using “evil” because they think that people in the first camp (who tend to the political left) are uncomfortable with the idea of personal responsibility and blameworthiness.

I said these were the two opposing camps. In truth there is another one—one that used to be very popular but now seems sparsely populated, at least among philosophers. People in this third camp eschew efforts to domesticate evil; for them, what we mean by “evil” is not equivalent to what we mean by “bad” or “wrong” or even “very very very bad” or “very very very wrong”. In other words, evil is not just illness, misfortune, or malevolent choices by another name but rather a positive, substantial rottenness in the universe. It is, or has its origin in, some non-agential force or shadow side of reality—something spooky, imperceptible, but out there (“in them woods”).

2. Two Distinctions in Evil: Kinds and Origins
The late antique (Plotinus, Proclus, Augustine, Boethius), medieval (Anselm, Ibn Sina, Aquinas), and early modern (Descartes, Leibniz, Bayle, Kant) eras contain sophisticated traditions of reflection on questions about evil—about its being or non-being, its intrinsic features and natural manifestations, and its origins in nature, will, or supernature. Over the course of that centuries-long discussion, two main distinctions emerged.

The first main distinction has to do with the nature or kinds of evil: is evil at bottom just an empirical phenomenon—something that is given in the causal, phenomenal world of our experience? Or is there a deeper, metaphysical aspect to some evils? Note that this is not an exclusive distinction: people who endorse the idea of metaphysical evil typically assume that it also has an empirical character or manifestation.

Suppose, for example, we come across the sort of scene that drove Friedrich Nietzsche mad in Turin: a coachman mercilessly beating his horse (Prideaux 2018). In this version of the case, however, suppose that the coachman’s cruelty is a response to his having been recently diagnosed with terminal cancer. So here there is certainly some empirical evil: the cancerous disruption to the body, the cruelty of the man, the pain of the horse. Some philosophers will say that there is also metaphysical evil: neither the man nor the horse is metaphysically perfect, and so on the Absence Theory considered below (section 3.1.1) both are in that respect evil. We might also regard the body and character of the man as corrupted and conclude that he is metaphysically evil in a “privative” way too (see section 3.1.3). Metaphysical evils like these are distinct from the empirical evils of the cancer, the cruelty, and the suffering of the horse, though on many accounts they are the ground of the latter.

The second main distinction has to do with the origins of evil, and tracks the differences between the three camps mentioned above. The first option here is to say that a specific evil arises entirely from natural phenomena for which no one is responsible. In the case of the man and his horse, it is common to think that their metaphysical finitude and incapacities, as well as the cancerous tumor and the canine pain, are “natural” in this way: they seem to be based in facts about the natures, events, and causal laws involved.

An alternative is to say that a specific evil has its origin in moral actions and intentions. Applied to our case, it is common to think that the man’s agency—the choice to beat his vulnerable steed—is the origin of the cruelty and the pain. If there is metaphysical evil here, then it too might have a moral origin: on some religious pictures, for instance, the corrupted human nature that leads to disease, cruelty, and enmity between him and other creatures is a result of free choice on the part of his primordial ancestors. (If there is agency in any non-human creatures—animals or angelic—then it would also fit here.)

The third (and now-quite-unpopular) view about origins says that a specific evil arises ultimately not from nature or from choice but from something that is both supernatural and non-agential (call this a “spooky non-agential” origin). On such views there is a dark force or side of reality that is the ultimate origin of, say, the metaphysical evil in the man’s nature. It may also be the ultimate source of the empirical evil involved.

Three further preliminary notes:

The second main distinction here is often regarded as exclusive with respect to a specific evil, since we are asking about its ultimate origin. If the man’s cancer and ill-treatment of the horse originate entirely in the causal powers of the physical universe, then they are not also based in either free choice or supernatural spookiness (and vice versa). “Typically” here is key, however, since a compatibilist picture of free will (O’Connor & Franklin 2018 [2021]) says that free choices themselves are determined by natural causes. On that view, perhaps, the origin of the man’s cruel act is both moral and natural. Compatibilisms between natural and spooky origins are also conceptually possible.
Although some theorists think that all instances of evil (whether metaphysical or empirical) are grounded in just one of these ultimate origins, most will allow that different evils have different ultimate origins. For instance, someone might coherently think that the man’s cancer has a natural origin, that his cruelty has a moral origin, and yet that the ferociousness of the beating has a spooky or “dark force” origin. Section 4 considers some historical efforts to suggest that all evils are ultimately moral in origin.
Although these conceptual distinctions are fairly clear, there is terminological variation in the historical and contemporary literature with respect to the term “natural evil”. Some philosophers and theologians use “natural evil” to refer not to an origin but to a kind—the kind that is here called “empirical evil”. When the distinctions are maintained, however, it should be clear that they are orthogonal: both metaphysical evil and empirical evil can have natural, moral, or spooky origins (see Figure 1).
Origins:	Natural	Moral	Spooky non-agential
Kinds:	 
Metaphysical	The finitude, susceptibility to pain and disease, and other incapacities essential to the coachman and the horse and that are ultimately based in facts about natures	The finitude, susceptibility to pain and disease, and the inclination to cruelty that are essential to the man and the result of damage to his nature and/or that of his species, which damage is ultimately based in facts about immoral acts and intentions (e.g., Original Sin)	The finitude, and susceptibility to pain and disease in the coachman and the horse, and the corruption in the coachman’s nature and/or that of his species, and that are ultimately based in facts about a dark force or shadow-side principle in reality
Empirical	The cancer of the coachman, whose ultimate explanation consists in causal facts about natural phenomena	The cruelty of the coachman and the suffering of the horse, whose ultimate explanation consists in facts about the man’s immoral acts and intentions.	The cancer, anger, cruelty, and suffering of the coachman and the horse whose ultimate explanation is based in facts about a dark force or shadow-side principle in reality.
Figure 1. The Table of Evils, applied to Nietzsche case

The distinctions represented in the Table of Evil are the topics of Sections 3 and 4. Sections 5 through 7 look at three varieties of evil (systemic, symbolic, and radical) whose positions within the Table are more difficult to discern.

3. Kinds of Evil: Metaphysical and Empirical
The first key distinction is concerned with the kinds of evil—with what evil is or consists in, and thus with where and how it manifests. Again, the distinction is not exclusive: someone might hold that there is both metaphysical evil and empirical evil, and that the latter is typically a manifestation of metaphysical evil. Someone else, however, might hold that there is no such thing as metaphysical evil, and that all evils can be accounted for at the empirical, causal level.

3.1 Metaphysical Evil
Many of the traditional kakologists believed in metaphysical evil—i.e., evil that has to do with the way things exist or fail to exist. Typically, metaphysical evil is supposed to be a function of a thing’s nature and characterized by a kind of unintelligiblity. As we have seen, many such theorists also typically assume that metaphysical evil has empirical manifestations.

Both metaphysical and empirical evil have been described in terms of four main theory-templates: Absence, Matter, Privation, and Real Property. These templates are laid out in more detail in the discussion of metaphysical evil in this section, and applied again in section 3.2’s discussion of empirical evil.

3.1.1 Absence Theory
The Absence Theory of Evil has its origins in the Platonic idea that there are different “degrees of being” corresponding to the number and kinds of capacities a thing has. Roughly speaking, the more numerous and impressive a thing’s capacities, the more real and thus better it is, metaphysically-speaking. A dog cannot stand erect; an ape can. A rock cannot pass through walls; an angel can. These lacks or absences are essential to being the kind of finite creature that a dog or a rock is; miracles aside, a rock is just the kind of thing that cannot pass through walls. All the same, the lack of that ability is an evil.

Absence theorists typically add a “plenum” thesis here: it is fitting, or beautiful, or perhaps even necessary, that all the different degrees of being are exemplified, and thus that every link in the “great chain of being” is occupied. Evil is a function of the way things ought to be or even must be.

Absence Theory was popular across antiquity, but it was particularly attractive to philosophers in monotheistic traditions because it allowed them to say that evil is not a thing—and thus not some thing that a good, all-powerful God created or sustains. Anselm writes in On the Fall of the Devil:

Just as nothing that is not good comes from the Supreme Good, and every good is from the Supreme Good, likewise nothing that is not being [essentia] comes from the Supreme Being [essentia], and all being is from the Supreme Being. Since the Supreme Good is the Supreme Being, it follows that every being is a good thing and every good thing is a being. Therefore, just as nothing and non-being [non esse] are not being [essentia], likewise they are not good. So, nothing and non-being are not from He from whom nothing is unless it is good and being. (De Casu Diaboli, v.1, 235; translation by Sadler [Other Internet Resources])

A lingering problem in the theistic context, however, is that Absence Theory entails that finite things have a degree of evil just in virtue of not being at the top of the chain. So even if God does not create evil (because absence is uncreated), God creates beings that are essentially evil.

3.1.2 Matter Theory
Plotinus rejects Absence Theory for a related reason: it compels us

to say that there are evils in the higher world too; for there the Soul is inferior to Intellect, and Intellect is lesser than [the One]. (Enneads I, 8, 13)

For Plotinus, it is not merely being lower than the highest One that makes something evil; rather, evil consists in being so low as to be associated with matter. Indeterminate or “unformed matter” is the final term on the cosmic chain from being to non-being—it is as far away from intelligence as possible, and thus equally far away from goodness. This is the Matter Theory of Evil that was influential in various gnosticisms, early Christian heresies, and late antique platonisms (see O’Meara 2019).

3.1.3 Privation Theory
Augustine, as well as many scholastics and early moderns, rejected Matter Theory on religious grounds. God created matter, and so it cannot be bad in itself. Instead of reverting to pure Absence Theory, however, these monotheists developed the Privation Theory of Evil. This can be construed as Absence Theory plus the Aristotelian idea that goodness is relative to a thing’s kind. Individuals of different kinds have different ends dictated by their natures: as long as the dog achieves the ends set out by its nature, it counts as fully good of its canine kind, even if it essentially lacks the good-making capacities (or “realities”) of beings higher on the chain. (For more on Augustine’s version, see King 2019; on Aquinas, see Davies 2019).

Privation Theory thus reduces pressure on monotheism: evil is not a being but rather an absence, and so God did not create it. Moreover, evil is the kind of absence that is not a function of the essential natures of things, and so God cannot be faulted for creating things that are essentially evil. Failing to accomplish the end set out by one’s nature—failing to be the way one ought to be—is a privation, however, and so it is evil.

Friends of Privation Theory offer different accounts of how and why things fail to accomplish their natural end. Most ascribe the failure to something in the individual creature—culpable ignorance, Original Sin, free agency—rather than to God, thereby grounding metaphysical evil in moral evil (on which see section 4.1 below). The extent to which these appeals to privation succeed in getting God off the hook is, naturally, controversial.

Plotinus’s view is effectively a hybrid of Privation Theory and Matter Theory: for him, privation only occurs when some of the matter in something is unformed or unmastered by a form (see Enneads I, 8, 5, 19–26). But even if other Privation Theorists do not view privation in this way, the conception of evil as infinitely distant from reason and intelligibility survives. In other words, Ignorance, Original Sin, and other misuses of freedom are not necessarily a function of irrational matter, but they are still opposed to rational mind. Descartes is illustrative here: for him, the only teleology (and privation) in the world relates to the souls—and in particular the wills—of human beings. Material substance is mechanistic, and aggregates of it (in the forms of animals and plants) can be explained mechanistically, without invoking teleology or kind-relative values (compare Newlands 2019).

3.1.4 Real Property Theory
A fourth major account of the nature of metaphysical evil takes it to be something more substantive than absence, privation, or unformed matter. Call this the Real Property Theory of Evil: evil is some sort of reality—a determinate feature of certain finite beings (see J. Russell 1977, 1981, and Frankfurter 2006). Some versions of this picture say that evil is ultimately dependent on the good. Other, more Manichaean versions take the two to be coeval and independent, locked in an eternal axiological struggle. Although the relevant “property” here is supposed to have more reality or substance than mere absence or privation, there is still a kind of unintelligibility to it. The evil side of reality, the dark force, or the malevolent will is a kind of black hole for the “natural light” of reason—positively real, but inaccessible to complete explanation.

3.2 Empirical Evil
Empirical evil is a capacious category: it covers bodily pain, damage, and disease as well as the psychological concomitants or effects of these physical phenomena—suffering, terror, depression, mental illness. Traditionally, it has also included social ills such as oppression, poverty, and structural injustice (see Sharpe 1909). Philosophers who believe in metaphysical evil often take them to manifest in empirical evils. Philosophers who reject metaphysical evil, by contrast, take the empirical kind to be fundamental.

Calling pain an evil might raise eyebrows, since clearly some pain is beneficial: it protects us from collisions, diseases, and predators. But the idea is that, whatever instrumental uses it has, pain qua phenomenal quality—the feeling of it—is intrinsically bad (setting aside for these purposes tricky cases like that of the masochist who takes pleasure in experiencing pain). Pain in the horrendous amounts and kinds that we encounter in human history as well as in the “charnel house” of evolutionary history is sickeningly and obviously bad (Murray 2008; Martin & Watkins 2019).

The templates used to characterize metaphysical evil above can be applied to empirical evil, too. An Absence Theory of empirical evil construes it simply as the absence of physical-psychological states of pleasure, health, stability, justice, and even life. Matter Theory regards pain, disease, mental malaise, and social ills as effects of our standing as material beings, vulnerable to the “matter” in our organism breaking down or coming into conflict with other parts of material creation. Privation Theory says that empirical evil is the absence of some such good which ought to exist. There will then be different accounts of why the good ought to exist, and why it doesn’t—some of these will be based in a theory of metaphysical evil.

A Real Property Theory of empirical evil, by contrast, insists that pain and suffering are positive realities and not mere absences (it is thus compatible with Matter Theory, but not with Absence or Privation Theory). As Malebranche notes, this is problematic in theological contexts, since God is supposed to be the ground of all positive beings (see Malebranche 1674–5 [1997: 348 and 392]). In a letter to a mathematician named Arnold Eckhard, G.W. Leibniz discusses this issue, and suggests “with some scruples” that “pain too is a perfection” (1677 [1969: 177]). That is a fairly unusual view, one that is made particularly poignant by the fact that Leibniz himself spent the last few years of his life in immense pain. He apparently had a habit of sleeping in a chair near his writing desk, and it

led to his having an open sore on his right leg. This caused him difficulty in walking; he tried to remedy it, but only by putting blotting paper on it. Later, to reduce the pain and to make the nerves insensitive he had a number of wooden clamps made, and these he screwed onto himself wherever he felt pain. I suspect that by doing this he so damaged his nerves that eventually he could no longer use his feet and had to stay in bed. (Guhrauer 1842 [1966: vol 2, p.336]; quoted in Mates 1989: 29)

Although such pain is a “perfection” and thus a real property, for Leibniz, it still involves a kind of weakness or imperfection in the person who has it, and so God cannot, in the end, exemplify this “perfection”. Rather, God’s possession of maximal pleasure is somehow sufficient to ground the “reality” that is found in both pleasure and pain (Leibniz 1677 [1969: 177]). This seems fishy, and other philosophers argue against Leibniz that if pain is a real property (rather than a mere absence), then God as the “ground of all reality” must indeed exemplify it. Some recent theologians and philosophers even welcome this idea, arguing that a conception of God as grounding empirical evil by suffering it has advantages for projects in theodicy (Hartshorne 1984; Wolterstorff 1988).

Mental illnesses can be classified as “empirical evils” if we assume that they have empirical causes and neural bases (Bhattacharjee 2018). Such illness often generates further empirical evil in the form of inexplicably cruel or destructive behavior to self and others, as will be familiar to anyone who has tried to live (or love someone) with mental illnesses like posttraumatic stress disorder (PTSD), borderline personality disorder, schizophrenia, and so on. It is unclear whether the theologians just mentioned would want to say that this kind of empirical evil, too, must have its ground in the divine by way of exemplification. The theological consequences of such a suggestion seem unattractive, to say the least.

4. Origins of Evil: Moral and Natural
4.1 Moral Evil
Moral evil is metaphysical or empirical evil that arises out of the acts or intentions of agents: other traditional terms for it include “sin”, “wickedness”, “trespass”, and “iniquity”.

Philosophers in the Abrahamic tradition typically hold that we can be damaged by moral evil even before we have performed any actions whatsoever. Augustine, for example, characterizes Original Sin as a result of a primordial choice that damaged our very nature such that each member of the species is born already worthy of infinite punishment and strongly inclined to engage in further moral evil. So this is a case of metaphysical evil that has its origin in moral evil: bad choices on the part of the forefather led to damaged natures all the way down the spermatic line.

Such metaphysical evil will often manifest in empirical evil (although it need not do so); it also makes it likely that there will be further metaphysical evil. Thus an initial moral choice starts off a kind of snowballing into hellishness. It is clear why ancient and medieval and even some contemporary responses to the “Unde Malum?” question focus on how a morally uncorrupted creature (Adam, Eve, or Lucifer) could have started the process in the first place (see, e.g., Anselm De Casu Diaboli, vol.1 and Johnston forthcoming).

Kant appropriates this idea in a modern context, arguing that we are all originally afflicted by a propensity to moral evil (Hang zum Bösen), even apart from any specific actions that we perform. This “radical evil”—the metaphysical evil at the root of our nature (“radix” = Latin for “root”)—is something for which not Adam but we as individual agents (or perhaps the species itself) are somehow responsible. And this is reflected even in garden-variety peccadillos: any act that subordinates commitment to the moral law to something else is an expression of radical evil. This makes it clear that people who use “radical evil” to refer to something particularly horrendous or awful (Arendt 1951, Bernstein 2002) are departing from the Kantian concept (see Section 7 below). That said, whether it is intelligible to say that we are culpable for a status that is not temporal is a notoriously open question in Kant-interpretation (see Card 2010, Wood 2010, and other essays in Anderson-Gold & Muchnik 2010).

Although moral evildoing often leads to empirical evil, it needn’t do so, at least on non-utilitarian accounts. Some crimes can be “victimless” at the empirical level.

Once again, the four traditional theoretical templates can be used to further characterize the evil acts and intentions that make an evil “moral”. Absence Theory says that the absence of a good will—either because something lacks a will altogether, or because its will is not good—is evil just by way of being an absence. Privation Theory says that a will is evil when it ought not have the absence of good orientation that it does. Matter Theory says that our embodiment and other engagements with matter explain the misorientation of our will. Real Property Theory insists that a will that is oriented to the bad is a real thing, and that its orientation to the bad is a real feature of it, not just a privation.

Declaring that moral evil is rooted in the will does not fully explain it. We would need a further account of how evil choices are made—of what sort of moral psychology could explain them. For Kant, explanation ceases at some point; evil choices, at bottom, are irrational—surds that we can identify and impute, but never fully explain. “There is no conceivable ground for us, therefore, from which moral evil could first have come in us” (Kant 1793 [AK 6:43; 1998: 64]).

Other philosophers offer partial explanations. Augustine says that although there is no determining cause of the devil’s choice, it is able to be partly “rationalized” in terms of Satan’s self-obsessed delight in his own powers (see King 2019). This is an early version of what is now called a “dispositional” account of evil agency (L. Russell 2014: ch.10; Kamtekar 2019). Early Islamic interpreters of the Qu’ran, by contrast, offer explanations in terms of ignorance: there is something that the supremely evil agent (Iblis) didn’t realize, or an inference that he failed to make, and this is what explains his orientation to the bad (see Germann 2019).

Moral evil is what contemporary people—including philosophers—tend to have in mind when they talk of “evil”. Extreme forms of it are viewed by many philosophers view as “unintelligible”—as defying ordinary explanation in significant and threatening ways. Such extreme and unintelligible moral evils are what many philosophers (though not Kant) are referring to when they speak of “radical” evil (Arendt 1951; L. Russell 2014; see also Section 7).

4.2 Natural Evil
Natural evil refers to metaphysical and/or empirical evils whose origins are “natural”—i.e., grounded in the natures of things and/or the natural laws. The very nature of a horse makes it incapable of language: if that incapacity is a metaphysical evil (as it would be on Absence Theory), then given its origin it also counts as a natural rather than a moral evil. Likewise, cancer, pandemics, earthquakes, meteor strikes, aging and perhaps even death itself are (typically) regarded as natural rather than moral evils. In the tradition there are characterizations of natural evil that use one or more of the four templates discussed earlier: Absence, Privation, malfunctions in Matter, or some other Real Property.

Outside of religious contexts, however, many philosophers (and people generally) will be reluctant to characterize hurricanes, diseases, and meteor strikes as a source of “evil”. The very idea of natural evil seems most at home in theological debates: can the Author of Nature be supremely good, wise, and powerful and yet still create a world that contains so much pain and suffering, not to mention Category Five hurricanes, animal predation, and Alzheimer’s Disease?

There are different kinds of responses to the problem of natural evil in theological traditions: Aesthetic responses say that we don’t presently have the right perspective to see the overall beauty of the natural system, and thus that there really is no natural evil; Soul-making responses say that this present vale of natural evil is justified because it gives us the chance to become virtuous; Skeptical theistic responses say that given our limited faculties we cannot reasonably expect to understand why God would allow natural evils. (For more on these responses see M. Adams 1998, Tooley 2002 [2019]).

A very different kind of response involves recharacterizing at least some natural evils as moral evils. For instance, we might focus on ways in which human activity has set in motion the kinds of environmental, climatic, microbial, and biospheric changes that lead to “natural” disasters, pandemics, famines, and other empirical evils. Or, in a more religious context, we might seek to explain the suffering and misery caused by “nature” by appeal to sin, karmic law, or divine justice.

On some versions of this view, the morally responsible agent can be someone other than the victim: Adam sins, and now the non-human “creation has been growning” (Romans 8:22, NRSV)—and makes the rest of us groan along with it. Or: we emit the greenhouse gasses, and our descendants three generations later suffer.

Other versions insist that the fault lies with the victim of natural evil himself: Eliphaz the Temanite says to Job

Think now, who that was innocent ever perished?
Or where were the upright cut off?
As I have seen, those who plow iniquity
and sow trouble reap the same.
By the breath of God they perish,
and by the blast of his anger they are consumed.
The roar of the lion, the voice of the fierce lion,
and the teeth of the young lions are broken.
(Job 4: 7–10, NRSV)

Eliphaz suggests that if Job continues to suffer, then there must be some explanation for these natural evils in Job’s own past behavior. The risk in this kind of doctrine will be obvious to less ancient sensibilities: to suggest that a victim of natural evil must be morally responsible for it seems like the very definition of adding insult to injury.

Another way to recharacterize natural evil as moral evil is by appealing to the actions of moral agents who are neither divine nor human. This appears to have been Augustine’s position in places; more recently, it has been invoked by Alvin Plantinga as an at least broadly logically possible scenario which could be used in a “defense” against the logical problem of evil (Tooley 2002 [2019]). The scenario says that

[n]atural evil is due to the free actions of nonhuman persons; there is a balance of good over evil with respect to the actions of these nonhuman persons; and it was not within the power of God to create a world that contains a more favorable balance of good over evil with respect to the actions of the nonhuman persons it contains. (Plantinga 1989: 58)

A final way to recharacterize natural evil as moral would simply be to hold God alone morally blameworthy for it, since God created a world in which sentient creatures suffer so terribly. Obviously this would not be a winning strategy if the goal is a successful theodicy. Indeed, some theistic traditions block this route a priori by saying that it is conceptually impossible for God to do wrong: a perfect creator is either unbound by moral principles, or essentially incapable of violating them (see Murphy 2017).

If these efforts to locate a moral basis for natural evil (whether empirical or metaphysical) are unsuccessful, then “natural” remains a distinct category of evil’s origin.

Here again is our Table of Evils, now with more examples that go beyond Nietzsche’s horse case:

Origins:	Natural	Moral	Spooky non-agential
Kinds:	 
Metaphysical	The finitude and other limitations that are essential to an individual or species-nature and that are ultimately based in facts about natures.	The finitude, limitations, and corruption that are essential to an individual or species-nature and the result of damage to natures, which damage is ultimately based in facts about immoral acts and intentions (e.g., Original Sin)	The finitude, limitations, and corruption that are essential to an individual or species-nature and are ultimately based in facts about a dark force or shadow-side principle in reality.
Empirical	Pain, suffering, and illness (including mental) whose ultimate explanation consists in causal facts (e.g., aging, accidents, genetic defects, disasters and other natural phenomena).	Pain, suffering, and illness (including mental) whose ultimate explanation consists in facts about immoral acts and intentions (e.g., interpersonal violence, social ills, self-harm)	Pain, suffering, and illness (including mental) whose ultimate explanation consists in facts about a dark force or shadow-side principle in reality.
Figure 2. The Table of Evils, with general examples

Some types of evil do not seem to fit nicely in the Table of Evils. The three dealt with in what follows are systemic evil, symbolic evil, and radical evil. The nature and the origins of such evils are difficult to discern, and may require an expansion of the Table.

5. Systemic Evil
Systemic evil is the kind of evil that exists at the level of systems or groups rather than merely at the level of individuals. Organized structures like governments, corporations, teams, and religious institutions can be evil in this way; so can more loosely-organized systems such as “Academia”, “White Supremacy”, and “Wall Street”. Indeed, Google includes the motto “Don’t Be Evil” in its Company Code of Conduct (for the origins of this, see Chang 2019); disgruntled employees later sued the company alleging that it had violated that pledge (Allyn 2021).

Systemic evil seems empirical rather than metaphysical, but its origins are difficult to identify. It does not appear to be a merely natural phenomenon, or a spooky non-agential one, and yet its supra-agential character makes it seem not entirely moral either.

Hannah Arendt’s early work on totalitarianism (1951) depicts systemic evil as a kind of empirical evil for which no individual or even collection of individuals is fully responsible. The cogs in the machine, as well as the leader or leadership, may be the origin of some of the harms involved, but the evil of the whole structure (on Arendt’s view) is somehow greater than the sum produced by its parts.

Racism is another prominent example of systemic evil. Recent accounts of entrenched racist structures in certain societies (the United States, for instance) suggest that such evil has its origin not merely in actions and intentions but also in omissions and passivity: people who do not explicitly support the systemic evil can count as agents of it. If this is right, then active work against the evil in question—“anti-racist” activity rather than “non-racist” activity, for example—becomes morally required (Kendi 2019). But even such active opposition efforts may not be sufficient to avoid complicity in systemic evil: some theorists argue that even anti-racist people living in racist societies are “tainted” by its evil all the same (Rothstein 2017). That leads to the next kind of problem case.

6. Symbolic Evil
Symbolic value is a less familiar idea in philosophy than it is in Anthropology, Religious Studies, and the other social sciences that deal with production, exchange, and consumption. The main idea is that an object or act can have far more “symbolic” value to a certain individual than its exchange or monetary value on some market or other, typically because of its causal history. That bauble given to you by a now-deceased friend has far more symbolic value (to you, at least) than the monetary or exchange value that it would fetch at auction. This is because the gift had its origin in the generosity of a friend who has since died, has the ability to invoke his memory, and so on.

Symbolic disvalue works in an analogous way. Products that fetch a certain monetary value on an open market may have significant symbolic disvalue that is not reflected in their price. This disvalue often arises, again, from the product’s provenance: an industrial chicken sandwich is a result of the obscene degradation of animals, workers, and the environment on the part of an industry that pays revolving-door lobbyists to promote food policy that, in turn, keeps production costs and prices artificially low, in part by externalizing most of the harms it causes. In some cases, such disvalue may be significant enough to make purchasing the product wrong, even if doing so does not lead, causally, to any actual harm or rights-infringement (see Chignell 2016).

The term “evil” is typically only applied in cases of symbolic disvalue when there is something excessive about them. Stepping on a flag or consuming a chicken sandwich might have some symbolic disvalue, but it would be strange to say that these are cases of symbolic evil. Having anything to do with soap made from the body fat of people murdered at the Stutthof concentration camp, by contrast, seems downright evil, even if the intended consequences (i.e., getting oneself clean) are good. Indeed, the symbolic evil attached to such a product may “touch” everyone who was part of the society that permitted its manufacture, whether they were directly involved or not. The evil of sustained chattel slavery may have a similar sort of symbolic power, even generations later.

Both the nature and the origins of symbolic evil are hard to characterize; thus it is hard to see where it fits in the Table of Evils. Is there a metaphysical aspect to symbolic evil, or is it fully empirical? Is its origin entirely natural, or moral, or is there something spooky and non-agential about it? A related set of difficulties has to do with the fact that, in some contexts anyway, the transfer of symbolic disvalue operates via the complex logic of “taint”, contagion, or uncleanness. Some philosophers who write about symbolic value argue that the only way to avoid being “touched” by such evil is via active and explicit dissociation—a symbolic “standing with the good” (R. Adams 1999). In other words, even if our abstinence does not make a difference, we must symbolically oppose an evil practice by, say, explicitly signaling opposition and (where possible) refusing to consume or benefit from its results (see Hill 1983; Appiah 1986).

7. Radical Evil: Four Conceptions
Kant is the source of “radical evil”, but his way of using the term is now out of favor. In fact, most people writing on this issue use the term in a precisely non-Kantian way to refer to something spectacularly excessive—an act or event whose badness is deeper and more mysterious than the badness of ordinary states or activities (Bernstein 2002).

In her famous account of the trial of Adolf Eichmann in Jerusalem, Arendt (1963) promoted the idea that even the worst evils can be “banal” and bureaucratical. A decade earlier, however, she conceived of “radical evil” quite differently. In a March 1951 letter to Karl Jaspers she wrote:

We know that the greatest evils or radical evil has nothing to do anymore with humanly understandable, sinful motives. What radical evil is I don’t know, but it seems to me to somehow have to do with the following phenomenon: making human beings as human beings superfluous. (Arendt & Jaspers 1992: 166, emphasis added)

Arendt went on in the letter to insist that “making human beings as human beings superfluous” is not the same as treating them as “mere means to an end”. She thus rejected the Kantian view that radical evil—no matter how benign or awful the effects—has its root (Latin: radix) in the willful violation of the categorical imperative by individual free agents (see Louden 2010: 98).

Jaspers’s view of radical evil, by contrast, was consistently non-exotic and Kantian: “there is evil because there is freedom. It is only possible for the will alone to be evil” (Jaspers 1947 [1958: 532], emphasis added). According to one commentator, this focus on human freedom as the root of all evil is “significant and commendable”:

Kant [and following him Jaspers] refuses to cater to our prurient craving for a special account that applies especially to the most extreme cases of evil…. He fears that occupying our imaginations with extreme cases of evil may be merely a way of indulging some of our nastier human traits—rationalizing our resentment and vindictiveness by supplying it with an object that would seem to justify it. (Wood 2010: 157)

The concerns expressed in these passages are common in discussions of the nature and origins of evil. Call them Jaspersian concerns for short. They are second-order concerns about how we should conceive and speak of evil, especially when calling it “radical”. The concerns fall into two broad kinds: (1) concerns about exoticizing wrongdoing with excess-locutions like “evil” and “radical”; and (2) concerns about tainting people and things that are touched by such evil, beyond the straightforward condemnation of the perpetrators (for more on such concerns, see Card 2002, Cole 2006, and Calder 2013 [2018]).

The Kantian conception of radical evil is considered in the next sub-section. Sections 7.2–7.4 survey three other conceptions and look at how each might raise Jaspersian concerns.

7.1 Radical Evil as Violation of the Moral Law
Kant argues in Religion within the Bounds of Reason Alone (1793) that free choices against the moral law are unintelligible in the sense that they are irrational; our propensity to make them is thus an inexplicable mystery at the “root” (radix) of our moral psychology. The post-Kantian idealist F.W. J. Schelling, picking up the refrain, rejects the Privation Theory of radical evil in favor of the view that it has its origin in the positive, irrational decision to prefer self-advantage over the moral law, though “just how the decision for good or evil comes to pass in the individual, that is still wrapped in total darkness” (Schelling 1809 [1936: 59]).

On the Kantian view of radical evil, then, there is a normative sense in which it is “unintelligible”: it is an irrational propensity and thus cannot be “understood” in the sense that it cannot be sanctioned by proper reason. Likewise, an immoral choice cannot be fully explained: full explanations appeal to good reasons, and there are no good reasons for wrongdoing.

There is a broader sense of “understand” or “explain”, however, in which such evil is no mystery at all: it is the most depressingly familiar thing in the world. Everyday immoral agents presumably see what they are doing under the aspect of some good or other—good for their company, good for their bonuses, good for their reelection efforts—even if they also know that what they are doing is ultimately wrong. If we interpret the war cry of Milton’s Satan—“Evil be thou my good!” (1667, Paradise Lost, Bk IV, line 110)—as implying that he is taking evil under the guise of the good, then it is not absurd. But it is still not wholly intelligible, either.

The Kantian conception of radical evil, then, says that that the unintelligibility of wrongdoing does not prevent us from assigning blame, holding perpetrators responsible, and refusing to taint the innocent. This explains why Jaspers found the conception so attractive:

To rank the will to happiness, which dominates among men’s motives, above the unconditioned law that shows itself in reason—that is the root of evil, the “propensity” which Kant calls “radical evil”. (Jaspers 1962: 321)

7.2. Radical Evil as Choosing Evil for its own Sake
Some philosophers, and more than a few novelists and screenwriters, find the view that agents are always choosing under “the guise of the good” as inadequate to the psychology of extreme malevolence. It may have suited the melioristic conceptions of the Enlightenment, but the well-publicized, mechanized horrors of the recent past allegedly demand a bleaker picture of perpetrator psychology. Serial killers, murderous dictators, torturers, derivatives traders: the idea is that at least some of these malign actors see their own actions as atrocious—as making the world worse rather than better on the whole (and sometimes worse for themselves)—and yet still choose to perform them. They are thus not saying “Evil be thou my good”, at least on the interpretation just offered. Rather, they are self-consciously male-volent: they will the bad under the aspect of the bad. And yet they are not insane—this is what makes their actions especially difficult for the rest of us to understand. This is a second common way in which the term “radical evil” is used. (See Pauer-Studer and Velleman 2015 for a reading of Milton according to which Satan’s war cry is interpreted in this way.)

Augustine reports in his Confessions that during the pears incident he took pleasure in

the theft and sin itself.… Behold, now, let me heart tell Thee what it was seeking there, that I should be gratuitously wanton, having no inducement to evil but the evil itself. It was foul, and I loved it … I loved my own error – not that for which I erred, but the error itself. (Confessions Bk 2, Chap. IV, paragraph 9 [1876: 30])

It is tempting to say that in “loving the sin itself”, Augustine was in some sense taking it to be good. But he at least gestures here at this second conception of radical evil: some acts can be performed under the aspect of their own abject deformation and rottenness—as ultimately bad even for the agent himself.

As we have seen, the Prussian philosopher who coined the term “radical evil” 1400 years later did not think that it can involve choosing evil for its own sake. In fact, Kant argues that such a “diabolical” conception of evil is incoherent or at least psychologically inapplicable to human beings. Kant’s skepticism, however, did not prevent Dostoyevsky from composing Notes from the Underground (1864) as an extended and somewhat plausible portrait of one man’s effort to choose evil qua evil. And there are more recent accounts of people who confess—in private diaries or braggadocios depositions—that they did what they did because it was evil.

In this context, consider (if you can bear it) Stone 2009’s portraits of the worst serial killers: although some perpetrators report that they take what they are doing to be good—ridding the world of “garbage women”, giving someone his “just deserts”, correctly following the orders of the voices in their head, and so on—others openly admit that what they are doing is bad, wrong, evil, despicable, and so on. In such cases, the perpetrators are not making a series of unsound inferences, or mistaking the bad for the good. Rather, they seem to be engaged in a self-conscious turning away from anything that could be regarded as good by anyone. Radical evil on this conception is sometimes described as a self-conscious turning away from being itself (Eagleton 2009: 16; L. Russell 2014: 23).

If Augustine’s confessions about the pears—or these more spectacular recent confessions—are accurate, then there may be (contra Kant) a baffling diabolical state that some people can fall into, and that the rest of us cannot entirely fathom. The unintelligibility here thus threatens to raise Jaspersian concerns about exoticization and taint.

7.3. Radical Evil as Repudiation of the Moral Law
A third conception of “radical evil” takes the term to refer to acts or practices (like slavery and genocide) that do not merely violate rational principles but also constitute an effort to repudiate moral rationality altogether, or at least to transcend it. When we steal or lie, there is a sense in which we might be failing to treat others with the respect that they deserve. And as we have seen, Kant does not hesitate to view such irrational choices as stemming from our propensity to “radical evil”. But radical evil on this third conception is different: it involves an intentional refusal to acknowledge that some group of persons has any moral standing at all—the kind of moral standing that would prohibit us from instigating or complying with their humiliation, degradation, or extinction.

Radical evil of this sort, in other words, denies the universal scope—and thus, perhaps, the very existence—of the moral sphere altogether. It is anti-rational rather than merely irrational: this is what threatens to make it incomprehensible or inexplicable in a unique way. This is also presumably what the early Arendt means when she says that radical evil involves making human beings “superfluous”.

By way of analogy: suppose that ordinary moral wrongdoing is like making a bad move in chess, or like cheating by moving one’s pieces in an illegal way when one’s opponent isn’t looking. A radically evil act on the present conception, by contrast, would be like crushing all of the opponent’s pieces and upending the table. The player who does the latter is no longer or perhaps never was a player: she cannot explain her behavior by saying that she was trying to win by making what turned out to be a bad move or a cheat. She cannot explain her behavior in terms of chess at all. Likewise, a radically evil act is supposed to transcend the terms of moral rationality.

The analogy extends only so far, however. That’s because there are still some reasonable explanations left to our non-player—explanations external to the game:

I was hungry, so I crushed all of your pieces and swept them off the board in the hopes that you would suggest lunch.

By contrast, it is not obvious how there can be any credible reason given for doing something that constitutes a denial of reason altogether. That is why radical evil in the repudiative sense threatens to be uniquely unintelligible and troubling. If someone—not a beast or a machine, but a human being—acts in a way that entirely disclaims not just an awareness of moral authority but also the basic rules of moral reason altogether, he effectively places himself in an antelapsarian state, unburdened with the knowledge of good and evil. His act asserts that he has transcended entirely the moral sphere—that what he does cannot be wrong.

Can human beings really perpetrate radical evil in this sense? Richard Ramirez, the “Night Stalker” serial killer in 1980’s Los Angeles, certainly took himself to be doing so. He repeatedly snuck into people’s houses to rape and kill them, sometimes cutting off body parts and taking them with him. After his fourteenth murder, he was caught and then boasted to his captors:

You don’t understand me… you are not capable of it. I am beyond good and evil…I love to kill people. I love to watch them die. I would shoot them in the head and they would wiggle and squirm… I love all that blood. (quoted in Stone 2009: 208).

This idea—that a human being could do something that would enact his own transcendence of moral norms, make other persons superfluous, and establish his own status as somehow “beyond good and evil”—is what concerned Karl Jaspers, and what he was hoping Arendt would resist. In her face-to-face confrontation with the quotidian, bureaucratic evil of Adolf Eichmann, she seems to have changed her mind (though see Cesarani 2007 and Margalit 2019 for a portrait of Eichmann that resists her famous “banality thesis”). But even if Arendt ultimately repudiated the repudiative conception of radical evil, others remain sympathetic to some version of it (e.g., Bernstein 2002 and Motzkin 2019).

7.4. Radical Evil as Systemic Evil: Human Beings Made Superfluous in Another Way
“Systemic” or “structural” evil exists at the level of groups, networks, races, and collectives rather than merely at the level of individuals (see section 5). The fourth conception of “radical evil” applies the concept to particularly rampant or entrenched evils of the systemic sort.

Although Arendt’s interest was in totalitarian state systems, contemporary philosophers are equally interested in non-state but still super-human systems: corporations, collectives, markets, artificial intelligences. These are now some of the most potent sources of value and disvalue at work in the world. As noted in section 5, the nature of systemic evil is empirical, but its origin is more difficult to grasp. Corporations have boardrooms and executive suites, and are even treated as persons by some legal systems, but (as investigation after investigation indicates) it is hard to find the heart of their darkness when assigning responsibility. Still-powerful systems of white supremacy are based in structures of which no individual human mind was or is fully aware (Rothstein 2017). Likewise, markets, algorithms, blockchains, and various forms of artificial intelligence arise out of innumerable individual human efforts, but are also designed to make human beings superfluous in a literal and unprecedented way. Some of this is and will be for the good—there are already fewer back-broken menial laborers in the fields or exhaust-guzzling toll collectors on the highways. But some will surely be for the bad, and in ways that are not foreseeable. Radical systemic evil—especially in a technological age—does not seem natural or spooky, but it does not seem fully moral either.

In sum: although natural, moral, and spooky non-agential conceptions of the origins of evil were the focus of most traditional discussions of the origins of evil, it looks like some varieties of radical evil have a different origin altogether—one that is not entirely intelligible. This raises Jaspersian concerns: a world in which (non-Kantian) radical evils are widespread is one in which ascriptions of moral responsibility are increasingly difficult to make.

1. On Poets: How to Judge Poetry?
This work, a dialogue in three books, was apparently quite widely read in the ancient world. While the Poetics seems to have received no echo in antiquity, On Poets seems to have acquired the status of a reference work on Aristotle’s aesthetics; the fragments that we have come from a wide array of sources, including Philodemus, Ps-Plutarch, Athenaeus, Diogenes Laertius, Macrobius, and Proclus. Many fragments deal with entertaining stories such as the birth and death of Homer, or the presentation of the rivalries between the poets. However, it would be a mistake to draw the conclusion that the whole work would have consisted of such stories without any theoretical background. Actually, two topics that are crucial in the Poetics seem to have been central in On Poets as well: mimesis and the shortcomings of poets.

When opening the Poetics, the reader is struck by the repetition of the word mimêsis (and the verb mimeisthai), to the point that it defines what is (what we call) a work of art. Very roughly, one might say that the word mimêsis has both a static, or “pictorial” aspect, and a “dynamic”, or “theatrical” aspect. According to its “pictorial” aspect, mimêsis designates the fact that in such and such mimetic work, the receiver recognizes a resemblance. In the Poetics, Aristotle gives us a very telling example in evoking a painting or a sculpture in front of which the beholder recognizes, about a character represented, that “such and such a character is so-and-so” (4, 1448b17). According to its “dynamic” aspect, it is rather the behavior of the one who makes a mimêsis, which may range from the mimicking of noises or gestures to theater enactment. In Latin, mimêsis was rendered as imitatio, which indeed could include both meanings; but the English “imitation” hardly works, and perhaps the best solution after all is to keep the Greek term, transliterated as “mimesis” (“representation”, or “depiction”, works for the first connotation, but hardly for the second). Of course, often the use of the word, especially in poetry, includes both connotations: a play is a mimesis both in the sense of an enactment of some actions that are made by the characters of the play, and we can also recognize them as being the actions of such-and-such characters. Traditionally, a poetic work was defined in its opposition to prose, by versification (see, e.g., Gorgias’ definition of poetry: “I consider and call poetry every speech that possesses meter”, Helen 8). Aristotle opposes that idea, on the grounds that it does not allow us to understand the specificity of a poetic work, by giving the example of Empedocles “who has nothing in common with Homer except for the metric form” (Poetics 1, 1447b17–18): we do not infer from the fact that Empedocles writes in the same kind of verses as Homer that his work is epic poetry! What distinguishes Homer from Empedocles is that the former wrote a mimetic work, whereas Empedocles is a “philosopher of nature” (phusiologos, b19), who did not compose a mimesis. This concern is also to be found in On Poets, where Aristotle asks this question:

Are we not going to say that even though not in verse, the so-called mimes of Sophron are works in prose and works of representation, and that the same goes for the dialogues of Alexameneus of Teos, which were written before the Socratic dialogues? (Janko 2011, F44a)

Since such written dialogues are meant to be a mimesis of dialogues that did, or rather might have taken place, they too must be considered as a kind of poetry (or what we would call literature). Athenaeus, who quotes that question, interprets this as an attack against Plato:

While in the Republic Plato expelled Homer and mimetic poetry, he himself wrote his dialogues in mimetic form, and he is not even the inventor of that genre: before him, Alexamenus of Teos and Sotion invented that type of work in prose. (Janko 2011, F44a)

We cannot say whether Aristotle also intended this to be an attack against Plato; and it is important to note that we do not find anything like an explicit rebuttal of Plato’s views in the fragments we have, but it is difficult not to see in the quoted question at least a certain irony against Plato’s critique of mimesis.

The first century BC Epicurean philosopher Philodemus wrote a treatise also called On Poetry in which he seems to offer quotations or perhaps summaries of some passages of On Poets. What Aristotle seemed to have focused on is the nature of poetry, and the importance of mimesis (Janko 2011, F4: “mimêsis has been posited as essential to the art of poetry”). And he seems to have clearly stated that mimesis, at least in the case of poetry, involves people in action (Janko 2011, F6a: Poets “depict the actions of people acting…”), and that this action must be complete (Janko 2011, F45: “The poet is a representer of a complete action”), all of which will be elaborated upon in the Poetics.

The second theme that seems to have been central to this dialogue is that of the shortcomings of poets. It is on this theme that Aristotle refers to his dialogue in the Poetics, where he analyzed, Aristotle puts it, the “many mistakes” that poets can make

with regard to what affects the reactions of the public which are necessarily connected to the art of poetic composition. (15, 1454b15–18)

This sentence has often seemed cryptic. But we find an example of such a mistake in one of our fragments. It is a passage from the later Roman author Macrobius, who quotes an excerpt in Greek assuming that he is quoting Aristotle’s words (ipsa Aristotelis verba), and where we see Aristotle denouncing an error of Euripides, who in one of his plays states that Aetolian warriors fought without sandals on their left foot. It is an error because those warriors habitually fought without sandals on their right foot. As benign as it may seem from a historical point of view, it is a deep flaw if one considers the impact on spectators. Imagine that spectators realize the error in a theater setting: the scene would no longer be tragic, and the error would become comical and provoke laughter.

This topic is not at all marginal. On the contrary, pointing out the mistakes that poets, even the best ones, commit should help us readers tell the difference between good and bad poetry (or at least good and bad passages or scenes in a poem or play), that is, to make us “critical” readers of poetry, or watchers of theater plays. It is the first century AD rhetorician Dio Chrysostom who reports a tradition according to which Aristotle was “at the origin of literary criticism (kritikê)” (Janko 2011, T4). By that, he surely meant primarily the work of scholars such as Aristarchus of Samothrace, which consisted in commenting on Homer’s epics, or of those who commented on Aristophanes; but such kinds of works made sense only if one supposes that discriminating or judging (which is the core meaning of the verb krinein) the qualities and defects of such and such a literary work is what is at stake. And that activity, as we can see from On Poets, was not a domain reserved to the academic happy few. Rather, every educated person could be expected to engage in such critical assessment. In his Protagoras, Plato makes the famous sophist state:

In my opinion, the most important part of a man’s education consists in being proficient in poetry, that is, in being able to understand, in the productions of poets, those that are correctly made and those that are not, in knowing how to distinguish between them, and in knowing how to give an account of these judgments, if asked. (338e–339a)

Plato certainly mocks such a claim, just as, in the Ion, he mocks the rhapsode who believes he possesses a “science of poetry” or a “technique of poetic composition” (in 532c, Plato equivalently uses the words technê and epistêmê poiêtikê, or simply poiêtikê). In writing a “Treatise on poetic composition” (Peri poiêtikês), we can hypothesize that, unlike Plato, Aristotle followed the path indicated by Protagoras. And in publishing a more accessible work such as On Poets, Aristotle seems to take it for granted that every person should be offered the opportunity to become a good judge of poetry so that she can better appreciate the value of the poetry she reads or the plays she regularly goes to see in the theater. [3] (Note that in Aristotle’s time, many tragic and comic authors were still writing numerous plays, and that several of the plays of Aeschylus, Sophocles, or Euripides were available in book form, and were also regularly staged in Athens and elsewhere in the Greek world).

2. Homeric Problems: How to Defend Poetry?
Literary criticism is at the very core of the other major published work dealing with poetry, the so-called Homeric Problems. One cannot overestimate the role and importance of Homer in ancient Greek culture: he is commonly called “the poet”, and every educated Greek person knows many passages by heart. And yet, Homer had also been harshly criticized from very early on, notably by Xenophanes and Heraclitus, who reproached Homer for giving wrong images of the gods. This is in part because both read Homer literally, as if Homer were describing the real world of the gods. Plato famously follows suit: he too addresses the way people usually read Homeric epics as a sort of moral description and prescription of right and wrong behaviors; and since gods and heroes are to be taken as our moral paradigms, we must condemn (and withdraw or perhaps rewrite) the numerous passages where those gods and heroes commit wrongdoings. In turn, those critiques provoked equally strong reactions aimed at defending the poet. Some simply defended the Iliad and the Odyssey as offering right ethical models: after all, according to Homer, Achilles is the paradigmatic example of courage, and Odysseus of practical intelligence and resilience. This seems to be why the Socratic philosopher Antisthenes took Odysseus as one of his philosophical heroes, as a man whose endurance may be seen as paradigmatic of the endurance required for a virtuous life. A rather different way of defending Homer was to read his poems in an allegorical manner, as, notably, did the philosopher Metrodorus of Lampsacus (a contemporary of Socrates), who interpreted many Homeric passages in the light of the cosmology of Anaxagoras.

Aristotle’s Homeric Problems offers yet another manner of defense. As one can see from the fragments that we have, Aristotle never tries to exonerate Homer’s apparent shortcomings by arguing from an ethical or allegorical perspective. What he instead proposes is to respond to criticisms made against Homer from an art-centered perspective. He sought to determine, that is, how such and such a passage or verse should be judged by reference to the aim or function of the art of poetry, considered as art and in no other way. As he clearly states in the Poetics, which seems to recap how he presented his material in the Homeric Problems:

It is not the same criterion of correctness that applies in poetry and in politics, nor in poetry and in any other art. (25, 1460b13–15)

Judging poetry from the perspective of another domain, such as politics, biology or psychology, would be a methodological mistake, since their respective aim or function is not the same. In response to ethical condemnations faulting Homer for depicting the gods in a morally bad fashion, Aristotle calmly answers:

It is quite possible that the poets speak about them neither by idealizing them nor in a true way, and that things are as indeed as Xenophanes states: but in any case, it is how people speak of them. (1460b36–61a1)

Aristotle seems to agree with Xenophanes that it is wrong to believe that the gods look like human beings and share our bad behaviors; but the poets must take into account how people generally imagine them to be if they want to have them intervening in their plots. For representing the gods as morally good beings would actually make Homer’s epics rather odd to the people they were addressed to, which would have disturbed their involvement in the plot, and spoilt their pleasure. Another example comes from the scene in which Achilles pursues Hector: the way Achilles prevents the army from taking up arms against him and lets him go by a simple nod of the head is, from a psychological point of view, totally implausible (1460a14–16; Iliad 22, 205–206). But this is not an error we have to blame Homer for, Aristotle replies, since this adds to the effect of wonder, which is part of our pleasure: even if it is a mistake from a psychological perspective,

it is right if it achieves poetry’s aim […], if that way an even more striking effect is produced in that part of the poem or at a later stage. (1460b24–26)

In Homeric Problems, Aristotle presumably only dealt with Homer. But in Poetics 25, he wants to extend his approach to tragedy. One example concerns Menelaus in Euripides’ Orestes. There Aristotle agrees (presumably with other critics) that Euripides made a mistake in representing him as a coward (15, 1454a28–29; 25, 1461b19–21)—but not for ethical reasons. The offense is purely “poetical” or “artistic”: since pity and fear require that we admire the heroes on stage, having a base character must jeopardize the audience’s emotional reaction; if, on the other hand, the plot requires having such a hero (which is not the case in that play), that would not be a fault. One will also notice that Aristotle takes for granted that we can extend such views to the other domains of art. Another example is that of the painting of a horse “with two right legs stretched out towards the front” (1460b18–18). Aristotle believes (wrongly, as it happens) that this is physically impossible. Still, he avers that we must not judge the quality of the painting from a biological point of view. On the contrary, if the horse’s galloping in such a way effects a stronger emotional reaction in the viewer, that is the right way to represent it!

3. The Poetics: How to Understand Poetry ?
In these two published works, Aristotle’s primary goal was to offer instruction for becoming a sophisticated reader or spectator of poetic works. There is no reason why this general aim might have been different in the Poetics. This text has often been held, since its rediscovery in the Renaissance, as a manual for a would-be poet. And indeed, Aristotle’s tone is often very prescriptive: this “Treatise on the art of composing poetry” seems to lay down the rules that one must follow if one wants to write a successful play. But Aristotle also says, emphatically, that “the art of poetic composition belongs to a naturally gifted man” (17, 1455a32–33), and that making good metaphors, which is the prerogative of a good poet, is “something that cannot be learnt from someone else, but is the sign of natural talent” (22, 1459a4–6). It is thus unlikely that Aristotle had the ambition of training poets. Surely, what Aristotle proposes is to reconstruct what he takes the best set of composition rules, which in his view had helped, or would help, poets write good tragedies. But the exposition of these rules (which the poets may or may not be aware of: 8, 1451a23–24) is meant to show what good poetry should be like, such that his readers could appreciate the quality of a piece. And indeed, this is what the conclusion of the Poetics states:

This is all there is to say about tragedy versus epic … about the reasons why some are good and others not so good…. (26, 1462b16–18)

What the readers of the Poetics are offered are the reasons or causes why such and such feature of a play is to be considered good or poor. Armed with such knowledge, they should be better able to judge the quality of the tragedies they read or see in the theater.

As Aristotle posits, notably in the first book of the Metaphysics, searching for causes defines philosophical inquiry; knowing the cause of x allows you to understand what x consists in. And, crucially in Aristotle’s eyes, the final cause is what matters the most: when one knows the final cause of x, one can truly understand not only what x consists in, but should consist in if it is to be the x it is supposed to be. More precisely, the end of x amounts to the ergon, literally, the “work” or the “activity”, or what we more usually call the “function” that x performs; thus, for an eye, its aim or end amounts to its function, or “functioning” (or “working”, energeia) which is, of course, its seeing. Similarly, in the case of a hand: its function is grasping things, and when we talk of the hand of a dead body, we use the name “hand” only homonymously —a dead, non-functioning hand is no longer what it is to be a hand, or a “real” hand. And when an eye is seeing well, this is what Aristotle calls its entelecheia, that is when it performs its telos in a perfect (entelês) way. It is true that in the Poetics, we don’t find the typically Aristotelian technical words energeia and entelecheia. But presumably one may take the phrase “the best tragedy” (kallistê tragôgia) which is emphatically used in Poetics 13 as the equivalent, in common parlance, to a tragedy in entelecheia, that is a tragedy that performs its function, or its telos, in the best possible way. This phrase does not refer to any particular outstanding tragedy (say, Sophocles’ Oedipus-Rex, which Aristotle seems to like very much), or to an ideal tragedy that a poet should try to emulate, but rather to any tragedy that would or does indeed fulfill its function properly.

This very rough characterization of what the aim and the method of the Poetics consist in is in fact announced from its very first sentence:

This treatise is about how to compose poetry: what is poetry as such? What are the poetic genres? What power (dunamis) does each of them have? How should plots be constructed so as to end up with a successful work of poetry? How many components should there be, and what should these components be like? (1, 1447a8–11)

This gives us a clear plan of the Poetics, which divides into two parts: a first part is devoted to poetry “as such” (ch. 1–5), which can be considered as a kind of general introduction to what poetry is; and a second part, actually the bulk of it, is dedicated to its genres, that is, mainly tragedy and comedy, where the plot is treated as the main “component”. But perhaps most crucially, it also introduces the question of the “power” of each of the poetic genres. The word used is dunamis, which is here to be taken in the sense of “the ability to put something into movement”, that is, in our case, the power it exercises on the poetry’s recipient, or the “effect” it has on its consumer. It is, one may say, the “subjective” counterpart to the more “objective” side of the same thing, that is the function or work(ing) (the ergon or energeia) itself. So, the function of each genre of poetry, or its power or effect, is really what is of central importance in this inquiry. And (for reasons we are going to see), plot is seen by Aristotle as the best tool for implementing that effect. Now, it is not to be denied that a successful poet is the one who concentrates on plot when writing his plays. But this is not Aristotle’s main point of focus. His dominant agenda is to warn his readers right at the beginning of his treatise that they must focus on how the plot is constructed if they want to judge the extent to which such and such work of poetry succeeds, i.e., how well it performs its function.

Now, what concretely is the aim or the function of poetry? Homer has already told us very explicitly, notably in the famous episode of the Sirens (Odyssey 12. 39–54; 154–200), that the aim of his poetry is pleasure. Listening to the Sirens who are singing poetry, presumably Homer’s own Iliad (as they sing everything that took place under the walls of Troy), is something Odysseus, and actually every man, strongly desires, so strongly, as this episode amply illustrates, that he might even forget the very goal of his journey, namely the goal of returning home and being reunited with his family. Aristotle does not hesitate in mentioning that episode when, in the passage from the Eudemian Ethics already quoted, he describes what gazing at beautiful statues or listening to beautiful songs, or poetry, should amount to. In the Poetics, this is what he takes for granted: he assumes, as Homer does, that providing pleasure is the aim a poet must seek. A piece of poetry is successful when it provides pleasure to its consumers. But what precisely is this pleasure that poetry is meant to provide? How shall poetry accomplish that function? These are the questions that underlie the Poetics.

3.1 What is poetry as such?
Two major themes run through the first part of the Poetics on “poetry as such”: the naturalness of poetry and the division of poetry into serious and comic poetry.

The first sentence of Poetics 4 is famous for stating that two causes presided over the birth of poetry, and that those two causes are natural: the instinct that all men have for mimesis and the pleasure they take in the objects of mimesis (1448b4–5). A few lines further down, Aristotle adds to the instinct for mimesis, the instinct for melody and rhythm (1448b20–21). What he means exactly by these two causes is disputed. It might seem more natural to opt for mimetic instinct and pleasure, taking the musical instinct as part of the mimetic instinct (music also being a mimesis for Aristotle). But it should be noted on the one hand that this instinct for rhythm explains the versification (which in Greek is based on the rhythmic alternation of long and short syllables), which can hardly be qualified as mimesis. And on the other hand, that the pleasure is not only the one we take in the works resulting from the mimesis, but also in the mimesis itself. It seems that Aristotle apparently wants to speak not only about the two causes which preside over the poetic creation, but also about the causes which make for our attraction to poetic works, the two perspectives being intimately linked. Both poetic creation and our attraction to poetic works have as their causes our mimetic instinct and our musical instinct, as well as the pleasure that accompanies the expression of these two instincts. Here again, Aristotle has a totally different vision from Plato’s. Poets are not divinely inspired people: they are people who are naturally more gifted than most other humans (1448b22). And unlike Socrates who in the Republic seems to advocate a “natural city” which would not contain the mimetic arts, Aristotle takes them as part of human nature which a “natural” city must take into account. (One will remember the vivid response of Glaucon against Socrates’ proposal: “But this is a city for pigs!” [Rep. II 372d], meaning that such a city would be deprived of everything that makes for a properly human city. Aristotle would have applauded)

It is from this insistence on the naturalness of poetry and the centrality of pleasure that one should understand the division that Aristotle operates within poetry, between “laudative” and “serious” poetry, and “denigrating” and “funny” poetry, whose points of arrival are the genres of comedy and tragedy. At first sight, when Aristotle introduces the history of this division, he seems to rely on an ethical or social distinction:

Poetry branched into two, according to each poet’s character: the more serious-minded poets represented admirable actions, that is to say the actions carried out by that kind of person, whereas the more trivial poets depicted the actions of base people. (Poetics 4, 1448b24–26)

Presumably, Aristotle takes it that the first poems to date must have been what we call “lyric poetry”, with on the one hand the poetry of praise (such as those that Pindar would later write) and on the other hand satirical poems (predating those of Archilochus), and which are the latest ancestors of the tragic and comic plays. However, Aristotle himself qualifies such a presentation, adding that between those first lyrical poems and these two much later genres, stands Homer, who is the author of poems depicting admirable actions in his Iliad and Odyssey, and ridiculous ones in the Margites (a comic epic poem Aristotle takes to be Homeric). Insofar as Homer cannot be both below and above average, we must conclude that this distinction refers rather to what we would call “fictional” possibilities. A good poet is the one who, like a good actor, knows how to put himself in the shoes of his characters, whoever they may be (17, 1455a32–33), and Homer was particularly good at that (24, 1460a5–11). Moreover, there is no evidence that Aristotle held comedy as inferior to tragedy, nor is there any reason to believe that he held the spectators or readers of satires, the Margites or theater comedies to be “inferior” people. To say that “superior” or “inferior” people invented serious and comic poetry is in fact to say that they were particularly good at impersonating those types of characters. And the reason why the characters themselves must be represented either as “superior” or “inferior” is due to the aims of these poetic genres. In the case of tragedy, only “superior” characters can elicit our fear and pity: the more we admire someone, the more we pity him or her when they don’t merit their fate. Conversely, in comedy characters must be represented as “inferior” to elicit our mockery and laughter.

Aristotle supposes a temporal and essential filiation between these poetic genres: lyric poetry, both laudative and satiric (which, Aristotle oddly supposes, must have begun before Homer); serious and comic epic; and the genres of tragedy and comedy. The key moment is Homer. It is in Homer that “tragedy and comedy loomed out” (4, 1449a2–3), that is to say that poets saw in Homer’s poems the promises of comedy and tragedy; it is from there that they gave birth to these new genres and developed them (Aeschylus and Sophocles are named for being such developers). But above all, Aristotle adds that once these genres were discovered from Homer, all the poets abandoned lyric and epic poetry, and wrote in these two genres, “because these new genres had more prestige and value than the older ones” (1449a5–6). That is to say, the poets realized that by writing comedies or tragedies instead of epics, they could gain more prestige, and it is because of their value that people immediately became attached to these new genres. But why? Because these genres were entirely dramatic, and involved a fully enactive mimesis and used music (while epics were recited without music, at least in Aristotle’s time), they fully implement our natural instincts for both mimesis and music, and so they were more pleasant. If pleasure is the aim of poetry, it is only reasonable to focus on the most pleasurable genres, tragedy and comedy. (It is true that Aristotle goes back to epic after his treatment of tragedy, in Poetics 23–26, but it is essentially to help better understand tragedy, and to oppose other critics who took epics to be the most paradigmatic genre of poetry).

It is now time to ask ourselves what this pleasure of poetry might exactly amount to. In Poetics 4, Aristotle notoriously introduces the paradoxical example of an abject animal: while seeing such an animal provokes disgust and pain in the real world, seeing a picture of it produces pleasure. He explains why:

Seeing a likeness is pleasurable, because in contemplating it, people come to understand (manthanein) through inference, what each of its details are: for example, that the man there is so-and-so. (1448b15–17)

This sentence has often been taken as a commitment to what we might roughly label a cognitivist approach to aesthetic pleasure. The pleasure a work of art, be it a picture or a tragedy, affords would come from the understanding, or actually the “learning” (the verb manthanein can mean both), it allows; and the example of the man depicted in the picture would point to the idea that gazing at it, we could learn something new and fresh about that man.[4] (Some interpreters have even gone so far as to read the phrase “the man is so-and-so” as referring to his essence or a certain essential quality that the picture would allow us to grasp [Gallop 1990]). And since this is the pleasure that a mimesis generally speaking seems to afford, it is tempting to conclude that the pleasure that a tragedy or a comedy provides must be one species of such cognitive pleasure. But as others have replied (notably Lear 1988, and Ferrari 1999), such an interpretation is based on an over-reading of the text, especially if one fully takes into account what Aristotle immediately adds:

In case you never saw the man before, you will not derive any pleasure from his likeness qua representation. But you will get pleasure from the brilliant execution or the colours, or for some other such reason. (1448b17–19)

In other words, the “understanding” here barely amounts to the recognition of the man you already knew as so and so, like Socrates in the portrait of him in front of you.

To be sure, there may be cases where a more complex cognitive process takes place, such as in the case of a god which you recognize as being such and such, because you infer from, e.g., the statue having a thunderbolt in his hand, that it is intended to be Zeus. But in all such cases, there is no new learning involved. This reading of the text, however natural, may seem rather unappealing. But one should remember that here Aristotle is only giving a description of the general cause of our being attracted to mimesis: and indeed, were you not able to recognize what the object of the mimesis is, be it the voice of such and such a person, or the identity of the person depicted in this portrait, you wouldn’t be able to enjoy it qua mimesis. In tragedy and comedy, we do enjoy such pleasures as well; it is certainly pleasant to recognize the characters and their features when they intervene in the play, and that recognition is indeed a sine qua non of your following a plot, and enjoying the whole play; and the same goes, more basically, in cases where someone, in the real world or on stage, mimics the voice or the accent of a certain person: this affords you with the pleasure of recognizing who is meant. But all these pleasures are common to all sorts of mimesis. This is perhaps the most important point. As we have seen from the very first sentence of the Poetics, what Aristotle is interested in is inquiring into each of the poetic genres, and the power each has. And indeed, what Aristotle repeatedly emphasizes throughout the Poetics is that the poet must seek to produce the pleasure that is proper to such or such poetic genre: “one should not seek any kind of pleasure from tragedy but only the appropriate kind” (14, 1453b10–11). As for tragedy, Aristotle leaves no doubt: “What the tragic poet must produce is the pleasure derived from pity and fear through mimesis” (1453b11–13). Since book 2 is lost, we no longer have the formulation that Aristotle must have made in the case of comedy, but it is fairly obvious that he would have mentioned the pleasure associated with the amusement provoked by the comic plot’s jokes and gags. We do have something of an echo of such a statement in Poetics 13, which mentions the “pleasure proper to comedy”, and gives the example of a comedy where Orestes would make friends with Aegisthes instead of killing him: this parodic treatment of the tragic end of Sophocles’ or Euripides’ corresponding play, Electra, is what creates our amusement, that is our comic pleasure (1453a35–39).

Thus, the pleasure that is the aim of tragedy is an emotional pleasure, and the same goes for comedy. In the latter case, amusement (of which laughter is the physical expression) is certainly not an emotion strictly speaking, but is rather the experience of a state of mind, which, like an emotion, is an affect or what early modern philosophers will call a “passion”. (Here we may note that in the quoted passage from the Eudemian Ethics, Aristotle does not shy away from using the word paschein, i.e., undergoing a pathos, to describe the experience of gazing at a beautiful thing or listening to a fine tune.) As Plato had said, when we watch tragic heroes suffering on stage,

we take pleasure and, surrendering ourselves, we follow and share the hero’s sufferings and earnestly praise as a good poet whoever most affects us in this way; (Rep. X 605d)

and the same goes for comedy, which offers the opportunity of “taking great pleasure” (606c) in letting us “give in violent laughter”, and even “be overcome by laughter” (III 388e-89a). To be sure, for Plato, this is a surrender to the irrational part of our soul, which may have some deleterious consequences. Aristotle does not seem to be worried by that, at least not in the case of adults. In one passage of the Politics, he warns that letting children go to the theater and watch comedies where people insult one another or make obscene jokes can be damaging, and he does not hesitate to prohibit them from going to the theater. But once “their education has rendered them immune to the harm such things can do” (VII 17.1336b22–23), there is no reason why adults should be prohibited, or even discouraged, from enjoying comic theater. In fact, a similar point can be made in the case of tragic poetry as well: if in the theater (or in an epic recital), we do take pleasure when, as Plato vividly depicts it,

we hear Homer or any other tragic poet representing one of the heroes in a state of grief and making a long speech of lamentation or even chanting and beating his breast, (Rep. X 605c-d)

it would be a shameful thing to do so in the real world (where men at least were not allowed to express their grief aloud). Aristotle seems to take it as evident that the world of the theater is a fictive world that obeys other rules than the real world, and provided we have received a good ethical education, certain “politically incorrect” features, such as laughing at incongruous insults or “sharing the hero’s sufferings” and his “state of grief” in a boisterous manner should not cause any harm. Quite to the contrary, Aristotle seems to take tragic emotions and amusement as typical human propensities, which give us lots of pleasures when they are experienced in the theater.

3.2. Tragedy
The best tragedy, as we have seen, is the one that can best produce the power typical of a tragedy, i.e., producing pity and fear, and therefore can best achieve its function or aim, which is the pleasure that comes from the experience of these emotions. What Aristotle proposes in his analysis of tragedy is highlighting the means by which the poet can implement this function, which in turn should provide theater goers with the right understanding they need to have in order to be able to judge the quality of such and such a play they may attend, or read. As Aristotle announced right at the beginning, the plot is to be considered the most important means and indeed it is given the most expansive treatment, while the other “elements” of the tragedy, such as the depiction of the characters and their expression (or what we more commonly call the style, in which their dialogues are written), come next. This focus imposes itself from the perspective of the aim or function of poetry. A tragedy typically depicts the change of fortune which includes reversal of circumstances and sufferings, and rightly so: this is of course how fear and pity for the main heroes can be produced. Thus, the actions of the heroes that lead to such an outcome must be what a tragedy should mainly depict, or represent, as well as other features that are part and parcel of that outcome. The way plots are constructed or assembled from the various deeds and words of the characters (or what Aristotle calls the “events”, pragmata) should be what constitutes “the aim” of the poet (Poetics 6, 1450a22–23). This is the aim a poet must seek if he wants to end up with a tragedy that can fulfill its aim or function properly.

If a poet were to string together tirades describing characters, however perfectly composed in terms of expression and reasoning they might be, he will not be able to achieve what we have said is the function of tragedy: (1450a29–31)

presumably he might well provide a certain pleasure to his public (if only for the beauty of his style), but he would not obtain the proper aim or function of tragedy.

There are two series of requirements for a plot to be a good plot. One series involves the qualities of the plots, which includes totality, unity and generality (or: universality); the second one involves the turning points of the plot, which include the reversal of circumstances, the recognition, and the sufferings. But both series are based on what a proper consumption of tragedy requires: the first series is about our being involved in a plot; the second series, about what creates the emotions of pity and fear. Producing the pleasure that comes from the emotions of pity and fear is the aim of tragedy, but in order to achieve this, the poet must create a plot the hearers can immerse themselves in; as we would say, a theater audience must “believe” in the story that is unfolding and in the characters who make up the story. Otherwise, they would just lose attention and interest, which would prevent any strong emotional involvement. One such requirement is that of “the fitting size”. In the case of gazing at a beautiful animal, whether in reality or in a picture, our admirative enjoyment cannot hold if the animal or the picture is so big that it cannot be grasped in one glimpse, or if it is too minute to be seen at our ease. The case of a tragic plot is similar: a proper size is needed to make the whole play “easy to remember” (6, 1450b34–51a6). One must be able to remember the important features and events the full time we watch it, which is a sine qua non condition for our being unflaggingly attentive and attracted to the plot.

Another such sine qua non requirement is what one may call the “law of likelihood”, or “plausibility”. Aristotle strongly insists on this: all events must offer the appearance of causality, and follow not one after the other, but from each other (10, 1452a20–21). Faced with an event that is totally unexpected in being disconnected from any normal causal sequence would just be incredible, and, in worst cases, hearers would laugh instead of feeling pity or fear. Of course, unexpectedness can be an efficient tool for evoking strong reactions. For example,

when the statue of Mitys in Argos killed the person responsible for the death of Mitys himself by falling on him as he was staring at it, (9, 1452a7–9)

it must have created a big surprise, and putting such an event into a plot might be very powerful. But even if the event happened by pure coincidence, the poet must suppose that his audience will, even if subconsciously, admit a certain divine vengeful intention behind the event, so it appears to them as having a certain plausible cause. The poet has full license for inventing whatever events he wants, but they must appear “plausible” in one way or another. Or as Aristotle summarizes,

the function of the poet is not to speak of what has happened but of things such as they might happen, that is to say, to tell us of possible outcomes in all likelihood or out of necessity. (1451a36–38)

The weight is not so much on the fictionality of events, since the poet can also draw his material from events that actually occurred, but in the way he presents them: whether they actually happened or not, and whether they are physically possible, he must describe them in the way we would think they might happen. It is to be noted that “in all likelihood or out of necessity” is a rephrasing of the well-known “for the most part or out of necessity” motto in the Physics, where “for the most part” describes what actually happens to physical things which obey causal necessity, barring a few exceptions. In the case of poetry, or any other mimetic genre, the only causality we need is “subjective” causality, or plausibility. Aristotle even goes so far as to say: “What is impossible but plausible must be preferred to what is possible but not credible” (24, 1460a27): an event that has taken place, and so is physically possible, but which would not be presented in a plausible way would not succeed in gripping hearers.

It is in this context that Aristotle makes a comparison with history: while history must report hard facts as they actually happened, poetry must tell them as they might happen according to the law of plausibility. It is in this context where we find one of the most famous, and famously contentious, sentences of the Poetics:

This is the very reason why writing poetry is more philosophical and more worthy than writing history. For poetry tends towards a general picture (katholou), whereas history tells us of particular case studies. (9, 1451b5–7)

It is very tempting to interpret this as if Aristotle were proposing that poetry dealt with “universals” in the usual sense of the term, and thus conveyed “universal truths”. Many philosopher readers have assigned Aristotle a grandiose view of poetry, while historians have blamed him for undermining the importance of history. Both are probably overreactions. Aristotle does not say that history is not philosophical, but that poetry is “more philosophical” than history. Since philosophy is the search for causal understanding, we may suppose that “philosophical” means, in effect: “that which sees things under the perspective of causality”. Of course, history, as Herodotus claims in the very first sentence of his Histories, is also a search for the causes of the events he reports; but his main focus must be to report and explain those events as they actually happened, for example “what Alcibiades did or what happened to him” (1451b11). What the poet does is to take whatever event he may want to introduce into his plot and turn it into a plausible one—even the most factually impossible events!

For nothing prevents what really happened to be turned into things such as they might happen in all likelihood or be possible—it is this which makes him the poet of these. (1451b30–32)

This makes poetry, in the active sense of the term “making poetry” (the word poiêsis allows for both meanings), a “more philosophical” craft. And this involves “universals” in a non-technical sense, as Aristotle explains:

A general picture is this: the kind of thing a certain type of person would say or do in all likelihood or out of necessity. (1451b8–9)

“General”, or “universal”, does not describe here a specific item, but the way how one may expect someone will say or do things given the person she is. And that goes not only for tragedy, but also for comedy where (contrary to most tragedies which draw their plot from well known myths) particular names of the characters are up to the poet’s choice and added after the plot is written (1451b11–14): thus, here too, the crucial thing is to create a plausible plot where all the deeds and words done or uttered by the play’s characters seem to be causally related; this is what makes them plausible to an audience.

The second type of requirement involves the content of the plot. Generally speaking, a plot is constituted by all the events making up, as we say, the “dramatic action”. But what counts primarily among such events are three features. First, the reversal of circumstances which “is a volte-face change in the sequence of events” (11, 1452a22–23). This must happen “in all likelihood or out of necessity” as Aristotle insists, otherwise they would not achieve their aim, which is to provoke strong emotional reactions in the public. Emotional reaction is also created by recognitions: after, or just before, something important happens to a character, he or she is recognized as having such and such relation towards his or her protagonist (1452a36–b3). And, last but not least: audience members respond to sufferings (pathê), or rather what Aristotle determines as “an action conducive either to death or great pain” (1452b11–12), thus an act of violence that causes great suffering. This action is best conducted when it involves kin: it is much more powerful, emotionally speaking, to watch a scene in which, e.g., a mother is going to kill her son, or a daughter her father (14, 1453b19–22). In such tragic scenes, involving unexpected yet plausible changes of fortune, startling recognitions, and acts of violence, managing them in the right way is absolutely key if one wants to elicit the tragic emotions. These are pity and fear, which are the two moments, or aspects, of the same emotional experience, where fear concerns primarily the moment when a character is about to commit the irreparable towards his kin, while pity comes about at the sufferings and deep misfortunes of the involved parties.

If plot is at tragedy’s core, that is not to say that the play’s other features are of no importance. That is especially the case for how the characters are depicted, and how they speak. Both are a significant factor in rendering a plot successful. In a word, characters must be credible: when we see a character doing or saying such and such, we must believe in them. Characters must be normally virtuous, for otherwise we couldn’t admire them, and pity requires that we think that their misfortune is unmerited. Conversely, a woman shouldn’t be presented as talking like a philosopher (such as Menalippe in one of Euripides’ play), because no ancient Greek spectator would ever expect to see a woman speaking like a male philosopher (15, 1454a16–32). Building characters that correspond to an audience’s expectations is key if one wants to immerse them in a tragic play. As to the “expression”, or the style, characters must speak in a relatively clear and common way, so the spectators can easily follow the plot. But figures of style such as rare words or metaphors must be added at the right times, especially when what a character says may add to the scene’s emotional impact. Aristotle compares two ways of saying the same thing:

In his Philoctetes, Aeschylus had written: “The ulcer which eats the sole of my foot”, whereas Euripides replaced “eats” by “feasts on” (thoinaô). (22, 1458b19–24)

The rare, poetic verb that Euripides used produces an unusual impression, and for that very fact strikes the spectators, which is of course a means of still further increasing the pity they feel for this character whose foot is affected by a painful gangrene.

By reconstructing how tragedies can best implement their function, Aristotle intended to help his readers to become better judges and appreciators of the tragedies they could read or attend in the theater. But for any modern reader, there remain two perplexing questions. Whereas pity and fear are normally painful emotions, how are we to conceive of them as pleasurable? And what about the most enigmatic word of the Poetics, katharsis, which also seems to be presented as the aim of tragedy?

Since the Renaissance, the theme of katharsis, which appears in the definition of tragedy (tragedy is “a mimesis of a momentous action”, which “by stirring up pity and fear, brings about a katharsis of such emotions”—6,1449b25–28), has been the subject of endless debates, as Aristotle himself never explains himself about it. The interpretive conundrum is basically this. Normally, the word itself refers to the action of rendering something “pure” (katharos), with all the possible senses or connotations of “pure”: purification in a religious context (such as Orestes who undergoes a katharsis when digging in the sea to be “purified” of the murder of his mother, which Aristotle refers to at Poetics 17, 1455b15), purgation in a medical context, or cleaning in the case of an object that is dirty. It might mean either in principle, but we might expect the word to get its meaning from its context of use. But what is the context supposed to be in the case of tragedy? Aristotle does not tell us. In Politics VIII, he briefly mentions a kind of music, the so-called “enthusiastic music”, which provides a medical cure that consists in a katharsis for those who are especially prone to “enthusiasm” (or what we would call “frenzy”, or “agitation”). And then he adds:

The same thing, then, must be experienced by those who are especially prone to pity and fear and in general by those who are suffering from their emotions (pathetikoi) on the one hand, and by any other person to the extant as she shares in those emotions on the other hand: they all undergo a certain katharsis and get a pleasant feeling of relief. (Pol. VIII 7, 1342a11–15)

At least for the pathetikoi, the context seems to be medical. But is it the case for “each other person to the extent that she shares in those emotions”, i.e., notably the spectators of tragedy? When Aristotle refers to “a certain katharsis” (tina katharsin), he may mean either that all these people, the pathetikoi as well as anyone else, undergo the same sort of medical katharsis, or that they each undergo a different sort of katharsis.

The best known scholar forcefully endorsing the first interpretation remains Jakob Bernays (whose nephew by marriage, Freud, adapted the views to psychoanalysis): Aristotle, a doctor’s son, would have defended tragedy against Plato’s rejection by conceiving of it as sort of beneficial cure for all theater spectators (Bernays 1858). That interpretation (which has known several refinements, especially in German scholarship; collected in Luserke 1991) has been fiercely criticised for reducing tragedy to being a medical cure. Whereas Bernays was actually reacting against the strong ethical views of the German writer and philosopher Lessing, many contemporary scholars have proposed coming back to such views. According to one reading, defended by Richard Janko, the tragic katharsis should be conceived of as the purgation of the excessive emotions so as to obtain the right measure of pity and fear, which nicely matches Aristotle’s conception of virtue (see especially Janko 1992). But Aristotle seems adamant in the text of Politics VIII that music for katharsis is to be differentiated from music for moral education (7, 1341a21–24; 1341b38). And in the Poetics, Aristotle takes it as evident that emotions must be strongly experienced; aiming at the right, moderated measure of pity would actually spoil the typical pleasure that a good tragedy provides! Others, notably Martha Nussbaum, have suggested that in the case of people who go to the theater to enjoy the emotions of fear and pity, katharsis might mean “clarification” (i.e., the removal of obscurities; in his logical works, Aristotle does use the adverb katharôs in the sense of “clearly”); the tragic theater would then aim at providing spectators with a clarification of the pitiful, indeed tragic, human condition.[5] As philosophically engaging as that reading may be, there is no textual proof that Aristotle would have adopted such a grandiose view; and contrary to Nussbaum’s own views on ethics, where pity is considered a central ethical emotion, pity only plays a very marginal role in Aristotle’s ethics. Yet another kind of reading, relying on the idea that pity presupposes that the object must primarily be the spectator him- or herself, has insisted on the relief that such an emotional releasing might offer, consisting in a “consolation”: as Jonathan Lear states,

In tragedy, we are able to put ourselves imaginatively in a position in which there is nothing further to fear. There is consolation in realizing that one has experienced the worst, there is nothing further to fear, and yet the world remains a rational, meaningful place in which a person can conduct himself with dignity. (1988: 326; A reading of a similar kind has been offered by Munteanu 2012: 131–136)

Again, this is a fascinating approach of the ultimate meaning of tragedy, but it does not seem that Aristotle himself ever expressed such an idea of “consolation”.

It seems, therefore, that a much more minimalist reading might better suit Aristotle’s texts. One such reading, advocated by John Ferrari, is to take the word katharsis as a way to describe the process of relieving the tension created by the stimulation of the tragic emotions (Ferrari 1999, 2019). The great advantage of this interpretation is that it does not presuppose anything beyond the context of the Poetics, and it does not seem to contradict any other explicit statement of Aristotle’s; but one may wonder if it really explains the phrase, the “katharsis of such emotions”, where the idea of “tension”, or “suspense”, does not seem to be present. Another such reading might look like the following. In his biological works, Aristotle regularly uses the word katharsis to refer to the menstrual blood or bleeding as well as to the male ejaculation: in those cases, katharsis means the flowing itself (or by metonymy, the blood); and the fluid is nothing deleterious (interestingly enough, against Hippocratic views, Aristotle does not take menstrual blood to be “impure”). He holds rather only that keeping such fluid inside the body without discharge can be unhealthy, and so a discharge is needed from time to time. As Aristotle knew very well, ancient theater audiences showed their emotional reactions in a very physical way by screaming and weeping loudly. So, the katharsis of the emotions of pity and fear could be meant to name that sort of physical expression or outlet of such emotions. This last reading might perhaps not appear worthy of Aristotle’s philosophical genius. But it would fit the aim Aristotle has himself proposed: to explain how a tragedy accomplishes its function, which is to produce the emotions of pity and fear, and the pleasure accompanying them. Saying that a tragedy is a mimesis which “by stirring up pity and fear, brings about a katharsis of such emotions” might simply mean that tragedy should indeed aim to allowing spectators to express and unleash their emotions in the theater.[6]

The second perplexing question involves what modern aestheticians call the “paradoxical pleasure of negative emotions”. How is it that we can enjoy emotions such as pity and fear that are normally painful? It is quite often assumed that katharsis can do just that, transform the pain of those emotions into pleasure, through the feeling of relief that accompanies it. But it would be very counter-intuitive to reduce the pleasure any audience gets from a play to just that pleasure, which follows or is the consequence of a katharsis. For one would then have to suppose that a theater audience would suffer all through the play to finally undergo a katharsis, and get their pleasure at the play’s end, on leaving the theater! As Aristotle surely noticed himself, an audience that has felt great fear for heroes on the verge of grave suffering and has wept and cried during a two hour show, will normally leave the theater with a physical sense of relief. But when he says that the poet must “produce the pleasure from pity and fear through mimesis (dia mimêseôs)”, Aristotle probably wanted to allude to the fact that mimesis is what makes those emotions a source of pleasure. How are we understand this more precisely? Relying on Kendall Walton’s theory of “make-believe”, one might be tempted to take mimesis here as what would allow spectators to pretend to have those emotions. But nothing indicates that Aristotle would have shared such a conception; on the contrary, describing what even the simple reading of such a powerful plot as Sophocles’ Oedipus-Rex would produce, he speaks of shuddering or getting goosebumps (phrittein), which seems to indicate that we actually experience strong emotional reaction (14, 1453b3–6). (And the same would hold for pity which produces tears). Thus, insofar as we immerse ourselves in a tragic plot and “believe” in its characters, it is probably in some way a painful emotional experience. But we also know that a story experienced in this way takes place in an imaginative world, where there is no real object to be feared. Thus mimesis, one might say, allows us to experience fear for itself (and as Aristotle says in a passage from De Anima, imagining fearful objects is “up to us”, while seeing them in the real world is not: 427b14–21), which is pleasurable. This is not to say, though, that this would annihilate the painfulness of the experience. As Plato explicitly said (and there is no reason to think Aristotle would have disapproved), this is precisely what makes tragic pleasures so paradoxically attractive: they are a kind of mixture of pain and pleasure (Philebus 48a).[7]

3.3 Comedy
Many if not most scholars seem to consider tragedy to be the best possible kind of poetry in Aristotle’s eyes. But Aristotle never suggests that; on the contrary, he insists that tragedy and comedy, each constituting a fully enactive mimesis, are the most perfect genres of poetry, each one corresponding to our natural inclinations for tears and laughter. Whereas the tragic poet must produce strong emotions in order to enjoy a tragic play, the comic poet must produce amusement, which finds its physical expression in laughter. (Also, as a few lines from the neo-platonists Iamblichus and Proclus forcefully suggest, it might well be the case that Aristotle thought there was a katharsis in comedy as well, which would perfectly fit the last proposed interpretation: a comic katharsis would just amount to the expression or the outlet of laughter.)[8]

In chapter 5 of our Poetics, Aristotle presents comedy with these terse words:

Comedy, as we have already said, is the representation of men of lesser value, but not in a sense that would imply all defects: ridicule is only a part of what is shameful. What makes people laugh is indeed a form of error or physical ugliness which does not cause suffering or death, as is immediately seen from a comedy mask: it is something ugly and deformed, but which does not express any suffering. (1449a32–37)

Scholars have often drawn on the idea that aggressiveness or, as Bergson would say, “malice”, would be at the heart of comedy as Aristotle sees it. And indeed, this is already what Plato defends in a famous passage of the Philebus (47d-50b): it would be phthonos (an emotion that is often translated as “malice” or “ill will”, but that designates above all “envy”) that would be the hidden cause of our laughter (Imagine your neighbor, whose new Ferrari you envy, inadvertently crashing it into his garage door: the strength of your laughter would be proportional to your envy!).[9]

But let us return to the example of the comedy he mentions in Poetics 13, where spectators see Orestes ending up being reconciled with Aegisthes. What makes them laugh at that ending? Undoubtedly, the ancient spectators must have felt a sense of indignation: not avenging one’s father by killing his murderer is a moral fault. But it cannot be indignation that causes them to laugh (if anything, indignation provokes anger, not amusement). What makes them laugh, rather, is the complete oddity of such a reconciliation, its sheer incongruity. So more generally, one may suppose, what makes one laugh is the incongruity of the blunders of the characters on stage, perhaps coupled with their ugliness (which is moreover accentuated by the mask the actors wear). That these characters must be “inferior” is a psychological requirement that is the opposite of that of tragedy: if we can only feel fear and pity towards a hero who does not deserve his fate, we readily laugh at a man full of himself who slips on a banana peel. Similarly, Aristotle’s insists that a character’s blunders must not cause him to suffer, and that therefore a comedy cannot end with the characters’ misfortune or death. That would not be funny: a comedy in which the arrogant character killed himself on a banana peel would end our amusement (provided we are average, morally good people). And this also implies that our negative feelings towards such characters, made up of a certain contempt and perhaps a feeling of superiority (as Hobbes would say), is really just a kind of game: we do not feel any real animosity or condescension towards comedy characters. If we really felt indignation when attending that comedy of Orestes, we would not be able to laugh.

In the third book of the Rhetoric, dedicated to the figures of speech, Aristotle recalls that in his Poetics, presumably in its second book on comedy, he spoke of different “types of jokes” (III 18, 1419b6–7). There is perhaps a difference of emphasis on this point between comedy and tragedy. If a comedy must also have a good plot, having good jokes is essential:

It is especially in regard to these [funny comparisons] that poets fail with the public if they do not make them well, and if they do make them well, that they are popular. (11, 1413a10–11)

It is impossible to reconstruct what exactly those “types of jokes” would have been. But in his treatment there of the figures of speech, Aristotle evokes those that the comic poet uses. The central idea that emerges is that of incongruity. Aristotle argues that a serious speech that wants to persuade its listeners must use a “suitable”, or “appropriate” style, where the words chosen are “in proportion” (to analogon) to the things they signify. If there is disproportion, or what we call “incongruity”, that speech’s style will be “like that of comedy”. And to exemplify what he seriously states, Aristotle makes a joke at the expenses of a tragedy writer, a certain Cleophon, whose style was (at least in Aristotle’s eyes) rather like a comedy writer’s: “Some of his expressions were like saying: august fig tree” (III 7, 1408a10–16). This expression is funny (and not at all appropriate to the tragic style), because there is a total incongruity between this adjective which is normally reserved for a goddess or a queen, and that tree which, in ancient Greece, was considered of little value.

In the case of tragedy, Aristotle’s aim is to reconstruct for his readers what the art of poetic composition should be, in order to enable them to distinguish a good tragedy from a bad one and to explain why. There is no reason to think that this should not also have been his goal in writing on comedy. We have no direct echo of this in our Poetics, but a passage from the Nicomachean Ethics provides a clue worth following. It comes from the chapter devoted to the virtue of “sense of humor” (eutrapelia), which consists in being able to make good jokes among friends as well as in being able to take mockery against oneself with good grace. It is in this context that we read this statement:

The amusement of the free man is very different from that of the slave, that of the educated man from that of an uneducated man. This can be seen also in the ancient and recent comedies: in the former, what makes one laugh is obscene language, in the latter, it is rather the innuendos (huponoia), and this difference is not small in view of elegance (euschêmosunê). (IV 8, 1128a20–25)

To be sure, this is an analogy that is intended to make clear what kind of jokes decent citizens should make or hear in the real world. But this analogy also reveals the normative judgment that Aristotle suggests when it comes to theater: good comedy, that which is intended for educated audiences, should use innuendo and allusion, not obscene language. This is not an ethical normativity, but a normativity intrinsic to the art of making people laugh. Jokes that use crude and obscene language are what might be called “easy jokes”, which require no originality and inventiveness on the part of the jokester. On the contrary, good jokes must be “refined”, or “elegant”, which show a good, truly incongruous and sophisticated use of the figures of speech.[10]

4. Music and the Value of Art
What is the value of poetry, or art more generally? Neither in the Poetics nor in the published works on poetry does Aristotle seem to be bothered by that question. But since this is yet another perplexing question for us, many scholars have been tempted to supply an ethical answer. Interpreting the katharsis clause in this way has been one major way of satisfying the urge to answer this question. But even without relying on that admittedly controversial phrase, other ethically tinged answers have been offered. For example, why can’t we read tragedies as providing us an attractive way to develop a typically “emotional understanding” of important ethical features, such as the importance of being moved by other people’s misfortunes (Halliwell 1986, 2002)? Or, more specifically, shouldn’t tragedies allow for imagining ethical situations that would help spectators illuminate issues linked to their use of practical wisdom (Frede 1992; Belfiore 1992; Donini 2004)?[11] There is certainly much to say in favor of such readings, especially if one focuses on, say, Antigone or Philoctetes. But when Aristotle mentions these plays, he does not at all give the impression that he takes them to be paradigmatic of the genre (In the Poetics, he mentions the former in the framework of a critique of the quite untragic way Sophocles has depicted Haemon, who did not dare to kill his father Creon: 14, 1454a1–2; as to the latter, one verse of Euripides’ lost play is quoted for the appropriate use of a rare verb: Poetics 22, 1458b22–24). Instead, the two plays that he seems to take as paradigmatic (and to which he refers the most) are Sophocles’ Oedipus-Rex and Euripides’ Iphigenia in Tauris. Certainly, Oedipus-Rex can easily be read as providing a kind of insight into the deep miseries a human being can be subjected to; but one may fairly doubt that it might really help people in their use of practical wisdom. As to Iphigenia in Tauris, it would require a tremendous battery of arguments to persuade modern readers that they should read it in a either such way—and it is no surprise that no scholar has attempted to do so!

More generally, there are at least two main rebuttals to be made against such ethical readings of the Poetics (and presumably Aristotle’s aesthetics in general).[12] First, what then about comedy? When he mentions a possible positive use of comedy, Plato proposes that virtuous citizens watch them for a better grasp of what vices look like: “We must learn to recognizes buffoonery in order to avoid doing or saying anything ridiculous out of ignorance” (Laws VII 816e). It is telling that Aristotle never mentions such an odd suggestion: how can we possibly learn about vice while we are enjoying the jokes and gaffes that pepper comedic plots? A second reply comes from what Aristotle says about music in Politics VIII. There, he begins by repeating faithfully what Plato had proposed: since music can represent virtues like courage, children must learn to play it and it is through enjoying singing and playing such music that they will subconsciously come to enjoy those virtues. But then, in a second instance, Aristotle vigorously, if implicitly, opposes Plato. In the Laws, Plato very much insists that adult citizens of Magnesia must continue singing and dancing in order to bolster their virtues (II 664d). Aristotle is no less insistent that once adults, the citizens of his ideal city stop singing and playing music and instead hire professional musicians and enjoy listening to them during their “free time” (VIII 6, 1340b35–39).

“Free time” or “leisure” (scholê) is a key concept (on this, see Too 1998; Heath 2014; and Ferrari 2019). It does not mean the pause or the rest one takes from work; it is a time, or a mode of life, where activities are enjoyed for themselves. In the Nicomachean Ethics, it is at the core of the argument for the primacy of theoretical activity: while contemplation is typically a leisured activity, political activities constitute the non-leisured life, one which is meant to achieve practical goals (X 8, 1177b4–24). And in Politics VII-VIII, when defending his own proposal as how organize a perfect city, Aristotle goes as far as saying: “To be able to enjoy leisure rightly is the principle of everything” (VIII 3, 1337b31–32). In the case of music, Aristotle makes it clear that if music can be listened to for the practical purpose of relaxation (a little bit like our ambient music), or fostering virtue (as children must do), music can also be listened to for its own sake, just for the sake of enjoying it. Here Aristotle seems to be repeating what he says in the passage of the Eudemian Ethics already quoted, but he adds something we do not find there: music “for the sake of leisure” is characterized as a music “for the sake of intelligence (phronêsis)” (VIII 5, 1339a14–26).

Unfortunately, Aristotle does not spell out what exactly he means by that, and the phrase has been variously interpreted. Again, some scholars have proposed an ethical reading: the intelligence involved would amount to the practical intelligence, or wisdom (which is typically called phronêsis), that is required in any moral action. Thus, by listening to music that depicts, or “represents”, say, courage, one may better understand what courage is. Despite its seeming natural, this reading contradicts what Aristotle repeats in those same pages: moral, or political, action is fundamentally a non-leisured activity (ascholia). It would be odd then if Aristotle were recommending such a leisured use of music while aiming to describe how to manage non-leisured moral life! Another theme that is also repeated in those pages should give us a better clue: the final aim of learning how to sing and play an instrument is to help these future citizens become “good judges” of music (VIII 6, 1340b35–39). In the Laws, Plato also used that phrase, and meant by it that adult citizens should be able to recognize morally good from morally bad songs, and thus enable them to decide which songs must be sung in Magnesia (II 669a-670c). There is no reason to believe Aristotle would have endorsed such a strong ethical-cum-political agenda. Instead, it is very likely that by “judging”, Aristotle just means that by having the first-hand experience of playing music, citizens will be able to judge and appreciate the good quality of music. And, as all musical connoisseurs know, such an appreciation is both perceptual (as we do enjoy listening to sounds and rhythm) and also somehow intellectual: a connoisseur, that is, equally enjoys how well or inventively a piece of music is constructed.[13]

Admittedly, all of this is absent from the Poetics, where the word “leisure” does not even appear. But when in the Politics, he presents his conception of a music that should be listened to and appreciated for its own sake, it is surprising that Aristotle gives as justification the example of Homer, and cites several verses from the Odyssey: besides music for relaxation and moral education, there is

music for a life spent in leisure, which is the very reason why people bring it in. They give it a place among the leisure time that they think befits free people. That is why Homer wrote these verses: […] “They invite the bard who charms them all”. And elsewhere, Odysseus says that the best way to spend one’s life is in view when men are rejoicing: “the guests sit in a row in the room to listen to the bard”. (VIII 3, 1338a21–30)

Of course, Aristotle must have been aware that the bard is the one who sings poetry, whether lyric or epic, with the accompaniment of the lyre. This argument therefore implies that it is actually the listening and appreciation of performed poetry that must constitute such a leisured activity. And since in the Poetics, tragedy and comedy are presented as poetic forms that are more valuable than epic or lyric poetry, Aristotle must also have considered attending tragic and comic plays as part of these leisured activities, or more generally of what he calls in that passage of the Politics, a “free life”, that is a life of free citizens (not of slaves), but also a life that in which time is spent enjoying music or poetry for their own sake.

In the case of music, Aristotle strongly recommends that children learn how to play an instrument (actually, the lyre) so that once adults they will be able to fully enjoy beautiful songs since they will have become able to “judge the beautiful songs” and therefore “enjoy them for themselves” (Pol. VIII 6, 1340b38–39). This is also the case for visual arts: children, Aristotle proposes, should learn how to draw images in order to be able “to contemplate the beauty of bodies” (1338a40–b2). In both cases, their pleasure can be taken along the lines in which Aristotle defines pleasure in his Nicomachean Ethics: it is the unimpeded activity, or “activation”, of their natural capacity, or faculty, of hearing and seeing respectively. But Aristotle importantly adds: “when it comes to the best possible object” to be heard or seen (NE 10.4, 1174b14–23). We therefore need to learn how to judge how good the objects of our listening or seeing are. And once we have learned this, we can fully enjoy pieces of music and paintings or sculptures that we know are those that best suit our respective faculties.

Aristotle never makes such a suggestion in the case of poetry; and indeed, if one holds that poetry is a matter of exceptional natural gift, as Aristotle does, it would be odd to propose that every child should practice composing poetry. One might then perhaps take his writings on poetry offered as a sort of late education on such artworks. Through their reading of either the Poetics (if they get the chance of enrolling in the Lyceum) or Aristotle’s published works, people can become such connoisseurs of poetry, and enjoy it fully too.

It is often claimed that the ancients did not value artworks in the ways we moderns do. Ancient Greek theater, which includes poetry, scene painting and music, the argument goes, is only one moment of religious festivals where a whole city came to unite in shared activities and values. And despite all his critiques of the theater, for which he coined the despising word “theatrocracy” (Laws III 701a), Plato nevertheless emphasized that the right sorts of poetry and music should play a crucial role in the moral education of youth. But Aristotle seems very much unconcerned about the religious and political context of theater performances; and it is telling that he doesn’t say anything worth noticing about the role of the gods in the tragedies. (Actually, these elements are even a matter of reproach that classicists often make against him![14]) And he vividly opposes a complete subjection of art to any other useful objective: “To search everywhere for what is useful is totally inappropriate for those who are great in soul and free” (Pol. VIII 3, 1338b2–4). In other words, if indeed it may be useful to listen to some sorts of music for educational or therapeutic purposes, music can, and should, also be enjoyed for itself, and the same goes for painting and poetry. To be sure, by themselves, the objects of mimesis may not be worthy. But even the lowest animals, as Aristotle states in the preface of his Parts of Animals, do offer “extraordinary pleasures to those who are able to know their causes and are naturally gifted for philosophical understanding” (I 5, 645a9–10). In the case of animals and plants, we admire the way “artistic nature” (dêmiourgêsasa phusis) has built them. In the case of artworks,

we enjoy gazing at likenesses of animals because we are at the same time contemplating how artistic craft (dêmiourgêsasa technê), whether painting or sculpture, has produced them; (645a11–12)

that is, when we are connoisseurs of art. True, as Aristotle also says in that preface, divine beings such as celestial bodies and unmoved gods are much more worthy objects to contemplate (mostly because they are eternal). And indeed, as he repeats in his Nicomachean Ethics and Politics, philosophical contemplation, which culminates in exercising our intellect towards such divine objects, is the highest activity one can attain. But such philosophical contemplation, whether it comes to these divine entities or to the causes of the animals and plants, is deemed to remain for the “naturally gifted for philosophical understanding” which are few, as Aristotle seems to admit with regret. By contrast, provided one gets the right artistic education leading to connoisseurship, contemplating art can be part and parcel of everyone’s happiness (or typically human flourishing: eudaimonia).

1. Inner Speech as Actual Speech
The auditory-sensory character of inner speech is usually thought to be due to its involvement of auditory-verbal imagery (for an exception, see O’Brien 2013). Mental images (in any modality) are generally viewed as representations of particular things (or kinds of thing), not instances of those things. A visual image of a duck, for example, is a representation of a duck, not an actual duck. Likewise, it may seem that inner speech, insofar as it involves auditory imagery, is a representation of speech (and of its sounds, in particular), not actual speech. “Grass is green”, produced in inner speech, would then represent an utterance of the sentence, “Grass is green”, but it would not actually be an utterance of that sentence.

Notwithstanding this, many philosophers working on inner speech hold that inner speech really is a kind of speech. When we produce inner speech, we are literally speaking, albeit silently. We will call this view the “actual speech view”. Proponents include Carruthers (1996), Martínez-Manrique & Vicente (2010, 2015), Gauker (2011, 2018), O’Brien (2013), Jorba & Vicente (2014), Gerrans (2015), Gregory (2016, 2018) (though Gregory has indicated in more recent work (e.g., Gregory forthcoming) that he no longer holds the view), Machery (2018), Wilkinson & Fernyhough (2018), Wilkinson (2020), and Frankfort (2022). Martínez-Manrique & Vicente argued for the view in their 2010 paper; their 2015 paper, discussed in Section 3.3.2, sets out an updated version of their theory which incorporates some further commitments. Historically, the view can be traced at least to the Soviet psychologist, Lev Vygotsky (1934 [1986]), and it was also held by Ryle (1949 [2009]). However, Gauker develops a somewhat different version of the view—one that sharply distinguishes inner speech from the auditory-verbal imagery typically associated with it (see Section 3.2 for discussion)—which has its origins in Sellars (1956).

If inner speech is a kind of speech, instances of inner speech could aptly be called “inner speech utterances”, as producing inner speech would really amount to saying something. However, in order to be neutral on the issue, we will use the term, “inner speech episodes”, in this section and throughout the entry. An important issue for a proponent of the actual speech view is to explain how inner speech can consist of genuine linguistic tokens, given that it seems to be an imagistic phenomenon—where, as noted, the images may appear to be representations of speech sounds. For, even if inner speech consists of images of speech sounds, this only suggests that it consists of representations of linguistic items, not linguistic items themselves.

One style of answer has been offered by Sam Wilkinson (2020), who draws a distinction between imagery and imagination. He holds that sensory imagining is a “personal-level phenomenon”, which has components (2020: 16). One of the components of sensory imagining (as opposed to propositional or “attitudinal” imagining, which is typically assumed to be non-imagistic in nature) is mental imagery. For example, if one sensorily imagines a duck, then one component of this personal-level mental state may be a mental image resembling the appearance of a duck. There might also be other components, such as a stipulation that the image is an image of a duck and not another bird of similar appearance. But, Wilkinson emphasizes, mental imagery can be involved in many personal-level mental attitudes apart from the attitude of imagining, such as remembering, judging, reasoning, and others. “In a similar way”, he claims,

imagery … may be involved in an inner assertion. That does not, however, make the inner assertion simply nothing more than the imagery involved in its production, still less an act of imagination. (2020: 16).

Imagery can play many roles, Wilkinson is saying, and there is no reason that one of those roles should not be as a medium for linguistic tokens. The inner assertion is “a genuine assertion”—an instance of language consisting of imagery.

It might be replied that, although imagery can play a role in many personal-level mental states apart from imagining, it plays a very similar role in all of them, viz., representing how a concrete object (whether actual or possible) appears or sounds. Mental imagery does not tend to play a role similar to that of a linguistic token. So, even if mental imagery is involved in a range of personal-level mental states, it is not obviously well-suited, in the case of inner speech, to play the specific role of actual linguistic tokens.

This challenge might be met by connecting the actual speech view with work on the metaphysics of word tokens, as proposed by Wade Munroe (2022a, 2023). Munroe holds that

what makes something, φ, a token of a word type, w, is that the process of generating φ is explained and guided by one’s (tacit) knowledge of w (or the morphological structure of w), e.g., one’s semantic, syntactic, morphophonological/orthographic, knowledge of w stored in one’s mental lexicon. (2022a: 4)

This allows him to hold that inner speech episodes can involve word tokens, insofar as their generation is guided by the relevant kind of tacit knowledge. (Though Munroe himself does not hold the actual speech view; see Section 3.3.1 for discussion of his view.) Relatedly, J. T. M. Miller (2021) explicitly denies that word tokens are necessarily substances and holds, instead,

that particular or token words are objects, which are bundles of various sorts (most notably semantic, phonetic, orthographic, and grammatical) properties. [sic] (2021: 5737)

One might hold that inner speech episodes in fact consist in such bundles.

Although the matter of how inner speech episodes can involve genuine linguistic tokens is of great importance for the actual speech view, it is only beginning to receive attention. However, several arguments have been given in support of the theory generally. These include the following:

Inner speech may be a developmental descendant of a kind of external speech. Piaget (1923 [1926/1959]) observed that young children have a practice of speaking to themselves aloud. He described this kind of speech as “egocentric speech” (ibid, passim) (egocentric speech can be seen as one kind of private speech; see Introduction). Vygotsky (1934 [1986]) presented empirical evidence that inner speech develops in children as they internalize the practice of producing egocentric speech (though see Gregory (forthcoming) questioning this evidence). Vygotsky held that egocentric speech becomes silent, inner speech, but that it does not change in its fundamental nature, so it remains a kind of actual speech (see also Wilkinson & Fernyhough (2018), Wilkinson (2020)).
Introspectively, it seems like we can perform speech acts—e.g., make assertions and ask questions—in inner speech. But it would only be possible to perform speech acts in inner speech if inner speech is a kind of speech (Wilkinson 2020; Wilkinson & Fernyhough 2018). (This issue is addressed further in Section 4.1.)
On the face of it, we produce inner speech for purposes such as focusing our attention, motivating ourselves, and evaluating our actions. These correspond to purposes which instances of external speech also often serve: focusing the attention of others, motivating them, and commenting on their actions. There are also parallels in terms of how inner speech episodes and instances of external speech are constructed. Both often take the form of short, sub-sentential items when this is sufficient (e.g., “Here!”, upon finding something which was lost) and more fully elaborated sentences when this is necessary (e.g., when carefully listing the considerations relevant to a difficult decision which needs to be made, whether by oneself or by a group). Marta Jorba, Agustín Vicente, and Fernando Martínez-Manrique have taken these systematic parallels as evidence that inner speech and external speech are simply different types of one phenomenon, namely, speech (Jorba & Vicente 2014; Martínez-Manrique & Vicente 2015).
There seems to be a contrast between imagining speaking and engaging in inner speech, as it is ordinarily understood. This contrast, Gregory (2016) suggests, parallels the contrast between two kinds of external actions which we can perform. When an actor says the lines in their script, what they are producing is a representation of speech that someone else might produce. The actor is, of course, speaking, but they are doing so in the context of a pretense. What the actor is doing contrasts with the speech which they produce in, e.g., an ordinary conversation with someone. The contrast between imagining speaking and producing inner speech seems to map neatly onto the contrast between what the actor does on the stage and what they do in an ordinary conversation. If this is so, then a natural analysis is that the contrast between imagined speech and inner speech is a contrast between a representation of speech and actual speech—which implies that inner speech is a kind of actual speech.
A couple of philosophers who hold the actual speech view but express it in different terms, or who hold very similar positions, should be mentioned. First, Philip Gerrans (2015) describes inner speech as involving “imaginary action” (2015: 296), but he is explicit that, by this, he means only to say that producing inner speech is an action performed covertly. He takes inner speech to involve speaking, but doing so silently.

Second, Johannes Roessler (2016) holds that there are different kinds of inner speech, one of which involves imagining speaking (rather than actually speaking), but in a particular way. He points out that we can imagine things, or imagine doing things, for different purposes. An act of imagining will then be successful to the extent that it achieves the purpose for which it is performed. So, one might, for example, imagine making an assertion, but do so with the intention of imagining making an assertion which is true and relevant to context. Then the act of imagining making the assertion “incurs the same liabilities” (2016: 548) that the act of actually making the assertion would incur. If you are puzzling over some question, and you imagine asserting a possible answer, then the act of imagining will be successful only if you have imagined asserting the correct answer. Although you have only imagined performing the speech act of making an assertion, your imagined assertion will be “in some ways tantamount to an assertion” (2016: 548).

It would be an open position, though not one Roessler takes, that all inner speech episodes could be analyzed in this way. On such a view, inner speech episodes would be something very similar to actual speech, yet without quite being speech acts, and thus without the commitment that producing inner speech involves producing actual linguistic items.

2. Inner Speech and Thought
A second question about inner speech is how it relates to thought. It seems that there must be some relationship, but it is an open question what that relationship is. In general, there are three views about the nature of the relationship: (1) inner speech episodes express thoughts; (2) inner speech episodes facilitate thoughts; and (3) inner speech episodes (at least sometimes) are thoughts of a certain kind.

The views are not mutually exclusive: one can certainly hold that inner speech is related to thought in multiple ways.

2.1 Inner Speech and Thought Expression
Langland-Hassan & Vicente (2018b: 10) observe that the view that inner speech (at least often) expresses thoughts that are distinct from the inner speech episodes themselves coheres with some larger theories about thought and language. If one is attracted to these theories, then they may well also be attracted to the view that inner speech merely expresses thought.

First, there is a natural connection between the language of thought hypothesis, most closely associated with Jerry Fodor (1975), and the view that inner speech expresses thought. On the language of thought hypothesis, our thoughts do take place in a language, but not in a natural language. Rather, our thoughts take place in a kind of mental language, often referred to as “Mentalese”. If the language of thought hypothesis is true, then, insofar as inner speech is keyed to a natural language, it seems that inner speech can at most serve to express the thoughts which occur in the mental language.

Second, on Willem Levelt’s influential theory about language production, speaking involves conveying a pre-existing “message” (1989: passim). The structure of this message is conceptual but not linguistic. Via several stages of processing, natural language sentences (or sub-sentential items) are formulated which, once articulated, express the conceptually structured message with which the process started. If one thinks that inner speech is actually a kind of speech, then one might incline to think that inner speech also expresses a pre-existing message.

Thus, Peter Carruthers (2009, 2018) approaches matters from a Fodorian and Leveltian angle when he proposes that

the first metacognitive access subjects have to the fact that they have a particular belief is via its verbal expression (whether overtly or in inner speech). (2009: 125)

For Carruthers, the inner speech episode is not a belief or judgment itself, but rather the expression thereof (see Section 5.2). In a similar way, Ray Jackendoff (1996, 2007, 2011, 2012) emphasizes the distinction between thought itself and the auditory imagery by which it may be expressed, identifying only the latter with inner speech (see Section 3.1). Likewise, José Luis Bermúdez (2003) and Jesse Prinz (2011) distinguish between conceptual thought itself and inner speech, while holding that we often come to know what we are thinking by attending to inner speech sentences that we might use to express such thoughts. They stop short of explicitly claiming that such sentences actually express thoughts, however, specifying instead that the inner episodes are sentences through which such thoughts “might be expressed” (Bermúdez 2003: 164), or that we “would use” to express them (Prinz 2011: 186) (see Section 5.1).

One can, however, hold that inner speech episodes express thoughts without committing to the view that a thought must be fully-formed prior to the production of the relevant inner speech episode. José Luis Bermúdez (2018), for example, holds that producing an inner speech episode can actually play a role in forming the thought which it expresses. For Bermúdez, a thought can be refined and precisified as an external utterance is being produced and, equally, a thought can be refined and precisified while an inner speech episode is being produced. Nonetheless, by the time an inner speech episode has been produced, it will express an existing thought.

Finally, it is worth noting the following point of contact between the actual speech view, discussed in Section 1, and the question of whether inner speech expresses thought. If it is an essential feature of speech that it serves to express thought, then defenders of the actual speech view are likewise committed to the view that inner speech expresses thought. If, on the other hand, one holds that there can be (inner) speech that does not express thought, then the question arises as to what the difference between (inner) speaking and thinking in a natural language might be—and whether there is indeed a difference.

2.2 Inner Speech and Thought Facilitation
There have been several suggestions as to how inner speech might play a substantive role in facilitating thought or thought processes—a role that goes beyond merely expressing thought processes.

First, inner speech is often thought to play an important role in working memory. According to Alan Baddeley’s influential theory of working memory (e.g., Baddeley 1992), we can retain a series of words or numbers in working memory by reciting them in inner speech. A short series of items will be retained long enough to recite them again. One can iterate this process via a “phonological loop” for as long as desired.

Following Vygotsky (1934 [1986]), Clowes (2007) and Jorba & Vicente (2014) hold that inner speech can serve as a tool for directing our own attention, just as external speech can serve as a tool to direct the attention of others. In making this case, both draw on the Vygotskyan developmental account of inner speech, on which inner speech is derived from the external phenomenon. See also Martínez-Manrique & Vicente (2015), who make the same point but are less directly influenced by Vygotsky’s original (1934 [1986]) developmental account.

There is evidence that inner speech facilitates various executive function tasks, such as planning, task-switching, and inhibiting impulsive and inappropriate responses, without being essential to them. The evidence that inner speech can play a role in these tasks is primarily empirical. For reviews of the relevant literature, see Alderson-Day & Fernyhough (2015) and Petrolini, Jorba, & Vicente (2020).

Munroe (2022b; forthcoming) argues that inner speech plays a role in reasoning which goes beyond merely aiding or improving it. He notes that reasoning processes often involve preserving representations in working memory. In doing complex mental arithmetic, for example, one might recite in inner speech the word for a number which they have determined will be needed later in the process, e.g., when regrouping values (i.e., “carrying” and “borrowing”). The number word will be stored in working memory via the process described above. But, on Baddeley’s model of working memory, which Munroe is working with, only sensory representations can be stored in working memory. In the present context, this means that only auditory representations of the relevant word sounds can be stored, not the conceptual content which the word would have if spoken aloud (or, possibly, if it were produced in inner speech in a different context, depending on one’s view on the contents of inner speech—see Section 3). When one needs to use the number at a later stage in the process, they will need to interpret the sensory representation which they are producing. For example, if they are reciting a sound corresponding to the word, “six”, in inner speech, they will need to interpret that as the word referring to the number, six, so that six becomes the number that they now use to continue their calculations. If this is so, then interpreting the inner speech that one was producing, and thus the inner speech itself, was essential to the reasoning process, not merely a dispensable aid. Munroe holds that the same will apply in many reasoning processes performed that require making use of an intermediate conclusion.

A number of theorists—especially those working in neo-empiricist (Barsalou 1999; Prinz 2011, 2012) and embodied cognition traditions (Borghi et al. 2017; Dove 2014)—have also proposed that inner speech plays an important role in facilitating abstract thought, i.e., thought about objects or properties that are not easily perceived. Here the idea is that language perception and production abilities—and their internalization, via inner speech—provide means for explaining the acquisition and use of abstract concepts in broadly sensorimotor terms. In particular, Guy Dove (2014, 2018, 2020, 2022) develops a view where language—often in the form of inner speech—is used as a “scaffold” or “tool” for enabling thought about abstract entities, and where the capacity for abstract concept use is closely tied to the capacity for language.

Finally, if subsystems and modules in the mind function in isolation from one another to any significant extent, then inner speech may play an important role in integrating their output. Carruthers (2002, 2006) suggests that the process of language production generally, including the production of inner speech, is especially well suited to integrate the output of multiple modules, because of the combinatorial nature of language. In producing an episode of inner speech, one can thus express complex content, which is then distributed to mental modules and subsystems for further processing. Other sources relevant to inner speech and the integration of information produced by different parts of the mind include Baars (1988) and Dennett (1991).

2.3 Inner Speech as Thought
A number of philosophers have argued that at least some inner speech episodes actually are thoughts or, at least, parts of thought processes. Gauker (2011, 2018) holds that all conceptual thought occurs in inner speech, where, as elaborated in Section 3.2, he takes inner speech to involve the tokening of items of a natural language in neural states that are distinct from the auditory-verbal representations that many identify with inner speech. In his 2011 book, he responds to arguments that conceptual thought cannot occur in natural language.

With respect to inner speech understood as a partly sensory phenomenon, Keith Frankish (2018) describes how inner speech can be used to break a complicated problem into smaller problems, which can then be addressed by lower level, automatic thought processes. Deciding whether to accept an invitation from colleagues to attend a party, for example, one might produce the inner speech episode, “What will it be like?”. This more circumscribed question can be addressed by autonomous processes, such as recalling previous parties with colleagues. Along with other autonomous processes, this might generate the prediction that an annoying colleague, Henry, will likely be at the party. If this is significant, it could result in the inner speech episode, “Henry will probably be there”, in turn prompting a largely autonomous evaluation of the effort involved in enduring Henry’s company. The process could result, depending on the outcome of this evaluation, in producing the inner speech episode, “I can’t face that; I won’t go”. (Quotes from Frankish [2018: 234], though the example is slightly modified.) The inner speech episodes, Frankish believes, are critical to making the decision, and are thus rightly considered parts of the process of thinking itself. See Kompa (forthcoming) for a similar argument; cf. Munroe (2022b), discussed above, who also holds that an inner speech episode can be essential to a thought process but does not infer from this that an inner speech episode can actually be a part of the process, but see also discussion of Munroe (2023) below.

Frankish (2018) also holds that inner speech episodes can be thoughts in the form of conscious commitments, where these are “a distinct kind of mental attitude” (2018: 237), which cannot be analyzed in terms of other conscious mental states, such as conscious decisions, beliefs, or desires, or expressions of other mental states. They are simply commitments made to oneself to “regulat[e] our future activities, including our intentional reasoning, in line with the choice or view expressed” (2018: 237). For example, the inner speech episode, “I will go to the gym today”, is a commitment to go to the gym today, not just the expression of a decision to do so, because it also generates a kind of obligation to oneself, as it were, to do so. For Frankish, this follows from treating inner speech as an internalized version of interpersonal speech, in which commitments also generate obligations.

On Frankish’s account, an inner speech episode can be like a judgment, insofar as it may involve committing oneself to act and reason in a way which is consistent with the truth of the proposition expressed by the inner speech episode. Munroe (2023), by contrast, holds that an inner speech episode can actually function as a judgment. If an inner speech episode is accompanied by what has been called a “Feeling of Rightness” (Munroe cites Thomson et al. 2013 and Unkelbach & Greifender 2013), then it will play roles typically attributed to judgments such as “terminating inquiry and causing overt actions” (Munroe 2023: 309). Munroe connects his claim to a model proposed by Ackerman & Thompson (2015, 2017a, 2017b) on which the roles that mental states play is determined partly by metacognitive monitoring. The “Feeling of Rightness” is a cue to a metacognitive monitoring system that a particular mental state can appropriately play the roles of a judgment. Munroe’s claim is that inner speech episodes can function as judgments if this is deemed appropriate by the metacognitive monitoring system, on account of being accompanied by the appropriate “Feeling of Rightness” (or at least by a feeling of sufficient certainty).

Nikola Kompa (forthcoming) adds a quite different argument for the identity of (some) thoughts and (some) inner speech episodes. She operates with a broad notion of inner speech, on which any “inner episode that substantially engages the speech production system” is an instance of inner speech (forthcoming: 4, emphasis removed). On this understanding of inner speech, any thought with semantic content and syntactic structure will be an instance of inner speech, even if it does not become conscious. Kompa rejects the language of thought hypothesis, on which thoughts can have linguistic properties because they occur in a non-natural language. Accordingly, for Kompa, the only way that a thought can have semantic content and syntactic structure is if its formation substantially involves the speech production system (which she understands in Leveltian terms, citing Levelt 1989; Levelt et al. 1999; and Indefrey & Levelt 2004). Insofar as we have any thoughts that have semantic content and syntactic structure, then, these are, on her definition, instances of inner speech. If the production of such thoughts does not proceed further through the speech production process, such that they are morpho-phonologically encoded in addition to having semantic content and syntactic structure, they will occur as unconscious inner speech episodes.

Finally, it has been suggested that there is a close connection between inner speech and a phenomenon known as “unsymbolized thought”. Using the Descriptive Experience Sampling paradigm, Russell Hurlburt and Christopher Heavey (e.g., Hurlburt & Heavey 2002; Heavey & Hurlburt 2008) have gathered introspective data that they interpret as providing evidence that people sometimes have the experience of

thinking a particular, definite thought without the awareness of that thought’s being conveyed in words, images, or any other symbols. (Heavey & Hurlburt 2008: 802)

Martínez-Manrique & Vicente (2015), Vicente & Martínez-Manrique (2016), and Vicente & Jorba (2019) suggest that these “unsymbolized thoughts” occur when the production of an inner speech episode is aborted at the earliest stage of production, when only the content or message to be expressed has been formulated. Appealing to accounts on which we experience conscious representations of actions which we begin to perform but abort, they suggest that an unsymbolized thought is a representation of the message which one commenced expressing in inner speech, which becomes conscious because the process was aborted. Insofar as the process was aborted prior to the message being organized in phonetic form, the representation is entirely amodal. See also Kompa (forthcoming).

3. Content-Based Theories of Inner Speech
We have seen that there are a variety of views taken on whether inner speech is indeed a kind of speech, or a kind of thought, or both. A popular way to gain added leverage on those questions is to advance an account of the contents of inner speech. Focusing on questions concerning the contents of inner speech also helps to clarify the depth of some of the puzzles and controversies already introduced.

Most generally, the content of a representation is what the representation is of or about—it is what the representation represents. The content of the word “cat” is a certain type of animal (namely, a cat). And, the content of the sentence “cats are animals” is the proposition that cats are animals. Two distinct representations can have the same content. For instance, the French word “chat” has the same content as the English word “cat”; and the French sentence “les chats sont des animaux” has the same content as the English sentence “cats are animals”. Thus—to borrow analogies from Siegel (2005 [2021])—the contents of a mental state, in the present sense, are akin to the contents of a newspaper article and not akin to the contents of a bucket. Mental contents are not things that are contained within mental states themselves (just as cats are not contained within the word “cat”) but are, instead, what the mental states are of or about.

We will distinguish three broad classes of views about the contents of inner speech and several sub-views within them, noting their main motivations and relationships to questions concerning inner speech’s proposed cognitive roles. According to what we will call the “phonological content view”, inner speech episodes always and only have phonological contents. The competing content-based theories to be discussed hold either that inner speech only has semantic contents (the “semantic content view”, as we will call it) or that inner speech has phonological contents and semantic and/or other kinds of contents (the “mixed contents view”).

As we will see, the phonological content view is a natural fit with the view, discussed at the beginning of Section 1, that inner speech is merely a representation of speech and not actually a kind of speech. This is because the phonological content view sees inner speech as consisting in imagistic representations of speech and as lacking the kinds of contents (or meanings) associated with word tokens themselves. Likewise, those who hold that inner speech is actually speech will typically hold either a mixed contents view or a semantic content view, as these views allow inner speech episodes to have the kinds of semantic contents that are typically viewed as essential to being a linguistic token.

3.1 The Phonological Content View
To say that inner speech has phonological contents is to say that inner speech episodes represent phonemes (or phones), where phonemes are the most basic meaningless building-blocks from which any word of a language can be built. There are 44 phonemes in English, different combinations of which account for the distinct sound each word has in relation to all other words from which it can be aurally distinguished. The notion of a phoneme is somewhat of an abstraction, however, as slightly different sounds (in terms of pitch, timbre, and frequency) can fall within the sonic range that constitutes a single phoneme type. These more specific, concrete sounds that can qualify as instantiations of a phoneme are known as phones. Whether inner speech episodes represent phonemes or, instead, the finer-grained property of being a phone is a matter of dispute among those who hold that inner speech episodes have phonological contents (Patel 2021; Langland-Hassan 2018; Hill 2022).

Note also that, while the phonemes of most natural languages are auditory in nature—and are thus perceived through the sense of hearing—the notion of a phoneme has also been applied to gestural languages, such as American Sign Language (Sandler 2012; Stokoe 2005). So, the concept of a phoneme is not specific to any modality. It refers to the smallest meaningless units of a language that can be arranged and recombined to form the smallest meaningful units of that language, no matter which modality the language occurs in. In spoken languages, however, the auditory modality takes precedence over the visual/written modality, insofar as the phonemes are typically held to be sounds, while the graphemes are held to be letters or groups of letters that represent phonemes. While most will not consider the visualization of graphemes and written words to be cases of inner speech, it bears noting that such visualizations satisfy the neutral characterization of inner speech provided at the outset.

There are several reasons one might hold that inner speech episodes have phonological contents. The first is phenomenological in nature. What it is like to have an inner speech episode is similar to what it is like to hear oneself saying the corresponding words aloud. One might explain this phenomenological similarity by appeal to the fact that inner speech episodes and the corresponding cases of hearing represent similar properties—either phonemes or phones of a certain sort—and, accordingly, have similar contents. A second reason appeals to the fact that we can use inner speech episodes to judge whether two visually dissimilar written words—such as “blood” and “mud”—rhyme. As rhyming is a relationship between the sounds of words, the usefulness of inner speech episodes in judging rhymes would be explained if inner speech episodes represented word sounds and thereby allowed us to compare those sounds (Langland-Hassan 2014). A third reason that has been proposed for thinking that inner speech has phonological contents is that it is the representation of those features that allows one to discern which language we are exploiting when engaged in inner speech (Langland-Hassan 2018). (See Patel 2021 for a rebuttal.)

Jackendoff (1996, 2007, 2011) proposes that auditory contents exhaust the contents of inner speech. Jackendoff’s view is motivated in part by a prior commitment to the thesis that we do not think in a natural language. Like many in cognitive science, he sees natural language primarily as a means for communicating thoughts that themselves occur unconsciously in some other medium (such as a Fodorian “Mentalese”). According to Jackendoff, thought itself is never conscious, nor is the use of concepts. By contrast, inner speech—what he calls the “talking voice in the head” (1996: 10)—occurs consciously and does not involve the use of concepts. In having inner speech, he explains, “[w]e experience organized sounds”, whereas,

the content of our experience, our understanding of the sounds, is a different organization … called conceptual structure. (emphasis original, 1996: 12–13)

“The organization of this content”, he holds, “is completely unconscious” (1996: 13). Jackendoff identifies the inner voice with a representation of “phonological structure”, a representation having phonological content, yet no conceptual or semantic content. Whereas, the mental states constituting our understanding of what the voice is saying, he notes, are distinct conceptual states that occur unconsciously:

What we experience as our inner monologue is actually the phonological structure linked to the thought,

he explains.

We are aware of our thinking because we hear the associated sounds in our head. (Jackendoff 2011: 613)

(See also Jackendoff [2007: 80–85] where he remarks on the counterintuitive nature of his view: “How can the contents of consciousness consist of just a string of sounds?” [2007: 85].)

It should be noted that Jackendoff also suggests that inner speech episodes “express” thoughts, which would seem to support the view that such episodes have the semantic contents of our thoughts (e.g., “the linguistic modality can make reasons as such available in consciousness” [1996: 19] and “only through language can such concepts form part of experience rather than just being the source of intuitive urges” [1996: 23]). On the other hand, he equally emphasizes the overlooked fact that “linguistic structure has three major departments: phonological, syntactic, and semantic/conceptual structure”, and that “the forms in awareness—the qualia—most closely mirror phonological structure” (2007: 81). Most recently, he has proposed a view where what we intuitively mark as “conscious thought” has three components: a “pronunciation” of the thought, a feeling of meaningfulness, and the meaning attached to the pronunciation. There he holds that only the first two are conscious and appears to identify inner speech with the “pronunciation” component. This is in keeping with the phonological content view, as the (semantic) meaning of the pronunciation is something separate from the pronunciation and is only represented “backstage” (i.e., unconsciously) (Jackendoff 2012: 84–5).

Langland-Hassan (2014) provides a qualified defense of a phonological content view, motivated by worries about how a single mental state—in particular an episode of inner speech—can be said to represent both word sounds and word meanings simultaneously. He notes that a word’s meaning and its sound are entirely distinct properties, related only by convention. If mental states are individuated by their contents, then it seems that distinct neural or functional states will be needed to represent these distinct properties. This has become known as the “binding problem” for inner speech (see Munroe 2023; Patel 2021; Bermúdez 2018 for different approaches to resolving it; see also Prinz 2011 for related remarks). In light of this problem, Langland-Hassan proposes that ordinary episodes of inner speech likely consist in two or more mental states triggered at roughly the same time (this would be a multiple-state version of the “mixed contents” view, discussed below). Yet he adds that, when inner speech has been divided into distinctly occurring states in this way, there are good reasons to identify inner speech solely with the component that represents word sounds. Doing so results in a phonological content view.

3.2 The Semantic Content View
In contrast to the phonological content view, the semantic content view holds that inner speech episodes always and only have semantic contents. By “semantic contents”, we mean the kinds of contents had by ordinary words, phrases, and sentences of a natural language. Such contents are typically equated with the meaning of a word, phrase, or sentence.

One version of a semantic content view, defended by Christopher Gauker (2011, 2018), holds that inner speech episodes exclusively have semantic contents and entirely lack both auditory contents and auditory phenomenology. Gauker allows that episodes of auditory verbal imagery often accompany inner speech. However, on his view, this auditory imagery is not to be identified with inner speech itself. Rather, according to Gauker, inner speech is a non-sensory linguistic phenomenon occurring in the brain that is (often) represented by episodes of auditory verbal imagery. Just as we may use auditory representations to represent someone else’s speech that we are actually hearing, so too, for Gauker, our inner speech is often represented by verbal imagery—imagery that is in fact distinct from the (inner) speech itself. (Here Gauker develops related remarks of Wilfrid Sellars (1956).) Notably, Gauker (2018) grants that, in the case of inner speech, this auditory-verbal imagery misrepresents our inner speech as having sonic features (i.e., as instantiating phones or phonemes), given that the neural events that constitute inner speech episodes are themselves silent.

Gauker’s style of pure semantic content view is not widely endorsed. This may be because it clashes with the widespread view that inner speech has a sensory character similar to that of hearing speech. On the other hand, Gauker’s view can be said to have an advantage in providing a literal sense in which, when we engage in inner speech, we are thinking in words of a natural language and not merely about them. On Gauker’s (2011) view, the neural events that carry semantic content are themselves tokens of words and phrases of a natural language, and the question of how auditory-verbal images can also be linguistic tokens does not arise. His view is also motivated by an opposition to what he calls the “Lockean” view that sees conceptual thought as something prior to and separate from the speech that expresses it. One can see Gauker (2011) as trying to preserve the idea that abstract (conceptual) thought occurs in a language (and is often non-conscious), while divorcing it from the thesis that there exists an innate, Fodorian “language of thought” (and one that must be exploited in order to learn a natural language).

Bermúdez (2018) offers a different style of semantic content view that allows for inner speech to retain a characteristic auditory phenomenology. According to Bermúdez, the auditory sensory character of inner speech is a result of inner speech episodes having non-representational auditory properties. For Bermúdez, the only representational contents had by inner speech episodes are those pertaining to the meanings of words. In response to the those who argue that inner speech episodes must also have phonological contents (e.g., to explain why we can use inner speech to judge whether two words rhyme), he argues that there is no entailment from the fact that inner speech episodes can be useful in judging rhyme relations to the conclusion that they represent phonemes (2018: 216–7).

A third type of theory on which inner speech exclusively has semantic content proceeds by arguing that inner speech is a genuine form of speech. This argument is typically made on either phenomenological or functional grounds. From there it is inferred that inner speech must have the same kind of contents as external speech. If episodes of external speech—i.e., the words we hear when someone speaks—have semantic content but no phonological content (because they do not represent phonemes), so too must episodes of inner speech. This approach to theorizing about inner speech is discussed in more detail in Section 1. Assuming that (unlike Gauker) proponents of such a view wish to maintain that inner speech episodes constitutively have auditory sensory character, they may concur with Bermúdez in his claim that the auditory phenomenology of inner speech does not entail the representation of auditory properties; or, alternatively, they may provide some other account of why, in many instances, inner speech seems to represent phonemes even if it does not really do so.

3.3 Mixed Contents Views
Mixed contents views hold that inner speech episodes typically have at least two kinds of content—phonological and semantic—simultaneously. On a mixed contents view, the inner speech episode “Dogs are mammals” represents both the sound of the sentence “Dogs are mammals”, as uttered aloud, and the proposition that dogs are mammals. We can distinguish two species of mixed contents view: single-state and multiple-state. Single-state views hold that what we intuitively mark as a single inner speech episode consists in a single mental state that has both auditory and semantic contents. Multiple-state views hold that the apparent unity of a single inner speech episode is in some sense illusory, as such episodes typically consist in the contemporaneous occurrence of two or more mental states, where one of the states represents phones or phonemes and another has semantic contents. (Some multiple-state views hold that inner speech episodes involve additional distinct states with articulatory and syntactic contents as well.) As earlier noted, some phonological content views hold that mental states with corresponding semantic contents occur contemporaneously with the representations of phonemes that are identified with inner speech. These phonological content views differ from multiple-state mixed contents views in that the former identify inner speech solely with the state that has phonological content, perhaps on the grounds that it is the only sort of state of which one is consciously aware (this appears to be Jackendoff’s motivation).

3.3.1 Single-State Mixed Contents Views
Carruthers (2011, 2018) defends a single-state mixed contents view, proposing that inner speech involves the generation of a representation of word sounds (i.e., phonemes) which—in a process akin to what occurs in outer speech perception—is then interpreted by one’s speech comprehension mechanisms so that a semantic content can then be assigned to the represented utterance. (He notes that a representation of the semantic content of the represented phrase—referred to as the “message” on Levelt’s [1989] speech-production framework—sometimes precedes the representation of the word sounds, albeit non-consciously.) Once the represented word sounds are interpreted, Carruthers suggests, the information that the represented utterance has a certain semantic content is “bound into” a single “event-file” that contains information both about the sound and the meaning of the represented utterance (2018: 41–42). (See Frankish [2004: 57; 2018] for a similar view.) Carruthers analogizes such binding to the way in which the color, shape, and category properties of a visually perceived object are said to be “bound into” a single object-file that accumulates multiple forms of information about a single object, despite those properties being represented in temporally distinct stages and in distinct neural regions. These event-files, when activated and globally-broadcast, are said to constitute a single conscious inner speech episode that has both auditory and semantic contents.

Munroe (2023) develops a similar style of single-state mixed contents view, arguing that, in addition to representing phonemic and semantic features, inner speech episodes also represent the likelihood that the content of the represented utterance is true. The latter is necessary, he holds, for inner speech episodes to qualify as judgments (see Section 2.3). These three distinct features are, for Munroe, bound into a single mental state in the sense that a single mental state predicates these three distinct properties of a single represented utterance (Munroe 2023: 304).

3.3.2 Multiple-State Mixed Contents Views
Other mixed contents views of inner speech—inspired by Levelt’s (1989) multi-stage model of speech production—attribute the different representational contents entertained during an inner speech episode to multiple distinct states that tend to co-occur. Martínez-Manrique & Vicente (2015) defend a multiple-state view under the moniker of the “activity view” of inner speech, highlighting the multi-component processes of both inner and outer speech. “It is quite natural”, they explain,

to try to understand inner speech in terms of all the representations that are mobilized in speech, i.e., semantic, syntactic, maybe articulatory …. The representations involved—from conceptual to phonological—form an integrated system. (2015: 8)

The view which Martínez-Manrique & Vicente set out in their 2015 paper bears clear similarities to the actual speech view, insofar as they hold that inner speech is functionally similar to external speech. What separates it from the actual speech view, however, is that they do not hold that inner speech consists of actual words and sentences which express semantic content, but of distinct representations of phonological and semantic (and other) content. (For complementary multiple-state mixed contents views in cognitive neuroscience, see Grandchamp et al. 2019 and Lœvenbruck et al. 2018.) While these representations are unified in the sense of occurring within a single system for language production, they remain distinct mental states—distinguished, in part, by their distinct contents, and their ability to occur in isolation of each other. (Note, however, that this way of categorizing the view assumes that each mental state is composed of exactly one mental representation. It may be possible to articulate a view where one mental state is composed of multiple mental representations. The question then becomes: in virtue of what do the multiple representations qualify as a single mental state, as opposed to components or stages of a single cognitive system?)

Christopher Hill (2022: 136–139) develops a similar multiple-state mixed content view, emphasizing that the representations of semantic content lack any associated phenomenology. The phenomenology of inner speech is, for Hill, entirely a function of its auditory-phonological contents. He further specifies that these phonological contents are (the more abstract) phonemes, and not phones, to account for the relatively impoverished sensory character of inner speech in comparison with speech perception. Patel (2021) also defends a multiple-state mixed contents view, on which, in addition to having some combination of semantic, syntactic, auditory, and articulatory contents, inner speech episodes have vocal contents. To have vocal contents is to represent some particular person’s voice as communicating some combination of semantic, syntactic, auditory, or articulatory information. According to Patel, whether we are representing the semantic, auditory, or articulatory contents, these mental events involve one’s representing a certain person’s voice as attempting to convey such information. This common representation of a voice, he argues, provides a kind of unity to the class of mental events that can be considered inner speech.

Because multiple-state views allow that the distinct components of inner speech can potentially occur in isolation, they face a question of which components need to occur for the episode to be properly counted as an instance of inner speech. Vicente & Jorba (2019), Martínez-Manrique & Vicente (2015), and Vicente & Martínez-Manrique (2016) see this as an advantage, insofar as it allows them to place different phenomena related to inner speech on a single continuum (see also Kompa & Mueller forthcoming and McCarthy-Jones & Fernyhough 2011). For instance, when the semantic and syntactic contents of ordinary inner speech are represented in the absence of any auditory-phonological contents, they propose, this can be understood as a case of so-called “unsymbolized thought” (Heavey & Hurlburt 2008; Heavey, Moynihan, et al. 2019). See Section 2.3 for further detail.

A notable feature of the surveyed mixed contents views (as well as the phonological content view) is that they need not (and often do not) hold that inner speech episodes occur in a natural language. Rather, on these views, inner speech episodes represent natural language utterances (in virtue of their phonological contents), without necessarily being instances of such utterances themselves. This is because, on mixed contents views, the semantic content of an inner speech episode may not be represented by tokens of a natural language. For instance, for Carruthers, the semantic contents of an inner speech episode are represented via symbols of an amodal language of thought (e.g., a Fodorian [1975] Mentalese), which are coupled with sensory representations of the sound of the corresponding sentence as spoken aloud. One language (Mentalese) is used to represent the meaning of an expression in another (e.g., English). In this way, Carruthers (2010, 2018) deviates from Carruthers (1996), with the latter defending the idea that inner speech episodes literally occur in—and are expressions of—a natural language. Carruthers now emphasizes the point, raised also by Machery (2005), that introspection does not provide grounds for claims about the representational format of our inner speech episodes.

4. Inner Speech and Pragmatics
In general, the philosophy of language has focused primarily on language used interpersonally. It is natural to wonder to what extent this material is applicable to inner speech. This question can be asked whether or not one thinks that inner speech is actually a kind of speech, as no one denies that there is some interesting relationship between inner speech and interpersonal speech.

4.1 Inner Speech and Speech Acts
As mentioned in Section 1, the intuition that we can perform speech acts in inner speech is the basis of an argument that inner speech is a kind of speech. There are different ways, however, that we might understand the claim, depending on how one thinks of speech acts.

On the traditional analysis of Austin (1962) and Searle (1969), performing a speech act is inherently something one does in accordance with conventions tacitly understood by both speaker and listener. For example, for Searle, asserting p involves (approximately) undertaking to someone that p is true, where the speaker does not know that the listener already knows that p is true. The reason that an assertion can be effective is precisely that both speaker and hearer understand that this is the nature of the transaction. It is hard to see how this kind of analysis could apply to inner speech. One would need to explain how one individual can have two distinct roles, as speaker and listener, such that the conventions that make interpersonal language-use possible can have any relevance (see Gregory 2017, 2020a for related discussion).

Not every version of speech act theory, however, emphasizes conventions. Drawing on some ideas from Strawson (1964) and Bach & Harnish (1970), though not adopting their theories in whole, Wilkinson (2020) holds that what is essential to speech acts is that they express particular mental states. An assertion, for example, is simply an utterance which expresses a belief; a question is an utterance which expresses a desire to acquire certain information; etc. On this view, understanding someone else’s utterance is simply a matter of grasping its content and knowing what kind of mental state the relevant type of utterance expresses. Setting aside the question of whether one needs to interpret their own inner speech, it may be that inner speech episodes can be speech acts if one thinks of speech acts merely as expressions of particular mental states, rather than as actions which depend on conventions in the way that Austin and Searle suggest. For another analysis of inner speech in terms of speech act theory, see Geurts (2018), who emphasizes that inner speech episodes can operate to generate commitments in a way characteristic of speech acts; see also Frankish (2018) and Fernández Castro (2019).

An issue which sits just behind the question of whether inner speech episodes are speech acts is whether they are actions at all. Gregory (2020b) argues that, in the vast majority of cases, inner speech episodes are not actions, because we cannot give reasons for them (which is the criterion for actionhood on Davidson’s (1963) causal theory); they are not subject to our control (the criterion on Harry Frankfurt’s [1978] guidance theory); and we do not try to produce them (the criterion on O’Shaughnessy’s [1973] theory and Hornsby’s [1980] theory). If inner speech episodes are not actions, then they cannot be speech acts.

Tom Frankfort (2022) takes the opposite view. He observes that a great deal of inner speech is involved in deliberation, where this is an expansive category including “reflecting, reasoning, considering, evaluating” (2022: 52). He then applies Mele’s (2009) distinction between actions which involve “trying to bring it about that one x-s” (Mele 2009: 18) and actions which are done in order to bring it about that one x’s. Frankfort suggests that deliberating is an action in the first sense, insofar as it involves (for example) trying to make a decision, and inner speech episodes are actions in the second sense, insofar as they are produced in order to bring it about that one deliberates successfully and (for example) comes to a decision.

Jorba (forthcoming) also holds that inner speech episodes are typically actions, applying affordance theory. Affordances are opportunities for actions suggested by things in one’s environment. For example, an apple has the affordance of being edible; a cup has the affordance of being graspable. Some hold that affordances can also be things which suggest mental actions (Jorba cites McClelland 2020 and Jorba 2020). Jorba’s suggestion is that some mental states afford the production of inner speech episodes. For example, an inchoate thought affords being articulated clearly in inner speech, and an emotion can afford being labeled. Insofar as inner speech episodes are produced in response to affordances, they are actions and, specifically, speech acts. See Bar-On & Ochs (2018) for another account on which inner speech episodes can be “acts of innerly speaking our mind” (2018: 19, emphasis removed).

4.2 Inner Speech and Conversation
Closely related to the question of whether there can be speech acts in inner speech is the question of whether inner speech can involve a kind of dialogue or conversation. A theory which characterizes inner speech this way has been developed at length in psychology, primarily by Charles Fernyhough (e.g., 1996, 2008, 2009). However, the suggestion has been made in a variety of ways by philosophers as well, including by Machery (2018), Frankish (2018), Gauker (2018), and Wilkinson, in collaboration with Fernyhough (Wilkinson & Fernyhough 2018).

The idea that inner speech involves an internal dialogue or conversation clearly has intuitive appeal for some. One often finds inner speech described outside the philosophical context as the “inner dialogue”. But, if inner speech involves a kind of internal dialogue or conversation on more than a metaphorical level, then it is natural to wonder who the interlocutors are (Gregory 2020a). Machery (2018) and Frankish (2018) suggest that different parts of the brain communicate with one another via inner speech. Gauker (2018) suggests that inner speech involves conversing with oneself (see also the discussion of inner speech as a means for interaction between subsystems or modules in the mind in Section 2.2). One difficulty with both of these suggestions, however, is that philosophers of language generally (though not universally) think of conversation as fundamentally involving distinct human agents.

Gregory (2017) appeals to Grice’s (1975 [2013]) account of conversation to make this point. Grice argued that conversations are “characteristically … cooperative efforts” (p. 314). But cooperation requires multiple agents and there is only one agent in inner speech. That said, Gauker (2011, 2018) is working with an explicitly non-Gricean picture of conversation, motivated by an opposition to the doctrine that speech acts serve to express thoughts that are distinct from and precede the expressive utterance. He holds that speaking is,

in the first instance, something we do whenever there is no reason not to, because of the good it tends to do. (2018: 71)

In certain circumstances, where multiple individuals are present,

[a] conversation can be the occasion for each interlocutor to reflect on what he or she has experienced, … and on that basis to elicit a statement that is useful from the other. (2018: 72)

Insofar as we can generate inner speech episodes which cause us to reflect on some matter and then produce further inner speech episodes which are useful for us in the context, inner speech will be conversational. Gauker’s analysis here obviously reflects the expression-oriented approach to the question of whether there can be speech acts in inner speech.

In contrast to Gauker, Deamer (2021) argues that inner speech can be seen as being communicative in a Gricean sense. She holds that, to at least some extent, humans are “self-blind”: mental states such as our intentions are not always transparent to us. When we produce inner speech, we reveal our communicative intentions to ourselves, just as we reveal our communicative intentions to others when we converse with them.

While there is disagreement as to whether a series of inner speech episodes can be a dialogue in a literal sense, most agree that inner speech often closely resembles dialogue. As Gauker notes, one episode of inner speech will often prompt another, as happens in interpersonal dialogue. We can produce episodes of inner speech corresponding to different points of view, e.g., when thinking about the considerations for and against some course of action, in a way similar to two people with different opinions. Some participants in studies report that some of their inner speech episodes take place in the voices of others (McCarthy-Jones & Fernyhough 2011; Alderson-Day, Mitrenga, et al. 2018). This last consideration raises an important issue. We can certainly imagine conversing with others and we can certainly imagine others conversing. Such cases are usually taken to be distinct from inner speech (see Section 1). However, if inner speech can involve the voices of others, possibly expressing viewpoints other than our own, it becomes difficult to say how instances of inner speech with these characteristics differ from cases of imagining others speaking. How to delineate the extension of “inner speech” in a way that distinguishes inner speech acts from cases of (merely) imagining speech remains an underexplored issue.

5. Self-Knowledge and Metacognition
Inner speech plays an important role in a number of philosophical accounts of self-knowledge and metacognition. By “self-knowledge” we will mean knowledge of one’s own mental states, including both dispositional states—like beliefs, desires, and intentions—and occurrent states, such as thoughts, imaginings, decisions, and judgments. The notion of metacognition is somewhat broader, also encompassing judgments and non-cognitive assessments (e.g., “feelings of knowing”) concerning the validity of one’s own reasoning, the quality of one’s evidence, one’s degree of certainty, and so on (Proust 2013). While some theorists implicate inner speech in their accounts of both self-knowledge and metacognition (Jackendoff 1996; Clark 1998; Bermúdez 2003, 2018), others focus more narrowly on the question of how inner speech might facilitate self-knowledge (Byrne 2018; Carruthers 2011; Roessler 2016). A common thread among theorists who invoke inner speech in their accounts of metacognition or self-knowledge is the idea that certain others of our mental states—namely, those that our inner speech helps us to know about—are either less readily available to introspection or less well suited to serve a metacognitive role. Thus, these views all appear against a backdrop of broader commitments about the nature of mental states and our introspective access to them.

5.1 Metacognitive Approaches
One approach sees inner speech as especially well suited to aid in metacognition due to its linguistic structure, or its link to public language more generally. According to Andy Clark (1998), the fact that inner speech occurs in a language—where such language is seen as abstracting away from the particularities of perception—allows it to play a special role in “second-order cognitive dynamics” (see also Prinz 2011, 2012). This, he holds, is because the natural language sentences featured in inner speech are “context resistant” and “modality transcending” in ways that facilitate a more objective and reliable assessment of the soundness of one’s own thought processes (Clark 1998: 178). Bermúdez (2003, 2018) builds on Clark’s proposal, specifying that awareness of inner speech is essential for enabling humans to become conscious of their own propositional thought processes, which are otherwise amodal and inaccessible to introspection. According to Bermúdez,

all the propositional thoughts that we consciously introspect … take the form of sentences in a public language. (emphasis original, 2003: 159–160)

While he does not identify these public language sentences with our core thought processes themselves—these, he holds, occur in a subconscious language of thought—Bermúdez argues that the linguistic structure of inner speech is needed to adequately represent the relationships of entailment and rational support that may (or may not) exist among the subconscious thoughts the inner speech episodes serve to express. As he puts it,

we think about thoughts through thinking about the sentences through which those thoughts might be expressed. (2003: 164)

Jackendoff (1996, 2007, 2011, 2012) and Prinz (2011, 2012) likewise hold that there is a level of conceptual thought that is not directly available to introspection and that inner speech is well suited for making us aware of such thoughts. Yet, for Jackendoff and Prinz, inner speech is able to play this role primarily because, like other imagistic mental states, inner speech occurs at an “intermediate” level of representation, which, on their theories, is the only level of representation at which mental states are consciously available to the subject. Thus Jackendoff’s comment that “we are aware of our thinking because we hear the associated sounds in our heads” (2011: 613). Echoing Bermúdez and Clark, Prinz finds it

likely that we often come to know what we are thinking by hearing inner statements of the sentences that we would use to express our thoughts (2011:. 186)

and judges inner speech to be “a way of registering complex thoughts in consciousness” (2011: 186). (See also Machery 2005, 2018.)

5.2 Inferentialist Approaches
Several theorists, who we will term “inferentialists”, follow Ryle (1949 [2009]) in his claim that we often come to know what we are thinking by “overhear[ing]”, or “eavesdrop[ping] on … our own silent monologues” (1949 [2009: 165]). On these views, we come to know what we are thinking, or what we believe or desire, by drawing a kind of inference (the nature of which differs, depending on the theorist) from the fact that we “hear” ourselves say something in inner speech. The views of Clark, Bermúdez, Jackendoff, and Prinz, already reviewed, are inferentialist in nature. Yet there are other approaches that incorporate inner speech into a process that is even more explicitly inferential.

Carruthers (2009, 2010, 2011, 2018) is an inferentialist of this latter sort. While, in earlier work, he argued that thought itself occurs in inner speech (Carruthers 2002), Carruthers later abandoned that idea to hold that thoughts (including one’s beliefs) are always unconscious. On this view, inner speech episodes remain more or less directly available to introspection, yet only provide a kind of indirect evidence for what we are in fact judging or deciding (or believing, desiring, or intending) unconsciously. He emphasizes the fallible nature of such inferences, arguing on the basis of various empirical studies that many of the inferences people arrive at about their own beliefs and desires are in fact incorrect. Similarly to Jackendoff and Prinz, Carruthers holds that only sensory states are able to serve as inputs to the mental mechanism responsible for self- (and other-) directed mindreading. These inputs include visual and other forms of sensory imagery in addition to inner speech. However, in cases where we are having thoughts about abstract matters that are difficult to unambiguously represent with other forms of imagery—such as the thought that philosophy is a challenging subject—episodes of inner speech are held to provide an especially important source of information that one is having such thoughts. Carruthers emphasizes that the process becomes especially inferential in nature where other contextual information—such as that one sees oneself lingering over a choice of cereal box—combines with one’s inner speech to generate an all-things-considered appraisal of what one is currently judging or deciding. Cassam (2011, 2014) likewise implicates inner speech in a multi-faceted inferentialist account of self-knowledge, though not pitched in terms of “mindreading” mechanisms or other constructs from cognitive science.

Alex Byrne (2011, 2018) puts inner speech to somewhat different ends in his inferentialist account of how we know what we are thinking. For Byrne, there is no such thing as inner speech, strictly speaking, because there are no sounds (or voices) in the head. However, there are such things as auditory-phonological representations of voices. These give rise to an apparent perception of what we come to think of as the “inner voice”. By trying to attend to what the inner voice says, Byrne proposes, we can reliably form judgments about what we are thinking. The epistemic rule he proposes for doing so is:

THINK: If the inner voice speaks about x, believe that you are thinking about x.

As with Carruthers, a key motivation for Byrne’s account of how we know what we are thinking is a background view—motivated by the work of Shoemaker (1994), Dretske (2003), and others—that we have no other, more direct introspective method for knowing our own thoughts (i.e., we lack something like an “inner sense”). Note that Byrne’s approach is inferentialist in that he takes inner speech to be implicated in inferences that lead to knowledge of one’s own occurrent thoughts. Yet the sort of inference involved is quite different from that envisioned by Carruthers and Cassam, who both hold that inner speech episodes are just one kind of information among many that may be brought to bear in inferences about one’s standing and occurrent mental states. Importantly, the form of inference envisioned by Carruthers and Cassam is essentially the same in its first and third person applications, whereas Byrne’s THINK rule is of an inferential procedure that can only be used to reliably generate true beliefs about one’s own mental states. In Byrne’s view, this helps to explain the “peculiar” nature of introspection, where this peculiarity lies in the fact that our methods for knowing our own mental states are (intuitively) different from those we use to know others’ (Byrne 2011, 2018). Further, on Byrne’s version of inferentialism, the inferences we form by trying to follow THINK are extremely likely to amount to knowledge—thereby cohering with the intuition that knowledge of one’s own current thoughts is epistemically privileged. Whereas, the kinds of metacognitive inferences that Carruthers and Cassam envision to rely on inner speech are (by their own telling) epistemically on a par with our inferences about the mental states of others and far more susceptible to error.

5.3 Inferentialism’s Critics
Several philosophers object that inferentialist proposals leave us at too great an epistemic distance from our own thoughts (Bar-On & Ochs 2018; Roessler 2016) or have other unworkable features (Langland-Hassan 2014; Martínez-Manrique & Vicente 2010; Roessler 2016). Roessler (2016) pursues a non-observational account of the role of inner speech in generating self-knowledge. Rejecting the idea that we need to “eavesdrop on ourselves” by attending to our inner speech, Roessler suggests we follow remarks of Ryle (1949 [2009]) and Anscombe (1957) in understanding the knowledge gained through inner speech as a kind of “practical knowledge”, (or, for Ryle, “serial knowledge”), where knowing what one is thinking is understood as a special case of knowing what one is doing.

Bar-On & Ochs (2018) likewise take aim at what they term “Neo-Rylean” invocations of inner speech, arguing that Byrne’s THINK rule fails to identify a special role for inner speech in facilitating self-knowledge. Drawing on Bar-On’s (2004) broader expressivist approach to self-knowledge, Bar-On and Ochs hold that a proper account of inner speech’s role in self-knowledge should show how such knowledge is “distinctive and uniquely first-personal” in that it is

knowledge that one can be said to have in virtue of being in a privileged position to give direct voice to one’s thoughts. (2018: 20)

They do not, however, develop a positive account in detail.

Vicente & Martínez-Manrique (2005, 2008; Martínez-Manrique & Vicente 2010) have criticized Bermúdez’s and related inferentialist views on the grounds that the semantics of natural language sentences—and inner speech episodes, in particular—are underdetermined in ways incompatible with providing knowledge of one’s thoughts. For instance, the sentence “Jane’s cup is full”, is ambiguous in several ways, including the sense in which it is Jane’s cup (does she own it? is she just using it? is it the one she merely wants?) and the sense in which it is full (is it full of air? of liquid? of coins?). If the explicit meaning of a sentence is only extracted (and disambiguated) at the level of thought itself, they argue, it is unclear how awareness of semantically indeterminate inner speech utterances could suffice for awareness of one’s own—presumably explicit and unambiguous—propositional thoughts. Bermúdez replies in his 2018 paper.

Jorba & Vicente (2014) and Martínez-Manrique & Vicente (2015) criticize what they call the “format view” of inner speech (which they attribute to Jackendoff and others) which holds that we are conscious of our inner speech episodes only because of their sensory format (see also Fernández Castro 2016). If these criticisms succeed, they cast doubt on views, such as those of Carruthers (2010), Jackendoff (1996), and Prinz (2012), which link the metacognitive or introspective value of inner speech to its occurrence in a sensory format.

Langland-Hassan (2014) raises a different sort of challenge for inferentialist views. Recall that it is a common assumption of those views that propositional thought itself is amodal (i.e., non-sensory) and non-conscious. For theorists such as Carruthers, Prinz, Jackendoff, and Bermúdez, inner speech is a conscious mental process just because it has sensory features that render it the sort of state that is apt to be conscious. Langland-Hassan argues that there is a conflict in holding that an episode of inner speech is a single mental state with both sensory features (relating to the representation of phonemes) and semantic features (relating to the meanings of the corresponding words). If this criticism is correct, it creates problems for the proposal that inner speech is especially well suited (due to its sensory character) to serve as input to inferences about one’s non-conscious mental states. Bermúdez (2018), Carruthers (2018), and Munroe (2023) have articulated different ways of responding to this challenge (see also Prinz 2011 for relevant remarks).

6. Auditory Verbal Hallucinations and Inserted Thoughts
Inner speech features prominently in philosophical and cognitive scientific discussions of auditory verbal hallucinations (AVHs) and thought insertion. Both are common symptoms of schizophrenia but can occur in other contexts (e.g., brain injury, drug use) as well. AVHs are hallucinatory experiences of another’s speech, while thought insertion is understood either as a non-veridical experience of having someone else’s thoughts in one’s mind (Wing et al. 1990), or simply as the delusional belief that someone else’s thoughts are in one’s mind (Andreasen 1984). Two central questions explored by theorists are, first, whether (abnormal) inner speech is indeed the basis of AVHs or thought insertion, and, second, what might lead an episode of inner speech to be experienced as an AVH or inserted thought.

On the first question, an initially plausible approach to AVHs is to hold that they are more a matter of hallucinatory speech perception than of (unwitting) speech production, and thus not well conceived as episodes of inner speech. Wu (2012) and Cho & Wu (2013, 2014) advance a theory of this kind, holding that AVHs result from the spontaneous activation of speech perception areas in the brain. On their account, inner speech—and, in particular, the neural regions implicated in speech production—are not implicated in AVHs. Despite the attractive simplicity of this account, most researchers have pursued options that explicitly involve inner speech, for several reasons. First, in formal surveys, patients often report that the phenomenological characteristics of their AVHs are different from those of hearing speech, insofar as their AVHs are not as subjectively “loud” as cases of hearing speech, are not equally rich in sensory features, and do not always seem to emanate from outside the head (Stephens & Graham 2000; Hoffman et al. 2008; Laroi et al. 2012; Nayani & David 1996; Stephane 2019). It appears that an explanation of the seemingly “alien” nature of these episodes, as well as of thought insertion, will require some other apparatus than an appeal to perception-like phenomenology. Given the need for such an alternative, one may hope to extend it also to cases of AVHs that are reported as having rich, perception-like phenomenological features (Langland-Hassan 2008; Moseley & Wilkinson 2014).

Second, neuroimaging has shown activation in both language perception and language production areas when patients are experiencing AVHs (Allen, Aleman, & Mcguire 2007; Allen, Modinos, et al. 2012; Bohlken, Hugdahl, & Sommer 2017). Here as in other areas of the study of inner speech, it is important to recognize that the neural regions underlying speech production (such as Broca’s area, in the left inferior frontal gyrus) are distinct from those governing speech perception (such as Wernicke’s area, in the superior temporal gyrus). This is why damage to one area but not the other (as in some cases of stroke) can result in markedly different language impairments. The fact that the mechanisms governing speech production and perception are dissociable in these ways provides an important means for assessing whether AVHs are best viewed as productive or perceptual (or both) in nature.

Nevertheless, those who see abnormal inner speech episodes as the basis for AVHs or thought insertion face a difficult task in explaining what would lead a person to not identify their own inner speech as their own, or to not feel in control of their own inner speech. Some have offered content-based explanations, where it is some feature of the semantic content of the inner speech that leads a person to disown it. For instance, Stephens and Graham (2000) argue that a patient may disown inner speech episodes with contents that are “intentionally inexplicable”, in the sense that they are not easily accommodated within a coherent self-narrative (see also Roessler (2013), Sollberger (2014), Bortolotti & Broome (2009), and Fernández (2010) on the idea that AVHs or inserted thoughts are episodes with contents the patient is unwilling to endorse). Challenges for this approach are patient reports of voices that are helpful or encouraging. As the Swiss psychiatrist Eugen Bleuler notes in early work on people with schizophrenia, “besides their persecutors, the patients often hear the voice of some protector”, and, occasionally, the hallucinatory voices “represent sound criticism of the [patient’s] delusional thoughts and pathological drives” (1911 [1950: 98]).

A popular alternative approach—sometimes known as the “comparator” or “sensory feedback” approach—builds on work in cognitive neuroscience concerning the mechanisms by which bodily movements are determined to be one’s own (Feinberg 1978; Frith 1992; Miall et al. 1993; Wolpert, Miall & Kawato 1998). The basic idea behind these approaches is that, below the level of consciousness, the brain is continually generating predictions about the likely sensory consequences of planned actions, which are then compared with actual sensory feedback. When there is a mismatch between the prediction and sensory feedback, one may have the phenomenological sense of not being in control of one’s actions (Frith 2012). A number of authors have proposed that the generation of both inner and outer speech may be attended by the same kind of prediction and comparison mechanisms, and that the malfunctioning of these mechanisms could lead to one’s own inner speech seeming not to be in one’s control (Blakemore, Smith, et al. 2000; Campbell 1999; Langland-Hassan 2016; Proust 2006). These proposals derive some support from the fact that people with schizophrenia have been shown to have broader deficits in automatically anticipating and adjusting for the sensory consequences of their own actions (Blakemore, Smith et al. 2000; Blakemore, Wolpert, & Frith 1998).

Nevertheless, the comparator approach to AVHs and thought insertion has come in for criticism on several grounds (Synofzik, Vosgerau & Newen 2008; Vicente 2014; Vosgerau & Newen 2007). One complaint has been that the lack of sensory features associated with inserted thoughts, in particular, makes sensory-feedback approaches ill-suited to their explanation (Vosgerau & Newen 2007). In response, some defenders have shifted to pitching the thesis in terms of predictive processing models of perception and action (Gerrans 2015; Swiney 2018; Swiney & Sousa 2014; Wilkinson 2014; Wilkinson & Fernyhough 2017), while others have developed other alternatives (Langland-Hassan forthcoming). The matter of how best to characterize the phenomenology and underlying etiology of AVHs and thought insertion—and the relation of each to inner speech—together with the precise relationship between predictive processing models and the comparator approach, remain active areas of research. See Wilkinson & Alderson-Day (2016) for an introduction to an edited special-issue on the topic oriented at philosophers; see López-Silva & McClelland (forthcoming) for a philosophically-oriented anthology on thought insertion. (Note: Parts of this section draw on a more in-depth overview in Langland-Hassan 2021).

1. Varieties of Transformative Experience
Transformative experiences permeate our everyday lives—at least if we’re to believe the growing literature that has sprung up around this topic. To fully appreciate its significance, this section offers a brief, non-exhaustive sampling of various purported cases of transformative experience. It’s up to the reader to determine whether these examples qualify as transformative in the sense of the strict definition offered above. But perhaps more important than whether each of these qualifies, is the fact that considering whether they do raises important issues that help us better understand the myriad dimensions of our own lives.

1.1 Parenthood
Parenthood is Paul’s paradigm case of transformation (2014, 2015a). Prior to having a child, one cannot anticipate what it will be like to become a parent. Further, having a child changes the parent in a personally transformative way. Core preferences and life goals are often reshaped around a new priority: the child. The way the new parent sees the world and perceives terror and joy shifts. Zadie Smith (2013 [2018]) eloquently puts the previously unknown complexity of having a child thusly:

Occasionally the child, too, is a pleasure, though mostly she is a joy, which means in fact she gives us not much pleasure at all but rather that strange admixture of terror, pain, and delight that I have come to recognize as joy and now must find some way to live with daily. This is a new problem. (Smith 2013 [2018, 331])

And of course, the transformation associated with parenthood is not restricted to one’s first biological child. It extends to adopted children, and even can extend beyond humans to feline, canine, and other beings who depend on us and with whom we can bond.

1.2 Sense Modality and Sensory Experiences
Children who grow up in the United States rather than Australia generally don’t experience the distinctive taste of vegemite. People born with color blindness don’t see certain colors. Tasting vegemite and seeing color for the first time are radically new sensory experiences with distinctive phenomenal characters, and probably only involve epistemic transformation. Some people experience even more radical changes when they gain or lose a sense modality. Someone who has always been sighted or hearing cannot appreciate what it is like to be blind or deaf. Likewise, people who have always been blind or deaf cannot appreciate what it is like to be sighted or hearing. In addition to not knowing what gaining or losing a sense modality might be like, one cannot anticipate how that gain or loss might personally change them and the way they navigate the world. The fact that most of society is structured to accommodate sighted and hearing people increases the difference between a change in sense modality. And of course, appreciating the transformative nature of gaining a sense modality carries practical implications since many people are in a position to decide for themselves or others whether to gain a sense modality via surgical interventions.

1.3 Love and Relationships
Love has the power to transform us little by little. In fact, this incremental change is one of the fascinating features of love when put in the context of transformative experience (see Paul 2014). Smiling at a new acquaintance is not transformative, nor is grabbing coffee with them. Once you’ve had coffee, eating dinner together is not transformative either. On and on each step goes. Yet, by the end of things, when you look back at a relationship that’s decades long, you realize that you could not have imagined what it would be like to bond with another person in such an intimate way. The way you approached life, the preferences and life projects you developed, and the way love’s rose-tinted glasses filtered the way you see the world have radically changed you. On the flip side, ending this sort of relationship is also wildly transformative, especially if you’ve formed a sense of identity that revolves around your beloved. Furthermore, because it’s not clear that reason ever requires that you love someone in a romantic way, it’s difficult to explain how choosing love or rejecting it could be done rationally. Thus, opening oneself up to love or boldly ending it invites transformation and raises questions about whether doing so could ever be defended rationally.

1.4 Social Identity
All of us inhabit a place in society that’s determined in part by race, gender, sexual orientation, ability, religion, country of origin, education, socioeconomic status, political affiliation, and much more. Due to contingent but very real injustices, our place in society shapes our experiences. Sometimes, it’s possible to change our place in society (McKinnon 2015 discusses gender transitions and Barnes 2015 offers the fictional case of Pip from Great Expectations), and when we do so, that change is transformative. But the way in which these social changes are transformative underlines some troubling features these social divides have created. First, just as, for instance, Pip, cannot explain to his past self what social elevation is like, it is generally extremely difficult for people in one social group to communicate what it is like to be a member of that group to someone who is outside of it. This partially explains the phenomenon of situated knowledge and also gives rise to the possibility of testimonial epistemic injustice (see, e.g., Fricker 2007). Hermeneutical issues also arise. People’s self-conceptions aren’t developed in a vacuum, and shared norms and concepts contribute to the way in which one’s self-conception can develop, thus influencing the ways in which one can transform. For instance, “brave inspiration” is a readily available self-conception for disabled people while “thriving person in an unconventional body” is not—and this is an unjust state of affairs (Barnes 2015). Social identities and structures contribute an ethically rich dimension to the conversation on transformative experience.

1.5 Tragedy
Unfortunately, life also contains tragedies. The trauma of war leaves veterans and civilians physically and emotionally scarred in ways that can change their outlook on life forever. We aren’t morally perfect, and often fail in extremely regrettable ways. To top it all off, people die, sometimes after extended periods of pain and suffering. Experiences of negative valence are often epistemically surprising. There’s no way to predict what the death of a close loved one will feel like, even if the death is anticipated. Life’s tragedies also have the capacity to change who we are. Sometimes, this change can happen for the better. Cashman and Cushman (2020) suggest that although we don’t want to intentionally morally fail, moral failure can often teach incredibly valuable lessons and change how we act for the better. When it comes to death, dying creates a chasm of understanding between those who are dying and those who are not. Chung (2017 [Other Internet Resources]), a philosopher who wrote about his impending death, tells of the chasm that death opens up between those who are dying and those who are not. Truly acknowledging that one is dying changes your priorities and the way you experience the world in a way that can’t be explained to someone who isn’t dying. The same holds for someone forced to witness their beloved slowly fade away. Thompson (2020) offers an insightful meta-point, arguing that dying is an “ultimate” transformation because it forces us to adopt a perspective of our life as a whole that allows us to reconsider which of our prior experiences truly were transformative.

1.6 Ideology
Our beliefs shape who we are, and changes to our core beliefs have the potential to transform us. For instance, De Cruz (2018) and Chan (2016, 2019) argue that religious conversions are transformative. The mission statements of universities appear to acknowledge and strive for transformation among students (Paul & Quiggin 2020). Schwenkler (2020) and Stump (2020) both investigate the nuances of opening oneself up to ideological conversion in light of the potential for doxastic and philosophical transformation, respectively. Crucially, when ideological difference are too great, people lose the ability to adopt the perspective of others. This applies both intrapersonally, such as when we look back at our past selves and marvel at how we believed in Santa Claus, and interpersonally, which leads to the sort of breakdown we see in hyper-polarized political contexts. Speaking of politics, holding office or participating in politics in some deep way might also be transformative, both because they involve encountering people with different perspectives and also because they require representing them in a meaningful way (see Allen 2017). It’s tempting to think that changes in belief are merely epistemically transformative since they clearly change our epistemic states. But in the New Testament (Acts 9), Saul’s conversion on the road to Damascus into the Apostle Paul isn’t just a change in beliefs, it’s a deeper change that completely alters the way he lives his life. The scales falling off his eyes does not merely signify seeing the truth; it also represents a change to who he is.

1.7 Art and Fiction
Paul (2014) opens Transformative Experience with thought experiment about transforming into glamorous, slightly murderous, creatures of the night: vampires. Obviously, none of us actually face this choice. However, rhetorically, thinking about turning into a fictional creature helps motivate the idea that doing so would be both epistemically and personally transformative. After all, who can say for sure what becoming a vampire would really be like or how it would change a person? Furthermore, this case reveals something further about the connection between fiction, art, and transformative experience. It could be that art is transformative—or at least quasi-transformative—because it helps us understand things that we have not (or cannot) experience (Aumann 2022). Furthermore, art and fiction might be transformative from the perspective of the creator as well as the audience. For instance, the expression of oneself through art may be a transformative experience (Riggle 2020). Finally, art clearly involves our imaginations, and imagination may provide a way to gain knowledge about what a transformative is like prior to experiencing it. (More on this in §2.)

2. Epistemology: Can We Know What It’s Like?
Epistemically transformative experiences are experiences where one cannot know what they are like before having them. In the paradigm case of parenthood, the experience of becoming a parent for the first time has “an epistemically unique phenomenal character” (Paul 2015a, 157). One cannot know what these are experiences are like prior to having them.

Prior work, such as Jackson (1986), provides support for this epistemic claim. Jackson’s “What Mary Didn’t Know” invites us to engage in a thought experiment about Mary, a person who has lived in a black and white room her entire life. Prior to encountering color, can Mary know what it is like to see the color red? Does it matter whether Mary is a color scientist who has studied the physics of color and read extensive testimonies about what the color red is like? Jackson’s objective—to show that physicalism is false because it cannot give us knowledge of qualia—applies to transformative experience as well. In general, experiential, or “what it’s like”, knowledge cannot be obtained prior to experience. Thus, what transformative experiences are like cannot be known without having the relevant experience.

Many philosophers reject the claim that one cannot know what transformative experiences are like prior to having them. These rejections tend to center upon one of two strategies. First, one might turn to paradigm cases such parenthood and offer reasons to believe that one can in fact know quite a lot about what it’s like to be a parent prior to becoming one. For instance, Harman (2015) argues that caring for a significantly younger sibling in a quasi-paternal way or witnessing a close friend or family member become a parent provides at least some evidence regarding what becoming a parent is like. Krishnamurthy (2015, 180) offers a similar suggestion, arguing that “we can on the basis of experiences of similar types know what it is like to have a child”. For instance, babysitting or raising a cat shares common elements with parenting. Knowing what these elements are like allows one to know something about what being a parent is like.

Sharadin (2015) offers a variation of this type of strategy that focuses on non-phenomenal elements of parenthood. For instance, becoming a parent typically involves things like financial and sleep deprivation, and we can know that becoming a parent typically involves these things. Plausibly, there is a supervenience relation between these non-phenomenal elements and an agent’s phenomenal experience of them. Further, many people, such as philosophy graduate students, are intimately familiar with the phenomenal experience of financial and sleep deprivation. By building upon the non-phenomenal elements that parenthood shares with other experiences a person has had, that person can develop an idea of what parenthood is like prior to actually experiencing parenthood.

Some empirical research work focusing on how people model uncertainty suggests that people do in fact attempt to use this first strategy of building on what one knows to figure out what one does not know. Zimmerman and Ullman (2020, 73) model epistemic uncertainty and purportedly demonstrate that “people can make informed decisions about trying unfamiliar things”. For instance, suppose that someone encounters an unfamiliar yellow grape. Though they wouldn’t know what the yellow grape is like, they might appeal to their knowledge of what green and red grapes are like to form a judgment about what the yellow grape might be like. At the very least, it seems like they are justified in believing that the yellow grape will not wildly diverge from their other grape experiences. Based on this information, they can rationally choose to try the yellow grape. Even if they are completely unfamiliar with grapes, they might use their knowledge of other fruits to form an informed decision. In general, novel experiences fall into a taxonomy of the types of experiences one might have, and agents can use the next closest experiences to project what the novel one might be like.

Even if this strategy is successful, it’s unclear that it shows that we can know what transformative experiences are like prior to having them. Recall that transformative experiences are supposed to have “unique phenomenal character” (Paul 2015a, 157). If one knows enough about what the experience is like, then it’s not in fact transformative. For instance, Paul (2015b) responds to Harman’s case by suggesting that through having similar enough experiences, Harman might in fact have known what becoming a parent is like. Thus, becoming a parent for the first time would not have been transformative because the relevant transformative experience already occurred earlier. Whether a type of experience is transformative may be agent-relative. Furthermore, while this strategy is promising, some cases may still elude it. For instance, when it comes to gaining or losing a sense modality, it really does seem that the pre-transformed person cannot have any relevantly similar experiences.

The second strategy does not deny that we often don’t know what transformative experiences are like. Rather, it locates our experiential ignorance in a lack of imagination rather than an in principle difference in types of knowledge (i.e., propositional and experiential knowledge). Kind (2020), for instance, argues that the difference between projecting what transformative and non-transformative experiences amounts to a difference of degree rather than kind. Plausibly, it’s possible to imagine what it’s like to babysit for the first time. Further, imagining what it’s like to be a first-time babysitter might be easier than imagining what it’s like to become first-time parent. For someone with a sufficiently good imagination—and our imaginative abilities do also come in degrees—one can gradually scale up from the babysitting experience to the experience of becoming a parent. It may even be that we can augment our imaginations through aids such as art and thus approximate what foreign experiences are like (see Aumann 2022). The effectiveness of artistic aids as well as the understanding gained from them also plausibly admit of degrees. Thus, the imagination strategy can explain why it’s easier to figure out what some experiences are like than others, and why this can vary from individual to individual. Since paradigm transformative experiences tend to be among the most difficult to imagine, positing distinct “what it’s like” knowledge is tempting. However, the difficulty in imagining what paradigm transformative experiences are like merely generates the appearance of a distinct kind of knowledge. The upshot is that in principle, we can know what a transformative experience is like prior to experiencing it.

Arpaly (2020) offers a wrinkle on the imagination strategy, blaming our “crappy” imaginations for our inability to project what transformative experiences are like. Sometimes, what an experience is like is opaque because the “devilish details” of the experience simply haven’t occurred to us. For example, most people have not considered that if a giraffe were to drink coffee, the coffee would cold by the time it reaches the giraffe’s stomach. Other times, we overestimate our ability to imagine things, and our overconfidence prevents us from accepting testimony or other evidence that would be illuminating. These cases can be tragic, such as when a depressed person’s family doesn’t believe their reports of depression. The failure to correctly imagine what things would be like thus causes it to appear as if our lack of experience is the problem, since experience would quickly correct this misimpression. Crucially, there is nothing special about the “what it’s like” quality of these experiences. Though we are ignorant, we are not in principle ignorant and could theoretically figure out what the transformative experience would be like.

Like the first strategy, this second one is most vulnerable when it comes to extreme transformations in experience such as gaining or losing a sense modality. One might also wonder whether the second strategy significantly undermines the core epistemic claim: that one cannot know what transformative experiences are like prior to having them. Even if there is no in principle difference propositional and “what it’s like” knowledge, it does seem like imagination-based responses do acknowledge that some experiences elude our imaginative abilities.

Finally, Moss (2018) offers an interestingly different defense of the claim that we cannot know what a transformative experience is like prior to having it. Moss argues that we often have “probabilistic knowledge”. For instance, suppose that 50% of the people who experience childbirth report that the experience is miraculous. Based on that, it seems reasonable to conclude that there is a 50% chance that experiencing childbirth is miraculous. While an individual can’t know that the experience will be miraculous for them, it does seem that they can know that there’s a 50% chance that the experience will be miraculous for them. But crucially, before one knows that probabilistic claim, one must know that they aren’t exceptional. For instance, suppose it turns out that of the people who decline epidurals, only 10% of them report that the childbirth experience is miraculous. If one is going to decline an epidural, then they do not know that there’s a 50% chance the experience will be miraculous since they are part of a group—what Moss (2018, 173) calls “an alternative reference class”—for whom the probability does not apply. Furthermore, those who can’t rule out that they are in the alternative reference class (in this case, perhaps those who are unsure of whether they will have an epidural) also lack probabilistic knowledge. Plausibly in the case of parenthood and other transformative experiences, probabilistic claims can be formed based on testimony gathered from those who have had the experience. However, it could be that among those giving the testimony, there is significant variance that’s based on whether individuals belong to an alternative reference class. Given the nature of transformative experience, it may be unobvious whether a particular individual has a trait that places them into an alternative reference class. Thus, even if there’s ample data regarding how likely it is to have a particular parenting experience, that won’t result in probabilistic knowledge of what it’s like to be a parent. This defense also works even if it turns out that there isn’t a distinctive “what it’s like” knowledge and our imaginations can sometimes help us understand what foreign experiences are like.

3. Metaphysics: Can I Become Someone New?
Personally transformative experiences are ones that change who an agent is. Certainly, there are epistemic elements associated with personal transformation: agents may not know how an experience will personally transform them and they won’t know what it’s like to be transformed. There might also be de se knowledge, or self-locating knowledge of the kind that occurs when one knows who or where one is within a world (see Lewis 1979). Significantly for the context of transformative experience, de se knowledge can apply to a temporal part of a person, so it’s possible that someone who undergoes a transformative experience will “locate” themselves as being the person who results from the transformation, while their earlier self would have lacked this de se knowledge since they may not have taken the future, radically transformed person to be themselves. In any case, many of the considerations in the previous section apply to these epistemic elements. However, personal transformation also introduces interesting metaphysical issues, and this section focuses upon those.

Paul describes the situation as being one that “radically changes what it is like to be you, perhaps by replacing your core preferences with very different ones” (Paul 2015a, 156). Life goals, core preferences, or the way one experiences the world may constitute part of the change that occurs. Ullmann-Margalit (2006, 159) adds that because our beliefs and desires shape the core of who we are, changes to them turn us into a “different person”. There’s also a terrifying sense of finality since these personal transformations are “irrevocable” (Ullman-Margalit 2006, 160) or at least potentially irrevocable. This picture painted by Paul and Ullmann-Margalit is sometimes characterized as a “Replacement Model” (see Lambert & Schwenkler 2020b, § 5) because the agent contemplating undergoing transformation ends up “replaced” by the transformed person. There’s supposed to be a literal sense in which the pre-transformation agent ceases to exist.

Spelling out the sense in which the agent no longer exists is tricky. Focusing on core features is one natural way to explain the difference between the pre-transformed agent and future transformed person. Perhaps when enough core features are lost, the pre-transformed agent ceases to exist. (In essentialist language, perhaps some of these core features are essential, or such that the agent cannot survive the loss of them.) There’s an open question regarding which of our features count as core in this way, and some empirical studies suggest that we at least do regard some our features as being essential in this way. Molouki, Chen, Urminsky, and Bartels (2020) have done empirical work to see whether people exhibit semi-uniform views about which features are central to their selves. Their findings support earlier work by Strohminger and Nichols (2014) suggesting that features concerning morality (e.g., character traits) and personality are taken to be most central. Changes that are perceived as most drastic involve morality and personality, as well as features that are taken to be causally central. For instance, if someone takes “being a philosopher” to be causally central to how they navigate the world, then they tend to regard losing that feature as a drastic change. Valence and desires also matter: people take negative and undesired change to be more drastic. The explanation for asymmetric valence and desire perceptions rests upon the fact that people project their future selves to undergo positive development, so negative and undesired changes lie outside their self-conception. These finding cohere with many of the sorts of personal transformations that constitute personal transformations.

A second sort of claim that’s frequently made is that the pre-transformed agent cannot identify first-personally with the future transformed person, and vice versa. For instance, Velleman (1996) draws a distinction between imagining what Napoleon saw when looking out at Austerlitz and imagining oneself as Napoleon looking out at Austerlitz. The latter removes all first-personal traces of the imaginer (and is likely impossible to do). Paul (2017) makes a similar claim when distinguishing between affective and cognitive empathy. The former involves projecting our current selves into potential future scenarios, like if I imagine what it will be for me (a childless person) to have a child. It’s akin to the way we empathize with others. The latter is much richer, and it involves experiencing the potential future from the perspective of the future, rather than current self. To cognitively empathize with my future self who has a child, I would have to put myself into the first-personal experience of that person who now has the preferences and perspective of a parent. According to Paul, I lack the ability to cognitively empathize with my future parent self because that self is too unrelated to my current self. In general, we are not able to empathize with our distant future selves, even if those selves haven’t undergone a dramatic transformation. If Velleman and Paul are right, then the severing of the first-personal perspective and empathy that occurs after transformation is one way in which the pre-transformed agent ceases to exist.

Finally, it’s helpful to take a step back and consider the ways in which the future-transformed person is a different person. Suppose that losing central features and a disconnect in first-personal perspective are sufficient for replacing the pre-transformed agent with a new person. How do we explain the connection between the future and past person? On the replacement theory, transformative change results in a new person. Can we coherently say that the pre-transformed person has become the new person? Or does transformation annihilate and replace them?

The trick to explaining how one can become someone new without being annihilated involves articulating two different senses of “sameness”: one in which the past and future persons are the same and one in which they are not. For instance, Glazier (2020) notes that the type of explanation demanded mirrors a debate about whether contingentism is a coherent view. On contingentism, it’s possible that I (or anyone else) could be someone else. Philosophers who champion the necessity of identity tend to reject this claim as straightforwardly absurd. To explain how their account is coherent, the contingentist must identify two senses in which identity claims are necessary. Glazier’s suggestion rests on the claim that that no one’s perspective is impossible. Roughly, there’s a proprial, or perspectival, sense in which a claim like “I am Fred” is necessary. It’s necessary from “I’s” perspective that they are Fred because from their perspective the world is centered upon both the “I” and “Fred”. However, it’s possible that the world could have centered on someone else, say Georgia. In that case, there’s a non-proprial sense in which “I” should have been Georgia. This latter, non-proprial sense in which “I should have been Georgia” generates the contingency of identity.

Chan (2019) offers a different account on which the past and future persons might be the same metaphysically speaking, but different in terms of their practical identities. These practical identities are characterized by the traits that are changed in personal transformation: core preferences, life goals, ways of navigating the world, continuity of first-personal identity, and the like. Importantly, these traits form the person’s self, or at least the self that’s relevant for considerations of self-interest. This explains why ordinary decision-making procedures have difficulty when it comes to transformative experiences. (More on this in the next section.) Furthermore, distinguishing between metaphysical and practical senses of sameness explains the (practical) sense in which the person becomes someone new and the (metaphysical) sense in which they endure and are not annihilated.

4. Decision Theory: Is It Rational to Choose Transformative Experiences?
Transformative experiences present several challenges for traditional models of decision theory. First, the epistemically transformative component of the transformative experience involves a lack of “what it’s like” knowledge. Purportedly, this in turn involves a type of uncertainty that is different from the type of uncertainty decision theory is designed to handle. Second, the personally transformative component means that the post-transformation person may be extremely different from the pre-transformed person. Their preferences may radically change in unpredictable ways, and decision procedures must find a way to accommodate these changing preferences. Third, the personal transformation may be so extreme that the pre-transformed person considering whether to undergo the transformative experience can’t make the decision in the straightforwardly self-interested way that’s presupposed in standard decision-making models. Any of these challenges make using decision-making procedures impossible in the case of transformative experience. And if that’s the case, then it looks like one can’t rationally choose to undergo a transformative experience—the choice is either arational or irrational. Given that so many of our momentous choices in life involve potentially transformative outcomes, it would be disturbing if we discovered that these choices could not be made rationally.

4.1 “What It’s Like” Uncertainty
Standard decision models take probabilities and values as inputs and produce expected values as outputs. For instance, suppose ESPN’s outcome predictor projects the Kansas City Chiefs as having a 50% chance to win the Super Bowl, yet the moneyline, which is based on betting activity, is +105 (meaning that if you bet $100 on them and they win, you receive $105 in winning plus your initial $100 wager). By weighting the possible outcomes with their corresponding probabilities, we can calculate the expected value of betting $100 on the Chiefs:

.5
(
105
)
+
.5
(
−
100
)
=
2.5
If one uses a decision procedure where one maximizes expected value, it’s rational to bet on the Chiefs since betting $100 has an expected value of $2.50 while not betting has an expected value of 0 since there are no gains or losses. There are other decision procedures one might use as well. For instance, one might prefer a maximin procedure, in which one chooses the option with the best worst outcome. Since the worst outcome of betting on the Chiefs is losing money and the worst outcome of not betting is neither gaining nor losing money, not betting is the best worst outcome. Thus, a maximin procedure would recommend not betting. Crucially for our purposes, it doesn’t matter which decision procedure one utilizes. Rather, what matters is that values and probabilities (or at least possibilities) are required for any decision procedure to operate.

On the first challenge for decision theory, transformative experiences are epistemically transformative, and agents do not know what the experience will be like. What an experience is like determines the value associated with the experience. At the most basic level, part of what an experience is like involves the pain or pleasure of the experience. Pain detracts from the value of the experience while pleasure enhances it. More complex components of what the experience is like also affect the value of the experience. While some other factors—such as moral or social ones—may affect the value of the experience, what the experience itself is like clearly plays a large role in determining value. Without knowing what the experience is like, agents can’t reliably project its value. If the value of the experience is not known, then the value of any outcome involving transformative experiences also remains unknown. Thus, an agent choosing between options that may involve transformative experience cannot use standard decision procedures since those procedures require values as inputs.

This purported challenge is highly contested. Recall from §2 that many philosopher reject the claim that experiences like parenthood are epistemically transformative (e.g., Harman, Krishnamurthy, etc.). Denying that the experiences are epistemically transformative means that agents can know something about what the experiences are like prior to having them. For instance, the testimony of parents or the non-phenomenal elements of the experience might provide some sense of what becoming a parent is like. If that’s right, then perhaps at least rough values can be used in a decision procedure.

Alternatively, one might grant that we can’t know anything about what transformative experiences are like. Nevertheless, one might still be able to use standard decision procedures since we may know about the values associated with the experience even if we don’t know what the experience is like. For instance, Dougherty, Horowitz, and Sliwa (2015) invite us to consider a mystery box. Everyone who looks into the mystery box has a positive experience, though they cannot describe what the experience is like. On the basis of this information—that the value associated with the box is always positive—one can rationally choose to look in the box without knowing what that will be like.

Similarly, knowing the valence of outcomes can often provide rational grounds on which to make a decision even if what the outcome is like is unknown. Pascal’s Wager offers one such scenario: the infinite positive value of eternal happiness swamps any finite value. Thus, one is rationally justified—or perhaps even required!—to try to believe in God. McKinnon (2015) offers the opposite sort of case by inviting us to consider an individual considering transitioning. Prior to transitioning, many trans people suffer from severe depression and are at risk of suicide. Even if they do not know exactly what it will be like to transition, the alternative is sufficiently horrible, and thus the decision to transition can be rational.

Paul (2015b) suggests that all these responses miss the mark. First, people who choose transformative experience purely on the basis of values without knowing what the experience will be like are not choosing on the basis of what the experience is like. Of course, it can often be appropriate to make choices without considering what the outcomes will be like—cases involving moral stakes are one such scenario. However, in many paradigm cases such as parenthood, agents often do envision what their future lives will be like and base their decisions, at least partially, on that vision. Second, Paul emphasizes the importance of the values being subjective, or assigned by the agent. Because decisions involving transformative experience are personally significant, agents need to make the choice authentically. On Paul’s account, authenticity requires that the agents themselves ascribe value to the experience. (More on authenticity in §5.) Thus, testimony from others about the values they’ve ascribed to an experience won’t help an agent choose authentically for themselves.

4.2 Future Preference Uncertainty
Suppose that the approximate values of associated with outcomes can be known. The second challenge for standard decision theory procedures arises from the fact that transformative experiences have the potential to be personally transformative. Since personal transformation can involve extreme preference changes, the way outcomes are valued might not remain stable from before and after the transformation. For instance, a prospective parent might not value sleep, but once they become a parent confronted with a screaming infant that needs to be fed every few hours throughout the night, the way they value sleep may drastically shift. The values of outcomes used as inputs typically correspond to the preferences of the pre-transformed agent making the decision. If those preferences will shift unpredictably post-transformation, then knowing the values associated with the outcomes still may not be enough to allow an agent to make a rational decision.

In some sense, the challenge of changing preferences blends two familiar problems. First, preferences change over time. For instance, people with the means to save for retirement must balance their current preferences with what they believe will be their future ones. An extreme saver’s later self may look back and wish their earlier self had spent more on vacations. Transformative experiences add a further wrinkle by making the future preferences unknown. Second, changing preferences present a diachronic problem in which past, present, and future preferences must be balanced against each other. Presumably, this balancing involves aggregating all of these preferences, as well as deciding how to weigh them. For instance, many people are near-biased, and tend to weigh the preferences of their current and near-future selves far heavier than the preferences of their distant-future selves. The preferences of past selves rarely receive any attention at all. (Parfit [1984] has a series of thought experiments generating the intuition that it’s better for pain to be in the past, even if past pain is more severe than future pain. Sullivan [2018] and Boonin [2019] offer uncommonly held dissenting views: the former argues for temporal neutrality, or not engaging in any temporal discounting of the past or distant future, while the latter argues that the past preferences—even those belonging to people who are no longer alive—matter.)

Pettigrew (2019) offers a sophisticated account that has two components aimed at tackling both of these problems. The first, aimed at the uncertainty surrounding what the future preferences will be, bears similarity to responses to the challenge of assigning values given the “what it’s like” uncertainty. Basically, just as we can gather information about how people value transformative experiences, we can gather information about how people’s preferences change after undergoing transformative experiences. In an oversimplified example, if it turned out that 95% of new parents would prefer to be childless again, then given that one chooses to have a child, there would be a 95% chance that their future self would have the childless preference while there’d be a 5% chance that their future self would have a child-full preference. Then, just as decision procedures weigh the values of potential outcomes by their likelihood when we make decisions, they could include weighted preferences.

The second component, aimed at tackling the diachronic aspect of changing preferences, introduces a weighted general value function that aggregates the “local” value functions that correspond to the past, present, and future selves that make up the entire life of the agent. Further, the way in which the local value functions are aggregated can incorporate differences in the ways in which local selves handle probability assignments, decision procedures, and the weighing of, e.g., past versus future preferences. Aggregating the local value functions thus begins to resemble a social choice problem. Agents faced with choosing for changing selves ought to maximize the general value function, which aggregates the local value functions accounting for each temporally located local self.

Paul (2022) points out that Pettigrew’s account is vulnerable in two ways. First, the type of solution proposed by Pettigrew likens future selves to third parties. As noted, the weighted general value function closely resembles functions that aggregate individual preferences to group ones in social contexts. But it seems inappropriate to treat our future selves as third persons instead of fully integrated selves—and this is precisely the thrust of the problem that transformative experience presents! Second, it assumes that meaningful intrapersonal comparisons can be made. There’s some empirical work, especially regarding how people with disabilities judge their own welfare, that suggests these comparisons are difficult to make. Pettigrew (2019, § 8.6) attempts to address this issue with his “matching intervals solution”, which attempts to scale the different utility functions to each other. Briggs (2015) offers a prudence-inspired alternative that rests on privileging a person’s actual preferences (as opposed to their counterfactual preferences) without privileging their present preferences (as opposed to privileging present over future ones). Though success of any of these intrapersonal comparisons is an open question, there is widespread agreement that they are necessary to explain how future preferences can matter.

4.3 Self Uncertainty
Finally, the personally transformative aspect of transformative experiences creates the possibility of future selves who are so different from their pre-transformed selves that neither the earlier nor later selves regards the other as being “the same” person. As Ullmann-Margalit (2006, 158) puts it, “big” decisions are “core-affecting” and “transform one’s future self in a significant way” such that one emerges from the situation a different person. As we saw in §3, spelling out the exact sense of in which one can become a different person is tricky. However, examples illustrate how one might regard a temporally related self as a different person, and shed light on the exact nature of the problem for standard decision-making procedures.

Parfit (1984, 326) tells the story of a young Russian who holds his socialist ideals close to his heart. The young Russian also happens descend from the aristocracy, and stands to inherit a fortune when his relatives die. He realizes that when this happens, he may lose his socialist ideals. His instructions to his wife—“if I lose these ideals, I want you to think that I cease to exist”—reveals that he regards that potential future self as a different person. This type of example reinforces Paul’s (2017) claim that we cannot first-personally project into futures that are too distant or different. Agents who fully appreciate the personally transformative element of transformative experiences realize that these experiences may potentially transform them into someone different. Thus, from the perspective of the agent deciding whether to undergo a transformative experience, they must treat outcomes involving transformation as ones in which they may no longer exist.

The possibility of non-existence (or something regarded as equivalent to non-existence) means that even if an agent knows all the fact about value and future preferences, they cannot use standard decision-making procedures in a straightforward way—at least not if they’re self-interested. After all, even if a future transformed self would be incredibly happy and live a good, meaningful life, that doesn’t help the agent contemplating transformation if that future transformed self is not them in the relevant sense. Chan (2019) makes exactly this point in the context of Pascal’s Wager. The infinite happiness of the believer doesn’t benefit the non-believer precisely because becoming transformed by faith would change them into a different person (which in turn undermines the Wager’s appeal to self-interest). This point is closely related to the challenge raised by Paul for Pettigrew’s view in the previous subsection (§4.2). The future self is alien, and though there might be some decision procedure by which to make a rational decision to bring about that future self, that procedure is not the one related to the sense of self-interested that is under discussion.

A response to this challenge requires choosing in a way that bridges the gap between the pre-transformed agent and the future transformed self. Given the nature of transformative experience, it’s difficult to see how this could occur precisely since the pre-transformed agent lacks the ability to connect in the right way to the future transformed self. That connection goes beyond knowing the relevant values, probabilities, and value functions. It gets at the agent’s ability to choose authentically, to which §5 turns.

4.4 Reasonable, Not Rational
Finally, it’s worth considering the possibility that rational choice is simply not possible when it comes to transformative experience. Ullmann-Margalit and Morgenbesser (1977) discuss “picking,” which is characterized by indifference between alternatives, such as when one pick's between cans of the same flavor of Cambell's soup. By stipulation, picking can't appeal to values, preferences, or the sense of rationality discussed above—those things undertermine which alternative is optimal, which gives rise to the indifference in the first place. Importantly for Ullmann-Margalit and Morgenbesser, picking extends beyond trivial “Cambell's soup” choices to bigger ones, including transformative experiences. Even our values and preferences themselves (assuming we have control over them) may be subject to picking, since it's would be circular to appeal to them in order to explain why we hold them. Thus, many of our major life decisions, including potentially transformative ones, are subject to picking, rather than rational choice.

Continuing this line of thought, Ullmann-Margalit (2006) suggests that rather than focusing on acting rational in the sense of “optimizing” (as one does with standard decision-procedures), one ought to focus on “acting reasonably”. For instance, transformation involves both discontinuity in one’s life as well as a point of no return. A reasonable way of navigating choices that involve these two features involves attempting to minimize those features. For instance, a person contemplating a transformative decision like marriage might try to ease into it by first moving in with their potential spouse. Doing so minimizes the discontinuity and gives them an out, which makes the decision revocable. If Ullmann-Margalit is correct in suggesting that one might be able to build up to a transformative experience little by little, this raises an extremely interesting question for transformations that unfold gradually over time. Is it possible that each incremental step can be made rationally while a giant leap to transformation cannot be made rationally?

5. Existentialism: Choosing Authentically
Investigating decisions involving transformative experiences magnifies an existential problem as well. Recall from §4 that many of the strategies for making rational transformative decisions fail to also preserve authenticity. When transformation is on the table, who we are is at stake. And the choice of who we are is more fundamental than the other types of decisions we make. Who we are provides the framework from which we make other choices, so a choice that could change who we are is a choice between frameworks that itself lacks a framework from which to deliberate. As Ullmann-Margalit puts it:

At bottom, we make our most fundamental choices of the canons of morality, logic and rationality in total freedom and without appeal to reasons. They embody acts that this literature variously describes as nihilist, absurd, or leaps (of faith). (2006: 172)

On the flip side, what “this literature” does permit is authentic choice. For instance, deciding to transform without appealing to reason (perhaps because one cannot appeal to reason) coheres well to the Sartrean idea that persons do not antecedently have a particular telos. Rather, we are free to be our authentic selves precisely because are unconstrained and can choose who we want to be.

On this existentialist gloss, authenticity stands in tension with rationality when it comes to transformative experiences. On the one hand, we want to be able to rationally defend our big decisions to have a child, marry a particular person, or commit to a vocation. But on the other, these are the types of decisions that we also want to make authentically, and that doesn’t seem possible if we’re merely following what a decision procedure recommends. In fact, empirical research suggests that people do care deeply about authenticity. Furthermore, people approach decisions perceived to involve authenticity differently from more ordinary ones. Oktar and Lombrozo (2022) gave subjects vignettes in which deciding based on “deliberation” (which was akin to standard decision making) conflicted with deciding based on intuition. For instance, one scenario involved picking between two people with whom to pursue a romantic relationship. One person is a better “on paper” match, and it’s stipulated that you believe the method that determines them as such is accurate. The other person is one with whom you vibe with better in that sort of intangible, gut-feeling sort of way. Subjects leaned strongly toward pursuing a relationship with the latter person (the “intuition” choice) instead of the former (the “deliberation” choice). They also perceived this case as one that involves authenticity. In fact, when a choice was perceived to involve authenticity, subjects favored intuition-based decisions over deliberation-based ones. The preference of for intuition over deliberation raises the question of whether there are non-deliberative (in the sense of standard decision-making) procedures that can help us choose in transformative decisions in a way that preserves authenticity.

In fact, there are accounts that purport to preserve authenticity while also preserving rationality. Paul (2014) favors choosing on the basis of revelation. According to Paul, the decision should be reframed as one that asks whether the agent wants to discover what life will be like and who the agent will become, post-transformation. Essentially, the agent chooses transformation for its own sake. In doing so, the agent chooses rationally by choosing in accordance with the preference for discovery. The agent also chooses authentically by choosing based on the desire to know what the experience will be like for them and who they will become once transformed. The problem is that choosing based on revelation does not always seem appropriate given the import of many transformative experience. For instance, many transformative experiences involve other people, such as becoming a parent or marrying someone. Choosing these experiences on the basis of revelation fails to capture concern for the resulting child or the future spouse. Paul might respond that this objection to revelation is a moral one, and that a similar objection could be levied if the choice were made on the basis of self-interest. Thus, the objection might not count specifically against revelation. However, there’s a second worry, which is that revelation doesn’t help in cases where the decision lies between two transformative outcomes as opposed to a decision between the status quo and one transformative outcome.

Callard (2018) picks up on this last worry and points out that revelation for its own sake lacks a mechanism for valuing different revelations differently. Instead, Callard favors an account based on aspiration and proleptic reasons. For Callard, proleptic reasons are provisional ones aimed at a good to which one aspires but may only have an “inchoate, anticipatory, and indirect grasp”. For instance, someone who aspires to be a wine connoisseur initially lacks any conception of what they are trying to appreciate or experience when they take a sip of wine. Importantly, proleptic reasons allow the aspirant to act rationally when they start the process even though they lack knowledge of what the end goal is like. Interestingly, these types of cases typically lack the sharp discontinuity that are associated with paradigm transformative experiences like parenthood. In fact, Callard suggests that parenthood is not as discontinuous as assumed because potential parents often dedicate a lot of mental energy to the decision and learning all they can about parenthood. What appears to be a climactic “let’s go for it” is “embedded in a longer transformative journey” (Callard 2018, 63). Putting all of this together, transformative decisions can be made rationally via proleptic reasons and are authentic since they result from intentional deliberation about who the agent hopes to become.

Chang (2015) offers an interestingly different way of approaching transformative decision making. Chang starts by distinguishing between event-based and choice-based transformations. The former are the type that fit how transformative experiences are typically described. They involve experiences that are downstream from the choice—like becoming a parent—that transform the agent. The latter is where Chang’s interest lies. Choice-based transformation is one in which “the making of the choice itself transforms you” (Chang 2015, 240). Here, Chang has in mind cases in which one invokes voluntarist reasons when making a decision. Voluntarist reasons are reasons generated by the will that—on Chang’s account—are appropriate when all other existing reasons underdetermine what an agent ought to do. (Chang favors an account in which the will strengthens an existing reasons, but there are other voluntarists like Korsgaard (1996) who hold that the will can create reasons ex nihilo.) Crucially, in willing a voluntarist reason, one simultaneously reveals themself to be the type of person for whom that choice is rational. This type of account also appears to preserve authenticity (since what could be more authentic than one’s own will) and rationality (since adding the voluntarist reason to the mix tips the scales in favor of the choice that’s been made). Of course, voluntarist accounts are somewhat controversial, so Chang’s account of how to make transformative choices would inherit all the worries that voluntarism does.

Although these accounts—Paul’s, Callard’s, or Chang’s—all face worries, it is worth noting that if any of them work, then there is a way to preserve both authenticity and rationality even while respecting the original challenge of transformative experience to standard decision making procedures. None require knowing what the experience will be like or who will come out the other end. Nevertheless, one can still rationally and authentically plunge into the unknown.

6. Applied Ethics: In Light of Transformation, How Should We Treat Others?
Transformative experiences also demand that we revisit how we treat others in light of the fact that they potentially may undergo transformative experiences. As seen in §1, transformation may arise in many contexts, far too many to attempt to tackle here. Instead, this section will focus on three types of contexts in which the issue of how we should treat others given the possibility of transformation might arise: when we choose for others, when we make long-term decisions involving others, and when we create environments that (dis)favor transformation.

6.1 Choosing for Others
Plausibly, it’s wrong to interfere with the autonomy of others, especially when it comes to important personal decisions. As Akhlaghi (2022, 7) points out, interference is even less defensible when others are choosing to undergo transformative experiences because there is a

moral duty not to interfere with the autonomous self-making of others, through their choosing to undergo transformative experience to discover who they will become.

However, people often find themselves in situations in which they need to make decisions on behalf of others that may lead to transformative experiences. In fact, those who take the leap and become parents then find themselves in a position to make transformative decisions on behalf of their children. For instance, some parents of deaf children must decide whether to get cochlear implants for their child. Ideally, children receive these implants when they are extremely young, usually before the age of three. The early age at which the decision must be made forces parents to choose on behalf of their children. In addition to being a choice that affects how their children will perceive and navigate the world, the choice carries incredibly significant social consequences. Deaf communities offer distinctive goods that hearing communities cannot, and vice versa. Furthermore, as disability advocates point out, the difference between being deaf and non-deaf is “mere-difference” rather than “bad-difference” (see, e.g., Barnes 2014). This offers a further reason to believe that the decision to give a child a cochlear implant is a transformative one that must be made on the basis of the child’s future experiences as deaf or hearing as opposed to on the basis of value.

Put this way, parental choices, especially significant ones like deciding whether to get a child a cochlear implant, also resemble the non-identity problem. Parfit (1984) raises the non-identity problem as a puzzle for decisions that affect who will exist in the future. For instance, suppose a potential parent is choosing between implanting one of two embryos, and chooses one that has a genetic defect that causes monthly migraines that the other embryo lacks. Given that there are no other relevant differences between the two embryos, it appears that the parent has done something wrong by bringing into existence a person who will have a lower level of welfare than the other person who could have existed instead. However, it’s not obvious who the potential parent has harmed. Assuming the migraine-inflicted person prefers existing to never having existed, they have not been harmed since the alternative is non-existence. If that’s right, then perhaps the parents have done nothing wrong since no one has been harmed.

Similarly, the personally transformative aspect of transformative experiences affects who exists after undergoing the experience. Receiving a cochlear implant and growing up in a hearing community will transform the child into a radically different person from who the child would be if they did not receive an implant and grew up in a deaf community. The person who develops won’t have grounds to complain since wishing their parents had chosen otherwise would be like wishing they did not exist. But even if one does not believe that the cochlear implant is an existence-affecting decision point, the fact that the two potential futures are so foreign to each other means that a person situated in one of those futures cannot project what it would be like to have had the other life. If the earlier points about projecting value are correct, the person who has grown up in the deaf community cannot assign a subjective value to the alternate life in which they had the cochlear implant, and vice versa. Without those value assignments, it doesn’t appear that either person can claim that they would have been better off in the alternate scenario. Thus, whether the person receives the cochlear implant or not, articulating how that transformation harmed them will be challenging. Perhaps the parent can choose either option without harming or wronging anyone. The situation is also similar from the perspective of the parents and their preferences, for they will have grown to love their child and the particular person into whom they have developed.

Philosophers diverge on how parents should choose in these cases. (See, e.g., Harman (2015) for an interesting discussion on when “I’ll be glad I did it” reasoning is appropriate.) However, there’s a more interesting lesson to be drawn from these types of cases. Recall from §4.1 and §4.2 that Paul objects to certain types of decision-making strategies because they treat future selves as if they were third persons rather than integrated parts of the same self. These cases show that even in the third person case, standard decision-making procedures are not particularly helpful. Transformative experiences still create major challenges for decision-making.

At the other end of life, decisions must sometimes be made for people post-transformation. People suffering from dementia or Alzheimer’s transform in myriad ways, and their preferences often diverge from the preferences of their earlier selves. Sometimes, these preferences regard end of life care. Since the person with dementia or Alzheimer’s is not considered legally competent to change their advanced directives, others must make important medical decisions on their behalf. These people must decide whether to respect the preferences of the past self, or the current one. In some ways, this a questions about the weight that ought to be assigned to past preferences, and we might resolve in the way Pettigrew (2019) suggests by constructing a general value function that aggregates the past and current preferences. However, if the personal transformation is taken seriously, the current self is a different person. Binding them to the preferences of the past self is akin to binding them to the preferences of another person.

6.2 Long Term Decisions
In addition to transforming in the early and late stages of life, people transform throughout their lives, either via momentous “big” changes or via an accumulation of “small” changes. Long-term decisions must factor the potential for transformative change into account. For instance, Lackey (2020) and Chan (2020) independently address this issue in the context of punishment. Lackey’s argument demands that those with the power to issue long-term punishments, like judges, must consider the possibility of transformative change. Plausibly, punishment should be sensitive to the relevant evidence available at that time. We know that people change, often in transformative ways, and imprisonment seems like exactly the type of experience that would be transformative. Issuing strict, long-term prison sentences—such as life sentences without the possibility of parole—screens off the possibility of taking relevant future evidence into account. Thus, recognizing the possibility of transformation offers a powerful argument for sentencing reform.

Extended partnerships also provide another context in which transformative changes need to be accounted for when making long term decisions. For instance, marriage vows typically involve some type of life-long pledge. And, as anyone familiar with married people knows, people change in ways that disrupt the marriage. In a way, identifying these changes as transformative makes sense of an already common phenomenon. But it also raises a further question: how can one felicitously make a life-long promise to someone knowing that both they and their partner may transform in a way involving a radical shift of preferences and life goals? The centrality of transformation to this decision is doubled because the marriage itself is often transformative. It’s true that transformations connected with relationships tend to involve incremental changes because the relationship develops incrementally. But these incremental changes can lead people away from each other and won’t be noticeable until the larger schism develops due to their incremental nature.

These long-term decisions involving others suggest that the normative questions transformative experiences raise go beyond self-interested decision making. Furthermore, some of the proposed ways of making transformative decisions have interesting implications for these types of cases. For instance, Ullmann-Margalit suggests that there simply is not a rational way to make transformative decisions. Applying that to these cases gives the radical result that decision to marry or sentence someone to prison cannot be made rationally! Ullmann-Margalit’s further suggestion that perhaps these decisions can be made reasonably also has interesting implications. She suggests that minimizing the discontinuity of a big decision and backing off the point of no return might help, and includes moving in before marriage as one such way to ease reasonably into a big decision like marriage. The application for the prison case is even more interesting, for it suggests that life in prison should not be such a radical departure from life outside of it, and that there must be meaningful short term possibilities for parole.

6.3 Transformative Environments
Finally, special attention must be paid to our social environments since they shape the way in which members of society may transform. Education provides the most obvious instance of this. Fights over parental rights to control their children’s educations are so contentious precisely because early experiences shape who children grow up to become. Even higher education is taken to have a transformative goal (see Paul & Quiggin 2020). College students are typically in their early twenties—the last age at which people tend to undergo radical change before developing fairly stable preferences and goals that tend to endure through most of adulthood. In addition, the transformative environment of education shapes more than the individual students who experience it. Students go on to become members of society who in turn create an environment that will shape those who come after them. Morton (2021) points out that this tends to have a self-reinforcing effect because elite institutions like the Ivy League schools disproportionately influence social policy. Even if students enter these institutions without the goal of maintaining the status quo of the elite, by the time they graduate, they can be transformed into members of the elite who are now invested in safeguarding the interests of their newfound group. This, in turn, shapes social policies that reinforce which transformative experiences are most readily available to members of society.

This idea that social policies can shape individual choices and preferences creates both an opportunity and a responsibility to reconsider which policies ought to be adopted in light of the potential for transformative experience. For instance, Pettigrew (2023) examines the concept of nudging introduced by Thaler and Sunstein (2008). In a nutshell, nudging is a sort of intervention that pushes people toward making a particular choice while leaving open the possibility that they choose otherwise. Opt-out retirement programs offer a paradigm case of nudging since they increase the likelihood that people will put money away for retirement while still allowing people the ability to opt-out. Thaler and Sunstein label nudging policies as “libertarian paternalism” because while these interventions look paternalistic, they purportedly help people make the decision that their future idealized selves will deem to be best “as judged by themselves” (Thaler and Sunstein 2008, 4). Of course, if the decisions are transformative, or incremental parts of multiple decisions that become transformative, simply looking at the judgment of the future self does not seem appropriate at all! (And Pettigrew makes precisely this argument before proposing a modification to the “as judged by themselves” test that’s based on his weighted general value function.)

There are at least two bigger-picture issues that come out of this discussion of nudging people, potentially toward transformation. First, the norms regarding nudging people toward potential transformation requires further examination. Most policies, nudging or otherwise, exhibit some degree of interventionism. In addition to which transformations might be preferable for society as a whole, there’s also the question of whether we should adopt policies that maximize individual autonomy. Second, we might wonder why nudging works in the first place, and if it really does work for transformative experiences. Recall that part of the nature of transformative experiences is that one cannot know what they will be like or how they will change prior to having the experience. Nudging, on the other hand, only works if these things are somewhat predictable. Of course, unpredictability with respect to an individual is compatible with predictability with respect to overall social trends. But, if nudging really works, it does lend credence to the idea certain elements of transformative experience are predictable and perhaps strengthens the case of those from §4 who believe that one can make rational choices with respect to transformative experiences.

Finally, we’ve structured our society in many unfortunate ways. This goes beyond the discrimination that people suffer on the basis of race, sex, ability, socioeconomic status, and the like. More subtle conceptual structures shape and limit the way we form our self-identities. These injustices are obviously bad in themselves, and they also interact with transformative experience in a way that furthers the injustice. Barnes (2015) argues that one’s “self-conception and self-identity aren’t developed in cultural isolation” and that “social norms and structures make certain ways of interpreting or thinking about ourselves readily available” (185). For example, society makes it easy for women to “re-shape their self-conception to cohere with the image of a dutiful, submissive wife” (186). Similarly, “brave inspiration” is a readily available self-conception for disabled people while “thriving person in an unconventional body” is not (185–186). These states of affairs are bad for women and disabled people. Since personal transformation involves changes to one’s self-conception, society plays an outsized role in the types of transformations that are available to us.

Higher-order evidence is evidence which bears on a believer’s rational capacities, epistemic performance, or evidential situation. Many epistemologists hold that this kind of evidence can rationally affect our “first-order” beliefs as well: that is, those beliefs about the world that we form using the relevant rational capacities, or in the relevant evidential situation. It is easiest to get a grip on this phenomenon by looking at examples. Here is a paradigmatic case (paraphrased from Schoenfield 2018: 690, based on similar cases put forth by Adam Elga [2013 and 2008—see Other Internet Resources], Lasonen-Aarnio [2014], Christensen [2010a], and others):

Hypoxia: Aisha is out flying her small, unpressurized airplane, wondering whether she has enough fuel to make it to Hawaii. She looks at the gauges, dials, and maps, and obtains some evidence, E, which she knows strongly supports (say to degree .99) either the proposition that she has enough gas (G) or that she does not (~G). Thinking it over and performing the necessary calculations, Aisha concludes G; in fact, this is what E supports. But then she checks her altitude and notices that she’s at great risk for hypoxia, a condition which impairs one’s reasoning while leaving the reasoner feeling perfectly cogent and clear-headed. Aisha knows that at this altitude, pilots performing the kinds of calculations she just did only reach the correct conclusion 50% of the time.

Aisha’s “first-order” evidence is her evidence bearing directly on her beliefs about G: that is, the evidence she receives by consulting her dials, gauges, and maps. Her “higher-order” evidence is her evidence suggesting that she’s at risk for hypoxia. Higher-order evidence bears directly on Aisha’s epistemic situation in various ways, but does not bear directly on G. The primary question that this case raises, for the higher-order evidence literature, is: does Aisha’s higher-order evidence also rationally affect her beliefs about G? (That is, does it bear on G in some indirect way?)

Some say yes. It would be reckless for Aisha to head out over the Pacific after realizing that there is such a serious chance that she’s rationally impaired, which surely shows (some argue) that she should not be very confident of G. Others say no. As the case stipulates, Aisha’s first-order evidence strongly supports G; whether Aisha is hypoxic has absolutely nothing to do with whether she has enough gas to make it to Hawaii. (Why would her total evidence prompt her to revise confidence in G, when the new evidence she has gained is irrelevant to G?) Answering “yes” raises questions about exactly how and why this level-interaction works. Answering “no” raises questions about how to explain the intuitive unreasonableness of Aisha’s maintaining high confidence in G.

Higher-order evidence thus raises a puzzle. Two apparent features of Hypoxia are in tension with one another: it seems that Aisha is required to reduce her confidence in G, and yet it also seems that her evidence still strongly supports G. Some see this as a conflict between the belief state that seems intuitively rational (reduced confidence or suspension of judgment about G) and the belief state that seems strongly supported by her evidence (high confidence in G). Others see it as a conflict within her beliefs. If Aisha maintains high confidence in G, but also takes seriously the possibility that she could be hypoxic, she will be in a state of epistemic akrasia: she will have a belief state (her high confidence in G) which she also judges likely to be irrational, or unsupported by her evidence.

In what follows we will see these issues crop up in several different forms. This article will be organized around a few core questions about higher-order evidence, which will give us different ways to approach these central puzzles.

Is higher-order evidence different from ordinary evidence? If so, how?
Why might we deny that higher-order evidence has a rational effect on first-order beliefs? And if we take that approach, which problems and questions remain?
If higher-order evidence rationally affects first-order beliefs, how does this work? What sort of principle might govern that interaction?
Is there a way to split the difference, embracing both sides of the puzzle? For example, might there be different, incompatible epistemic norms at work in cases like Hypoxia, one telling Aisha to believe G and another telling her to suspend judgment?
A few quick notes before moving on. Following most of the literature, the term “higher-order evidence” is used here, although it would be more accurate to speak of higher-order evidential effects or higher-order evidential import (since one piece of evidence may be relevant to a person’s beliefs about many different subject matters, in a number of different ways). And like most of the literature, the focus is on higher-order defeat, rather than higher-order confirmation, even though these possibilities go hand in hand. Some of the issues discussed in this article are discussed in more detail in the entry on epistemic self-doubt. Finally, although the higher-order evidence debate has its roots in the literature on disagreement, this entry focuses on the simpler, single-person case introduced above and mostly omits disagreement.

1. What is Distinctive About Higher-Order Evidence?
Higher-order evidence was introduced above by example. We can come across higher-order evidence in a variety of ways: brain-altering conditions like hypoxia, or drugs, but also more mundane sources like bias and fatigue. Many epistemologists have discussed disagreement as a source of higher-order evidence: if an equally well-informed and thoughtful person looks at the evidence and draws a different conclusion from yours, this arguably gives you reason to worry that you’ve made a rational error (see, e.g., Kelly 2005, Feldman 2006, Christensen 2007b, Elga 2008 and much following literature). And some have argued that “irrelevant influences” on belief, such as one’s religious upbringing, can be a source of higher-order evidence as well: the realization that you would have had different religious beliefs if you had been raised in a different community, for example, might give you reason to doubt the rationality of those beliefs (see Elga 2008—see Other Internet Resources, White 2010, and Vavova 2018). It is hard to find an uncontroversial definition of higher-order evidence, as different authors understand it in different ways. But even without a general definition, we can look at cases like Hypoxia to see how higher-order evidence is importantly distinctive. (This section assumes that higher-order evidence has some first-order rational import, as this is where the distinctive features arise.)

At first glance, one might think that Hypoxia is simply a case of undercutting defeat, and Aisha’s justification for believing G is defeated just as one’s perceptual beliefs might be defeated by, for instance, evidence of tricky lighting. But as Feldman (2005) argues, higher-order defeat and undercutting defeat are importantly different. Compare Hypoxia to a case in which you find out that a certain wall, which appears to be red, is illuminated by a red light. In the latter case, you might recognize that your experience as of a red wall generally supports the belief that the wall is red, and also recognize that you responded appropriately to that evidence before finding out about the tricky lighting. But in Hypoxia it seems that Aisha is not in a position to recognize either of these things. If she takes her higher-order evidence seriously, she will come to doubt whether her first-order evidence ever supported G in the first place. And she will also doubt whether she evaluated it correctly. (See Coates 2012 and Christensen 2010a for similar discussion.) So higher-order defeat is not simply undercutting defeat.

Joshua DiPaolo (2018) argues that higher-order defeat is also distinctive in that it is “object-independent”. For instance, learning that there is a red light on the wall will undercut your belief that the wall is red, but will have no effect if you believe that the wall is not red. Higher-order evidence, on the other hand, typically does not discriminate based on the contents of the beliefs it targets. Extending DiPaolo’s argument to our case above, Aisha’s higher-order evidence targeted her belief about G, but could have easily targeted a belief with different contents if she had been reasoning about a different matter while flying at that altitude.

Christensen (2010a) argues that higher-order evidence is distinctive because its import is agent-relative. While Aisha's information about hypoxia gives Aisha reason to doubt her beliefs about whether she will make it to Hawaii, it would have no such effect on another person looking at the same charts and dials from the safety of the ground. However, merely treating higher-order evidence as indexical does not seem to make it less puzzling; see Schoenfield 2018 for further discussion.

Perhaps the most puzzling feature of higher-order evidence is that it seems to obligate agents to ignore or set aside parts of their total evidence. (See Christensen 2010a for an early defense of this idea; others, including Elga (2007), also defend it in the context of disagreement.) Even as Aisha comes to doubt her conclusion about G, her first-order evidence and reasoning is still plainly before her. But she cannot rationally appeal to it in forming her beliefs. If she could, Aisha might argue:

although I’m at an altitude that renders me susceptible to hypoxia, I must be immune. After all, my first-order evidence in fact supports the conclusion that I have enough fuel to make it to Hawaii, and that’s the very same conclusion I reached!

To a supporter of higher-order defeat, such reasoning looks irrational and dogmatic. (Furthermore, if she could reason this way, Aisha could arguably keep her high confidence in G—which his just to say, her higher-order evidence wouldn’t have a rational effect after all.) In order to rule out the rationality of such reasoning, while acknowledging that Aisha’s first-order evidence has not vanished, some epistemologists say that Aisha must set aside the targeted first-order evidence and reasoning once her higher-order evidence comes in. This thought is often called “Independence”. Section 3.1 discusses it in more detail.

Finally, related to this last observation, Horowitz (2019) and Schoenfield (2015b, 2018) both argue that higher-order evidence is predictably misleading: if it has first-order effects, we can predict a priori that it will lead ideally rational thinkers away from the truth. We expect first-order evidence to generally make us more accurate, provided that we respond to it rationally. And we expect higher-order evidence to lead us away from what our first-order evidence supports. So when we accommodate all of that evidence rationally, we’ll tend to end up with a less accurate belief state than we would if we had ignored the higher-order evidence.

In all of these ways, higher-order evidence presents distinctive problems for epistemology. (Though distinctive does not necessarily mean rare: Hedden and Dorst [forthcoming] argue that almost all evidence has higher-order import.) As we will see in the next section, the difficulties presented by higher-order evidence have prompted some epistemologists to deny that it has first-order import altogether.

2. Denying the Import of Higher-Order Evidence
Higher-order evidence raises special problems. One very general problem is that it is hard to see how we could make sense of higher-order evidence within a consistent, total picture of rationality. Maria Lasonen-Aarnio (2014) brings out this point especially forcefully and comprehensively, so this next section will largely follow her presentation. (See also Schechter 2013 for discussion of many similar concerns.) We will then look at a slightly different objection to theories that accommodate higher-order evidence: the charge that they are self-defeating. The section will conclude with some options for theories according to which higher-order evidence does not rationally affect first-order beliefs.

2.1 Structural problems for higher-order evidence
Lasonen-Aarnio begins with the observation that respecting higher-order evidence can compel a rational agent to violate genuine epistemic rules. (This is very similar to the idea presented in the introduction: that after Aisha receives her higher-order evidence, her total evidence still supports G.) Since this thought is common to much of the higher-order evidence literature, it will be worth spelling out in detail the reasoning behind it. One simple way to make the thought plausible is to focus on a case of entailment—so, let’s suppose Aisha’s first-order evidence entails G. Then we can argue as follows:

P1.
Aisha’s first-order evidence entails G.
P2.
After Aisha receives her higher-order evidence, her total evidence entails G.
P3.
It is a rational requirement to believe what our evidence entails.
P4.
After Aisha receives her higher-order evidence, she is rationally required to believe G.
P5.
After Aisha receives her higher-order evidence, she is rationally required to suspend judgment on G.
C.
After receiving her higher-order evidence, Aisha is rationally required to violate a rational requirement.
P1 is built into (this version of) our case. P2 follows from P1, assuming that Aisha’s body of total evidence grows monotonically over the course of the story. P3 is plausible, and would nicely explain our verdict at the beginning of the story: that, before she receives her higher-order evidence, Aisha should believe G. P4 follows from P2 and P3. And P5 is the intuitive verdict that many epistemologists share about cases like Hypoxia. (Though see Henderson forthcoming, Staffel forthcoming, and Steglich-Peterson 2019 for alternative views on which higher-order evidence does not require suspension of judgment, but still has a rational effect.) So if P5 is right, this means that Aisha is required to disobey an epistemic rule which applies to her current situation: she must suspend judgment in a proposition that is entailed by her evidence, rather than believe it.

Though focusing on entailment makes the argument above particularly clear, it’s not essential. The same problem can, plausibly, arise with non-entailing evidence as well. Lasonen-Aarnio focuses on a perceptual case, and Christensen (2010a) argues that cases involving inductive reasoning can lead to the same conclusion. (Both argue for versions of P2 in non-entailing contexts: they argue, respectively, that the perceptual or inductive support remains even in the face of higher-order undermining.)

How can we make sense of Aisha’s situation, in light of this apparent conflict? Lasonen-Aarnio (2014) surveys several possible ways to make room for the conflict within our overall epistemological theory. One strategy we might try is to say that epistemic rules have built-in exceptions for higher-order defeat. That doesn’t solve the problem, Lasonen-Aarnio argues, but just pushes it back: if any rule can be defeated, then so can rules with built-in exceptions. A second strategy is to say that epistemic rules are hierarchical, with some taking precedence over others; but again, the higher-level rules must themselves be defeasible, leading to an infinite hierarchy. The rules must also be ordered by a “meta-rule” which determines which rule governs one’s current situation; what if the meta-rule itself is defeasible? It seems that we are either left with an infinite regress, or that we must accept a stopping point—a rule which itself cannot be rationally defeated. Another strategy is to posit an “Über-rule” which specifies the rational response for each unique situation. This fails, Lasonen-Aarnio argues, because it is too complex for us to grasp, too different from the epistemic rules we ordinarily take ourselves to follow, and endorsing it leaves us with an unsatisfying kind of particularism about rationality. This proposal also runs into the same problem we saw above. If the Über-Rule is always correct, we must say that it cannot be rationally defeated. If we are willing to accept indefeasible rules, why should we think that higher-order defeat happens at all? (Why not just accept the rules governing Aisha’s belief in G as themselves indefeasible by higher-order doubt?) Lasonen-Aarnio concludes that in light of these problems, we should reject higher-order defeat. (See Bradley 2019 for response to Lasonen-Aarnio, and a defense of the Über-rule.)

2.2 Higher-order defeat and self-undermining
Another route to doubt about higher-order defeat is what’s sometimes called the “self-undermining” objection. As several epistemologists have pointed out, there are strange consequences for anyone who believes a theory on which higher-order evidence has first-order effects: it seems that this belief is liable to undercut its own justification. Let’s consider a candidate principle on which higher-order evidence can defeat first-order beliefs; call it “HOD” (for “higher-order defeat”). Now consider a person, Sam, who believes that HOD is a true principle of rationality. What happens if Sam’s belief in HOD is itself a target of higher-order defeat? This might happen through peer disagreement (which is the focus of most of the literature on this objection) or other means. Afterwards, according to HOD, Sam cannot rationally believe HOD.

This might already seem like a problem to some: a true theory of rationality should not (arguably) undermine our justification for believing it. And taking it a step further, it can look like the theory is either paradoxical or incoherent. If HOD can call for its own rejection (in the cases where belief in HOD is itself undermined), how can it give us coherent directions? Suppose Sam’s higher-order evidence targets two beliefs simultaneously: his belief about some proposition P, and his belief about HOD itself. HOD tells him to reduce confidence in P. But since HOD is undermined as well, it is arguably also telling him not to reduce confidence (or to reduce confidence to a lesser degree). These recommendations are incompatible, yielding the worry that HOD is internally inconsistent. On the other hand, the worry about paradox arises when we consider what happens when Sam begins to revise his beliefs. Once HOD is undermined, suppose Sam no longer believes it, and goes back to revise the beliefs that he formed on its basis. But his last application of HOD was what led to his doubting HOD—so if he “undoes” that step, he’ll believe HOD again! But then his belief in HOD will be undermined by his higher-order evidence, and the loop will start over. (For further discussion see Elga 2010, Weatherson 2013, Christensen 2013 and 2021a, and Bradley 2019. See Roush 2009 for discussion of a related phenomenon.)

Advocates of higher-order defeat have offered a few different responses to this class of objections. One option, proposed by Elga (2010), is to say that principles like HOD are exempt from undermining. (Elga’s view draws on an argument found in Lewis 1971 and H. Field 2000, for the conclusion that our most fundamental belief-forming methods must be self-recommending.) Another option is to say that our attitude towards principles like HOD is not one of belief, but some other type of attitude. (See Goldberg 2013, Barnett 2019, and Fleisher 2021 for examples of this approach.) Finally, one might simply deny that Sam is obligated to enter the paradoxical loop once his belief in HOD is undermined. Christensen (2013, 2021a, 2021b) argues along these lines. The loop gets started if we think that once Sam rationally doubts HOD, he is rationally required to stop adopting the beliefs that HOD recommends, and instead adopt whichever beliefs he now regards as most rational. But this thought is not part of HOD, and we are free to deny it—we can maintain that HOD is true, and rationally binding even for those who rationally doubt it. (This is different from saying that HOD must be self-recommending; on Elga’s proposal, one should believe that one should always follow HOD, and also always follow it. On Christensen’s, one should always follow HOD, regardless of what one should believe one should do.) This response would bypass the paradox. But it comes at what some defenders of HOD see as a cost: it means we must accept some instances of rational epistemic akrasia. If Sam revises his belief in HOD, but continues to follow it, as this suggestion would have him do, he will end up with a belief state whose rationality he doubts.

In the face of these difficulties, some epistemologists conclude that higher-order defeat is simply not a genuine phenomenon. These views are discussed next. In sections 3 and 4, we will come back to the options for accommodating higher-order defeat.

2.3 Consequences of denying the import of higher-order evidence
If we hold that higher-order evidence does not have first-order rational effects, what should we say about cases like Hypoxia? There are two mainstream options defended in the literature. “Level-Splitters” suggest that Aisha’s first-order belief (about G) should follow her first-order evidence, so she should be highly confident of G. But her higher-order belief (about her own rationality, or about what her first-order evidence supports) should follow her higher-order evidence. “Steadfasters” argue that Aisha should just dismiss the higher-order evidence altogether—believe G, and believe that her evidence supports it. (See Alexander 2013, however, for an argument that no response is justified in Aisha’s case, and Leonard 2020 for an argument that it is indeterminate what Aisha should believe.)

Defenders of the Level-Splitting view include Lasonen-Aarnio (2014), Coates (2012), Weatherson (ms–see Other Internet Resources), Williamson (2014), and Wedgwood (2012). The obvious advantage is that this view straightforwardly takes all of Aisha’s evidence into account: her evidence about G affects her beliefs about G, and her evidence about her epistemic situation affects her beliefs about her epistemic situation. An obvious upshot of this view is that epistemic akrasia can be rational: in particular, in Hypoxia, Aisha should end up believing G, while also believing that her evidence likely doesn’t support G.

But admitting rational epistemic akrasia in this case incurs a large intuitive cost. As Horowitz (2014) argues, if epistemic akrasia is rational in cases like Aisha’s, then so is bootstrapping: if Aisha finds herself in this situation a number of times in a row, or regarding a number of different beliefs, she can use her first-order beliefs to establish a fantastic track record of success, ultimately dismissing the possibility that she was ever rationally impaired. This reasoning looks absurd—because, presumably, there is a tension between strongly holding a belief while also believing one’s evidence doesn’t support it. So why allow this tension in even one case? Horowitz also illustrates other examples of irrational reasoning and action that seem warranted by epistemic akrasia; see also Brown 2018, Littlejohn 2018, and Silva 2018 for further discussion of intuitively irrational reasoning and action licensed by epistemic akrasia in these cases, as well as Feldman 2005 for an earlier rejection of epistemic akrasia in similar circumstances.

The second possibility is to say that higher-order evidence simply has no effect on first-order or higher-order beliefs. (Following the peer disagreement literature, we could call this a “steadfast” view; Smithies [2019] calls it “upward push”.) Kelly (2005) seems to tentatively support this view in the context of peer disagreement (though his later work defends a more moderate “total evidence” view; see, for example, Kelly 2010). Titelbaum (2015) argues for the nearby position that a rational agent can never be mistaken about the rational requirements that apply to her situation (although she might be rationally uncertain as to which situation she is in); he writes that “mistakes about rationality are mistakes of rationality”. Titelbaum’s argument for this rests on the assumption that epistemic akrasia is irrational. (See also Titelbaum 2019. See C. Field 2019 for criticism of this position.) Tal (2021) also defends the steadfast view, in part appealing to the irrationality of epistemic akrasia.

Smithies (2019; see especially ch. 10) gives an extended defense of a steadfast view. He argues that ideal rationality requires rational omniscience—that is, omniscience both about what the rational requirements are and which requirements apply to one at all times. This means that misleading higher-order evidence is, in an important sense, impossible to come by. While non-ideal agents like Aisha may be mistaken about what evidence they have or what it supports, these mistakes are themselves departures from ideal rationality. (Smithies also supplements this with a view about non-ideal rationality, which is discussed below.)

One of the primary objections to the steadfast view is that ignoring higher-order evidence appears to be blatantly dogmatic: it is hard to see how it could be rational for Aisha to continue believing G with no reduction in confidence, after hearing that her altitude puts her at risk of hypoxia. (Though Tom Kelly has argued [see Kelly 2013, for example] that dogmatism in such cases is not inherently irrational.) Additionally, acting on her belief that G—say, setting off for Hawaii with a plane full of passengers—would be shockingly irresponsible, suggesting that something is wrong with the belief that G. (See Christensen 2010a, for example.)

Cases like Hypoxia therefore leave us with three choices. If we say that Aisha should maintain confidence in G, but reduce confidence that it’s rational for her to belief G, we must explain why epistemic akrasia can be rational. If we say she should maintain confidence in G and dismiss the possibility of hypoxia, we must explain why dogmatism can be rational. And if we say she should reduce confidence in G, we must explain how this type of defeat works, and how it fits into our broader theory of rationality. Let us now return to that last question.

3. Accommodating the Import of Higher-Order Evidence
In this section we will look more closely at the possibilities for accommodating higher-order defeat. Even setting aside questions about how such defeat fits into the rest of our epistemological theory, there are several more local questions about how higher-order defeat works. What sort of principle could explain the intuitive verdicts in cases like Hypoxia? Which evidence and reasoning can agents like Aisha rely on in forming their beliefs, and under what circumstances? What determines which attitude, exactly, Aisha should have towards G after taking both her first-order and her higher-order evidence into account?

There are a couple of different ideas that epistemologists sympathetic to higher-order evidence typically aim to capture. One is the thought that higher-order evidence requires us to “bracket” or set aside some of our first-order evidence and reasoning. This is often called “Independence”. The second is the thought that, in accommodating one’s higher-order evidence, one should somehow adjust one’s first-order belief to match what one has learned from the higher-order evidence: either to match what one believes or has reason to believe would be rational, to match one’s expected level of reliability, or some variation on one of these. Principles dictating how one should calibrate one’s first-order beliefs in light of higher-order evidence are often called “level-bridging principles” or “calibration principles”.

Many specific proposals for how to accommodate higher-order evidence (often focusing on the case of disagreement, which we can think of as a variety of higher-order defeat) combine both of these ideas. For example, here is Elga’s (2007: 490) formulation of the “Equal Weight View” of disagreement:

Equal weight view Upon finding out that an advisor disagrees, your probability that you are right should equal your prior conditional probability that you would be right. Prior to what? Prior to your thinking through the disputed issue, and finding out what the advisor thinks of it. Conditional on what? On whatever you have learned about the circumstances of the disagreement.

Calibrating your confidence to the probability that “you would be right” is a way of matching your confidence in P to your expected reliability about P, so the Equal Weight View is in part a level-bridging principle. And the “prior” and “conditional” clauses commit Elga to an Independence principle as well: he is saying that we must set aside the specific first-order evidence and reasoning about the relevant matter, and consider only the surrounding “circumstances”. In the very next paragraph Elga clarifies that “whatever you have learned about the circumstances of the disagreement” must be independent from the reasoning leading to the disagreement itself. Instead, it should include facts about the situation that would affect your or your peer’s reliability, such as how much coffee you have each had, how absurd you find the other person’s answer, and whether the reasoning invites a type of mistake that you or your friend is especially likely to make.

The next two subsections discuss Independence and level-bridging separately. But many authors who endorse one of these (in one form or another) also endorse the other.

3.1 Independence principles
Independence principles say, roughly, that higher-order evidence calls for us to set aside some of our evidence, and not rely on it in our reasoning. This thought is required to rule out the kind of dogmatic response to higher-order evidence discussed above: if we did not have to set aside our first-order evidence and reasoning, it would be rational to rely on it to dismiss any higher-order doubt. Independence raises a number of questions. How can it be rational to ignore evidence? What, exactly, do we have to ignore? And does Independence open the door to skepticism?

Some balk at Independence because it simply seems irrational to ignore evidence (see Kelly 2005, 2010, for example). Kelly (2010) presents the objection roughly like this: if higher-order evidence requires us to ignore our first-order evidence, he asks, doesn’t that make rationality far too easy to come by? It appears that someone could botch their first-order reasoning completely, adjust their confidence in the face of higher-order evidence, and (since they are now obligated to ignore the first-order considerations) have all of their earlier rational mistakes forgiven. Christensen (2011) disagrees, arguing that while someone in this situation may have responded to part of her evidence correctly, we can still distinguish between her final belief state and the belief state of someone who didn’t make the initial mistake: someone who did every step properly is more rational than someone who did not. (See Sliwa & Horowitz 2015 for further discussion; see Schoenfield 2015a for an objection.)

If Independence is right, there are remaining questions about exactly what needs to be set aside—evidence? Reasoning? Something else?—and under what circumstances. As several authors have brought out, it is not quite right to simply say that our assessment must be independent of our first-order reasoning; often, facts about that reasoning, sometimes involving very specific aspects of our evidence, are highly relevant and it would be irrational to ignore them. (See Arsenault & Irving 2012, Kelly 2013, and Lord 2014 for versions of this objection; see Christensen 2018 and 2019 for replies.) It is also not quite right to say that our assessment must be independent of our first-order evidence; often, a single piece of evidence has several different effects, some of which should be set aside and others of which should be taken into account. See Christensen 2019 for detailed discussion of these complexities and others.

A different sort of question concerns how much evidence, information, or background beliefs we must set aside, and whether there are any limits on this. Intuitively, the scope of what we must set aside depends on the scope of higher-order undermining we receive, and which instances or types of reasoning are called into question. But how far can it go? What if everything is called into question? Considering this possibility leads to paradox. On the one hand, reason to universally doubt our reasoning seems to justify skepticism across the board. On the other hand, the way we would arrive at that skeptical state would be by using our reasoning… which has been, by hypothesis, called into question. Some epistemologists have suggested that since there is no stable and consistent response to such cases, we ought to conclude that universal defeat is impossible. But this response is hard to reconcile with the fact that defeat comes on a spectrum; if the extreme cases are impossible, what about the intermediate cases? (For further discussion, see Egan & Elga 2005, Enoch 2010, Sliwa & Horowitz 2015, Schoenfield 2015a, Christensen 2010a, 2019. See also the entry on epistemic self-doubt for further discussion.)

Even if we restrict our attention to more moderate cases of undermining, one still might wonder whether rationality can ask us to doubt so much that we end up skeptics about large and important domains (even if we are not skeptics across the board). Could it be rational for us to come to doubt, for example, all of our moral or religious beliefs on the basis of higher-order undermining? Elga (2007) argues (again in the context of peer disagreement) that we should not worry about this possibility. In order to doubt all of our moral beliefs, for example, we must have independent grounds to judge these beliefs to be unreliable. But if we set aside all of our moral beliefs, Elga argues, we won’t have enough left over to make any sort of reliability judgment—and therefore, won’t have rational grounds for doubt. So undermining can only happen locally. (And even then, he argues, many of our religious and moral beliefs will be safe, since on any particular occasion, the targeted religious or moral belief will be supported by our other moral and religious beliefs—the ones that aren’t called into question.) See Vavova 2018 and Christensen 2011 for further discussion of this point. Vavova and Christensen both differentiate between two possible ways to formulate Independence: one on which you must revise your beliefs insofar as you fail to have good independent reason to trust them, and one on which you must revise insofar as you have good independent reason to think that you’re mistaken. They argue that the first formulation has skeptical results, while the second avoids them.

3.2 Level-bridging principles
While Independence principles tell us what to set aside, level-bridging principles specify how our first-order beliefs should cohere with our higher-order beliefs. (As mentioned above, however, these principles aren’t always presented separately. Level-bridging principles also often include some element of Independence, or are meant to apply alongside an Independence principle.)

Some epistemologists argue that our first-order beliefs should line up somehow with our beliefs about rationality. (See Smithies 2019 and Titelbaum 2015, 2019, for example.) Others argue that our first-order beliefs should line up somehow with our beliefs about reliability. And still others incorporate elements of both. Some literature discusses both notions together and other literature distinguishes explicitly between the two. (See Christensen 2016b, Sliwa & Horowitz 2015, and Schoenfield 2015a for a few examples of more explicit discussions. See also Dorst [forthcoming] for a comprehensive overview of some of the principles proposed in the literature and the relationship between them, and the entry on epistemic self-doubt for discussion of some other level-bridging principles in greater detail.)

I will highlight the choice by first describing an intuitively plausible level-bridging principle that focuses on rationality, and a problem case for this principle which has motivated some epistemologists to move towards reliability instead. Although the second principle, too, has come under criticism, the case is a useful illustration of some of the complications that arise in this choice.

3.2.1 Rationality-focused principles
The general thought behind rationality-focused level-bridging principles is that your beliefs about the world should line up with your beliefs about what’s rational for you to believe. If you think it’s rational to believe P (or, if it’s rational for you to think that’s rational…) you should believe P. If you think it’s rational to believe ~P, you should believe ~P. This line of thought involves deferring to rationality as you would defer to an expert.

What if you’re uncertain about what’s rational? Arguably, this is Aisha’s situation: she’s unsure whether her evidence rationally supports G, as she initially thought it did, or whether hypoxia has confused her and her evidence doesn’t support G after all. One initially plausible response to this sort of situation is to say that when we’re uncertain, our beliefs should reflect a kind of weighted average of the responses we take to be possibly rational. This yields the result that if we’re about equally confident that we should believe P and that we should believe ~P, the thing to do is to suspend judgment.

The principle Rational Reflection makes this thought more precise in a degreed-belief, or “credence” setting. (Christensen [2010b] introduces and discusses the principle at length. Salow [2018] and Skipper [2021] are proponents of the principle. Elga [2013] and Dorst [2020] amend it, as we’ll see later.) Roughly, according to Rational Reflection, one’s credences (“Cr” in the formulation below) should align with those that one regards as rational (“Pr”):

Rational Reflection: Cr(A∣Pr(A)=n)=n

If this principle were a true rational requirement, then rational agents in situations like Aisha’s would end up with a strong coherence between their first-order beliefs and their beliefs about what is rational. Rational Reflection says, for instance, that it cannot be rational for Aisha to be rationally certain that it is rational to have .9 confidence in G, without also having .9 confidence in G. And it cannot be rational for Aisha to be uncertain, without also adopting a weighted average of the credences that she regards as possibly rational (weighted by her credence, in each, that it is rational). In the version of the case discussed in the introduction, one might interpret Aisha’s higher-order evidence as indicating that her high confidence in G is only 50% likely to be rational, given her first-order evidence. If it is 50% likely that high confidence is rational, and 50% likely that low confidence is rational, Aisha should plausibly adopt the weighted average of these possibilities and arrive at a middling level of credence. Rational Reflection supports this explanation of the story.

However, Rational Reflection has counterintuitive consequences in some cases, such as the “Unmarked Clock” case, introduced in Williamson (2014). Here is that case (based on the presentation in Christensen 2010b; see also Elga 2013; see Horowitz 2014 for a similar case, discussed in connection with epistemic akrasia, and see also Sliwa & Horowitz 2015 for a non-perceptual case with similar features):

The Unmarked Clock: Chloe is looking at an unmarked clock with just a minute hand, which jumps discretely between its positions. The hand is pointing somewhere around where the 4 would be, if it were marked, which of course it isn’t. Chloe is wondering: is the hand pointing to the 19? The 20? The 21? (call those propositions “P19”, “P20”, and “P21”).

Williamson, Christensen, and Elga (among others) all agree on the following about this case:

(1)
Chloe shouldn’t be certain of any of P19, P20, or P21.
(2)
Chloe’s credence in whichever of these propositions is actually her evidence should be highest, and should taper off as possibilities become more remote. (In other words, if the hand is really at 20 minutes after the hour, she should have highest credence in P20, a bit lower credence in P19 and P21, and lower credence still in P18 and P22.)
And finally,

(3)
Chloe can figure out all of these facts about her epistemic situation simply by thinking about the setup of the case, as we just have.
The problem is that (1), (2), and (3) together entail that Chloe should violate Rational Reflection.

Here is why: suppose the hand is at 20 minutes after the hour, and Chloe’s credence is in fact distributed as it should be, with P20 receiving the highest value. Since Chloe is rational, then by (2) above, she should also give significant credence to P19 and P21. Now consider the implications for what Chloe thinks her credence should be. In P20, Chloe’s current credence is rational. But in P19 and P21 (which we have just said she gives significant credence to), her current credence in P20 is too high. So she is in a position to conclude the following about her credence in P20: it’s definitely not too low, but it may well be too high. This unbalanced state violates Rational Reflection, and also looks like an instance of epistemic akrasia.

Adam Elga (2013) argues that this sort of situation motivates a different principle, which he calls “New Rational Reflection” (following Ned Hall’s “New Principal Principle”, and the argument in favor of it; see Hall 1994). Elga argues that just as we should not in general defer (directly) to experts when we know more than they do, we should not in general defer to rationality in cases where it is rational to doubt one’s rationality. If we use “Cr” for an agent’s credence, and “Pr” for a candidate ideally rational credence, we can express Elga’s principle as follows:

New Rational Reflection: Cr(A∣Pr is ideal)=Pr(A∣Pr is ideal)

Both Rational Reflection and New Rational Reflection have us defer to rationality as we would defer to an expert. But whereas Rational Reflection has us defer to rationality as an expert with exactly our evidence, New Rational Reflection has us defer to rationality as an expert who has our evidence and is certain that it is an expert. Elga argues that this is the right way to defer to experts more generally, making New Rational Reflection (on his view) simply a more carefully-formulated expert deference principle. (See Pettigrew & Titelbaum 2014 for a concurrent view.)

In the clock case, New Rational Reflection also allows for epistemic akrasia: Chloe’s beliefs about the clock come apart from her estimate of what’s rational for her to believe about the clock. But New Rational Reflection offers us an explanation for why this is unproblematic. In normal cases, what’s rational is a good guide to what’s true: if it’s rational to believe it will rain, it’s also likely to be true that it will rain. But in the clock case, this generality doesn’t hold. We can predict that rationality and truth will come apart. So when Chloe’s belief matches her best estimate of what’s true, it will diverge from her best estimate of what’s rational. Furthermore, this divergence comes about because of uncertainty about what’s rational. So by eliminating that uncertainty—by deferring to rationality only on the condition that rationality is not uncertain about its own expertise—New Rational Reflection aims to eliminate that gap between rationality and truth. (See Elga 2013, as well as Horowitz 2014, for further discussion of this feature of the case.)

Several authors agree with this line of thought, accepting both the setup of the clock case and Elga’s general line of thought regarding how it should be treated. These authors take the clock case to be one in which epistemic akrasia is rationally permissible after all, showing that any anti-akrasia norms must be formulated carefully. For similar discussion, see Horowitz’s (2014) discussion of the dartboard case (which is modeled after the unmarked clock). There, Horowitz argues that epistemic akrasia is rationally permissible when we expect the evidence to be “falsity-guiding”; though see Weatherson (2019: ch. 10), and Hawthorne, Isaacs, and Lasonen-Aarnio (2021) for arguments that this condition is too narrow. Sliwa and Horowitz (2015) present an alternative level-bridging principle, “Evidential Calibration”, and argue that, like New Rational Reflection, it can help differentiate between rational and irrational cases of epistemic akrasia, as well as rule out bootstrapping. Christensen (2016a) proposes what he calls the “Idealized Thermometer Model”, and makes similar points in its favor. Dorst (2020; see also his 2019) proposes another principle, “Trust”, which is a weakening of Rational Reflection. Dorst argues that Trust allows us to be uncertain about rationality, while still treating rationality as an expert; this vindicates the idea that we should defer to our evidence, and (following I. J. Good) that more evidence is always epistemically beneficial. All of these level-bridging principles take higher-order evidence into account, but also allow some instances of epistemic akrasia.

If we agree with this line of thought, we might draw two lessons. First: treating rationality as an expert is a complicated job, and any rational expert deference principle must be formulated carefully. If we can predict that rationality and truth will come apart, we should not defer unrestrictedly to rationality. And relatedly, we might take cases like this to show that epistemic akrasia can be rational: it can crop up even on some views according to which higher-order evidence is epistemically significant.

Some epistemologists disagree with these lessons, and moreover, with the entire setup of the clock case. One camp of detractors objects to claims (1), (2), and (3) as set out above. These epistemologists argue that we cannot be rationally uncertain about what our evidence is (as Chloe is) or about what it supports. See, for example, Stalnaker (2009), Smithies (2019: ch. 11), and Skipper (2021). Salow (2018) argues that the conception of evidence required to get the puzzle going can be used to allow “biased inquiry”. Cases like the unmarked clock thus present a choice point for epistemologists who want to accommodate higher-order defeat: accept that such cases are possible and reject Rational Reflection, or reject such cases and adopt a strong transparency requirement about evidence and rationality.

Others disagree from the opposite side: Maria Lasonen-Aarnio (2015) argues that even the modified principle, New Rational Reflection, presupposes too much self-knowledge. She advocates rejecting all level-bridging principles, at least as far as our theory of evidential support is concerned (though see section 4 for discussion of her view on cases like Aisha’s).)

Finally, Christensen (2021a) argues that New Rational Reflection doesn’t go far enough. New Rational Reflection says we should defer, to some extent, to any theory of rationality we think might possibly be right. But what if we think it’s possible that rationality can come apart from the truth—as it does in the clock case, or even more broadly, as it does on views that allow moral encroachment? (On the extreme end, consider views on which rationality is about believing what makes you happy.) Surely we should not defer to them, even a little bit. (If one of them is right, of course, we should follow it—but the present question is just about whether we should treat different candidates for rationality as experts.) So Christensen argues we should reject New Rational Reflection, and focus on attaining beliefs we take to be accurate, rather than beliefs we take to be rational. This may mean moving away entirely from rationality-focused level-bridging principles.

3.2.2 Reliability-focused principles
So far we have focused on Rational Reflection and New Rational Reflection, which are motivated by the thought that in accommodating higher-order evidence, we should calibrate our first-order beliefs to our higher-order beliefs about rationality. Another way to approach the question begins with reliability. The thought here is that higher-order evidence affects our first-order beliefs because of its bearing on our reliability—that is, our propensity to get to the truth, under the relevant circumstances. Examples of this approach include Elga’s “Equal Weight View”, quoted above; Christensen (2007a)’s “Integration”; the “Calibration Rule”, discussed by White (2009); Weatherson’s (ms—see Other Internet Resources) “Judgments Screen Evidence” (which he argues against); “Guess Calibration”, discussed in Sliwa and Horowitz 2015; “Calibration”, discussed in Schoenfield 2015a; and the “Simple Thermometer Model” discussed in Christensen 2016a. As a representative example, here is White’s “Calibration Rule”, which he presents as a consequence of the Equal Weight View:

Calibration Rule: If I draw the conclusion p on the basis of any evidence e, my credence in p should equal my prior expected reliability with respect to p.

There are various complications involved in interpreting this rule (for instance, it refers to “drawing a conclusion”, which is an all-or-nothing judgment, as well as credences). But notice that, unlike Rational Reflection, the Calibration Rule makes no reference to rationality. (It does specify that the conclusion is drawn on the basis of some evidence, but as White points out in his discussion of the rule, the evidence plays no part in determining what we should believe.)

Reliability-based level-bridging principles must also be formulated carefully in order to navigate worries in the vicinity of the generality problem. (Among other things, one’s reliability estimate on a particular occasion must take base rates into account; see Isaacs 2021.) But just as we saw before, such principles are always put forth in conjunction with—or incorporate some element of—Independence principles. If we can answer the question of how to circumscribe Independence, presumably that will answer the generality problem in this domain as well.

Focusing on reliability, rather than rationality, has advantages and disadvantages. An important advantage is that this approach allows us to accommodate cases like the unmarked clock, discussed in the previous section. But the disadvantage is that reliability-based level-bridging principles do not answer certain questions which, for many epistemologists, lie at the heart of the higher-order evidence debate: can epistemic akrasia be rational? And should we in some sense treat rationality as an expert to which we should defer? If our level-bridging principle does not mention rationality, it will not have immediate consequences for these questions, which may seem unsatisfying to some.

Distinguishing between rationality-focused and reliability-focused level-bridging principles highlights another choice point for epistemologists. Some take questions about akrasia to be central, and defend rationality-based level-bridging principles for that reason (Smithies 2019 is a paradigmatic example; see also Neta 2019). Others—even those who support level-bridging—take cases like the unmarked clock to show that these questions aren’t so central after all, and that perhaps epistemic akrasia is not so significant (Christensen 2016a and 2021a are paradigmatic examples of this approach). A third strategy is to say that anti-akrasia, or level-coherence principles, have a special sort of normative status that differs from other rational requirements. Some of the proposals discussed in the next section defend this view.

4. Dilemmas and Two-Norm Views
We began by discussing a puzzle raised by higher-order evidence: it seems that agents in situations like Hypoxia should reduce confidence, but also that if they do, they will be ignoring evidence in a problematic way. We have seen some arguments against reducing confidence, and some proposals for how Aisha should reduce confidence. But some epistemologists think that neither of these possibilities gives us the full story. Maybe level-bridging principles are genuine rational requirements, but there is still something wrong with Aisha if she reduces confidence. Or maybe they are not genuine rational requirements, yet there is something else wrong with Aisha if she does not reduce confidence. Maybe they are both rational requirements, and Aisha is facing an epistemic dilemma. (An alternative way to frame the puzzle focuses on epistemic akrasia: maybe Aisha is rationally required to be epistemically akratic in her situation, but there is something else wrong with her if she is epistemically akratic, and so forth.)

This section discusses two strategies which acknowledge the remaining puzzle and attempt to solve it. One, defended most prominently by David Christensen, holds that higher-order evidence gives rise to epistemic dilemmas, where there is no fully rational response available (though there may be a rationally best response). So Aisha is both required to believe G and to doubt G, and she is just unfortunately unable to do both. Another family of responses, here called “two-norm views”, aim to separate different modes of epistemic evaluation, and argue that Aisha’s conflicting requirements somehow issue from different normative realms. The common goal of all these strategies is to explain the apparent conflict of norms in cases like Hypoxia, without denying the legitimacy of any of these conflicting norms.

4.1 Dilemmas
Christensen takes the puzzles surrounding higher-order evidence to show that the requirements of rationality sometimes conflict with one another, putting agents like Aisha in an epistemic bind. This proposal differs from the two-norm views discussed below, in that there is just one notion of epistemic rationality at work. According to the dilemma view, in some circumstances, it is impossible to satisfy all the rational requirements at once.(see Christensen 2007a, 2010a, 2013, 2016b, and 2021c). In his more recent work (see Christensen 2021c), Christensen sees the norms in conflict, in Aisha’s case, as one requiring us to believe what our evidence entails, and another requiring us to revise in light of higher-order evidence. (An alternative possibility, which Christensen suggests in earlier work [see his 2013], is that the second norm is explicitly an anti-akratic norm.)

An important feature of Christensen’s view is that, although Aisha is subject to conflicting requirements, there is still a best epistemic response in her situation. (Christensen holds that the best response is to reduce confidence.) This raises a question: what determines which response is best? See Leonard 2020 for arguments against the dilemma view; Leonard defends a nearby view according to which there are conflicting norms, but it’s indeterminate (among a restricted set of possibilities) what Aisha should believe. Knoks (2021) defends a view along similar lines, arguing that Aisha’s situation is permissive (again, among a similarly-restricted set of possibilities). The possibility of dilemmas in epistemology raises several questions: how can epistemic requirements relate (or not) to notions of epistemic blame, and how can they guide our beliefs? See Hughes 2019 and 2021, e.g., for further discussion of these issues (though mostly focused on a different putative source of conflicting requirements); Hughes defends the position that we should accept epistemic dilemmas.

4.2 Ideal vs. non-ideal modes of evaluation
A few authors have suggested that while higher-order evidence has genuine normative significance, this significance belongs only to some non-ideal normative realm. So while a rationally ideal agent’s higher-order evidence would have no effect on her first-order beliefs, a non-ideal agent’s evidence should have such an effect—precisely because she is non-ideal.

Joshua DiPaolo (2019) develops one version of this view. Drawing on work in political philosophy, he argues that we need an epistemological “theory of the second best”. On his proposed view, the norms that apply to ideal agents are different from the norms that apply to non-ideal agents. While the ideal norms define a standard of perfection for all of us, non-ideal norms tell us how to approach that standard, taking our imperfections into account. DiPaolo argues that this approach allows us to resolve the apparent tensions involved in accommodating higher-order evidence: ideal rationality requires ignoring higher-order evidence, but non-ideal rationality requires respecting it.

While non-ideal agents are limited in their reasoning abilities, they are also limited in their abilities to double-check and “police” themselves. Appealing to this consideration, Joshua Schechter (2013) makes a suggestion that cuts in the opposite direction from DiPaolo’s: he argues that perhaps our epistemic imperfections cap our responsibility to respond to higher-order evidence. This means that while our available evidence may call for extensive belief revision—and while a more ideal agent would revise her beliefs in response to higher-order evidence—non-ideal believers like us are permitted to stop when we’ve done what we can.

Declan Smithies (2019) also defends a view on which the ideal/non-ideal distinction comes into play. This is discussed in more detail below.

4.3 Best plans to follow vs. best plans to make
Another kind of two-norm view comes from Miriam Schoenfield (see her 2015b and 2018). Schoenfield interprets judgments about rationality, in cases like Aisha’s, as plans. She points out that we can evaluate plans in (at least) two different ways: by looking at what will happen if they are followed perfectly, and by looking at what will happen if they are made. If you need to leave the house at 10:30, but are habitually running late, “leave the house at 10” might be the best plan to make, even if “leave the house at 10:30” is the best plan to follow. This is similar to DiPaolo’s suggestion in that the difference between the best plan to make and the best plan to follow depends on a person’s (predicted) rational shortcomings: the best plan to make is one that takes these shortcomings into account in a particular way. However, Schoenfield’s view is not that higher-order evidence only has rational import for non-ideal agents; rather, it has the import it has for anyone who rationally believes that they are non-ideal.

An advantage of this approach is that it allows us to focus on accuracy as the primary target of epistemic rationality. Schoenfield frames both modes of evaluation in terms of accuracy: specifically, the accuracy-related consequences of the plan. So if she is right that the best-plan-to-make/best-plan-to-follow distinction can explain higher-order evidence, we would end up with a relatively uniform two-norm view. See Horowitz 2019 for two objections: first, that focusing on consequences seems to open the door to non-epistemic plans, such as “have a sandwich before reasoning”; second, that the best plan to make might differ among different believers, in which case this strategy would not yield anything like a general defense of revising in response to higher-order evidence. This second worry also raises questions about DiPaolo’s proposal and how universal we can expect a non-ideal rationality to be.

4.4 Reasons vs. rationality
Alex Worsnip (2018) argues that we can explain the oddness of higher-order evidence by distinguishing between two kinds of epistemic requirements: evidence responsiveness and coherence. If Aisha maintains her belief that G, and believes that her evidence may well not support this belief, she will believe everything her evidence supports. If she revises her belief that she has enough fuel (aligning it with her estimate of what the evidence supports) she will be coherent. This view is one that explicitly appeals to epistemic akrasia, as Worsnip’s coherence norms are effectively anti-akrasia norms.

4.5 Evidence vs. dispositions
Maria Lasonen-Aarnio (2020) argues that the conflict comes from two different modes of evaluation which we can apply to Aisha’s case. In addition to evaluating whether believers have correctly followed the epistemic requirements that apply to them, or have correctly accommodated their evidence, we can also evaluate their belief-forming dispositions. If Aisha holds onto her belief about G, while recognizing the danger of hypoxia in her own case, she is responding appropriately to her evidence and is (if her belief is formed in the right way) following the requirements that apply to her. But she is also manifesting a disposition that is unlikely to serve her well in the long term: ignoring what appears to be a conclusive reason for belief (her belief that she’s likely to be hypoxic, and therefore likely to be responding inappropriately to her evidence) will often involve actual conclusive reasons for belief. We can criticize Aisha for manifesting this disposition, even if this particular case is not one of the situations in which it goes wrong. (A relevant comparison here is Rule versus Act Utilitarianism: a good rule might occasionally lead to imperfect outcomes, whereas a good outcome might occasionally be the result of a bad rule. See Coates [2012] for a similar suggestion about rules in this context.)

Smithies (2019) defends a similar view, discussed in more detail in the next subsection.

4.5 Propositional justification vs. doxastic justification
Finally, Paul Silva (2017) and Declan Smithies (2019) each argue—though in different ways—that we can resolve the conflict by distinguishing between propositional and doxastic justification. (Also see Ye 2020 for objections to the move to doxastic justification in this context.) Let us consider Silva’s proposal first. An agent like Aisha, he argues, is propositionally justified in believing G, and in believing that her evidence may well not support G. But she cannot come to rationally hold the belief that G: in other words, she cannot be doxastically justified in believing G. Silva’s argument for this claim is in part an inference to the best explanation: although Aisha’s evidence supports G, it seems irrational for Aisha to act on that belief, suggesting that Aisha lacks knowledge of G. If Aisha’s belief is propositionally justified but not known, this in turn suggests that the belief is not doxastically justified. (See also van Wietmarschen 2013 for a similar proposal in the case of peer disagreement. Van Wietmarschen’s argument does not appeal to knowledge; rather, he argues directly that an agent’s belief is not well-grounded, or doxastically justified, if her basis for holding it does not respond to her higher-order evidence.)

Smithies (2019) invokes this distinction to a slightly different end. As mentioned above, he argues that we can never be propositionally justified in believing falsehoods about what our evidence supports—so, Aisha is propositionally justified in believing G, and propositionally justified in believing that her evidence supports this. But assuming that Aisha is a normal, non-ideal believer, her own doxastic dispositions won’t be sensitive enough to the evidence to safely track her propositional justification. This means that she can’t be doxastically justified in believing that her evidence supports G. Interestingly, Smithies argues that in this case a non-ideal agent like Aisha should be epistemically akratic; the ideal, non-akratic state isn’t available to her, and an akratic state is, he argues, the best she can do.

5. Conclusion
We began by noting that higher-order evidence gives rise to a puzzle. It seems that “higher-order evidence”—information about our own irrationality or unreliability—should prompt us to revise our beliefs about the world, but if we do so, we must ignore evidence and reasoning that is directly relevant to the truth of those beliefs. Much of the literature on higher-order evidence revolves around this puzzle: arguing that we should reject one side of the puzzle or the other, or finding ways to hold onto both.

As of this writing, two related questions emerge as central to the debate going forward. First: is higher-order evidence significant because it gives us information about our own rationality? Or because it gives us information about our own reliability? And second: are there genuine norms prohibiting epistemic akrasia? (So, should our beliefs about what’s rational line up with our beliefs about the world?) Or does the appearance of these norms merely arise because of nearby norms regarding reliability—or, perhaps, because there are multiple, conflicting modes of epistemic normativity? Untangling the puzzles surrounding higher-order evidence will ultimately involve answering these questions as well, as well as broader questions about the relationship between rationality and truth.


